{
    "pid": "bdr:bfttpwkj",
    "id": "bdr:bfttpwkj",
    "json_uri": "https://repository.library.brown.edu/api/collections/bdr:bfttpwkj/",
    "uri": "https://repository.library.brown.edu/studio/collections/bdr:bfttpwkj/",
    "db_id": 299,
    "name": "Theses and Dissertations",
    "description": "",
    "tags": [ ],
    "facets": [
      {
        "field": "mods_type_of_resource",
        "name": "Format",
        "sort_by_count": true
      },
      {
        "field": "genre",
        "name": "Genre",
        "sort_by_count": false
      },
      {
        "field": "keyword",
        "name": "Keywords",
        "sort_by_count": false
      }
    ],
    "parent_folders": [
      {
        "pid": "bdr:ms9qtgbu",
        "id": "bdr:ms9qtgbu",
        "json_uri": "https://repository.library.brown.edu/api/collections/bdr:ms9qtgbu/",
        "uri": "https://repository.library.brown.edu/studio/collections/bdr:ms9qtgbu/",
        "db_id": 298,
        "name": "Computer Science",
        "description": "The Computer Science Department at Brown has forged a path of innovative information technology research and teaching at both the undergraduate and graduate levels. From our modest beginnings as an interest group within the Divisions of Applied Mathematics and Engineering in the 1960s to its current stature as one of the nation's leading computer science programs, the Computer Science Department has continuously produced prominent contributors in the field, at both the undergraduate and graduate levels.",
        "tags": [ ]
      }
    ],
    "child_folders": [ ],
    "ancestors": [
      "Computer Science"
    ],
    "items": {
      "numFound": 209,
      "start": 0,
      "docs": [
        {
          "pid": "bdr:386312",
          "object_type": "pdf",
          "abstract": [
            "In today's networks, non-administrative users have little interaction with a network's control-plane. Such users can send probe traffic to develop inferences about the network's present state, yet they cannot directly contact the control-plane for answers because of security or privacy concerns. In addition to reading the control-plane's state, modern applications have increasing need to write configuration state as well. These applications, running in home, campus, and datacenter networks, know what they need from the network, yet cannot convey such intentions to the control-plane. This dissertation introduces participatory networking, a novel platform for delegating read and write authority from a network's administrators to end users, or applications and devices acting on their behalf. Users can then work with the network, rather than around it, to achieve better performance, security, or predictable behavior. Our platform's design addresses the two key challenges: how to safely decompose control and visibility of the network, and how to resolve conflicts between untrusted users and across requests, while maintaining baseline levels of fairness and security. We present a prototype implementation of participatory networking, structured as an API and controller for OpenFlow-based software-defined networks (SDNs). We call our controller PANE, and demonstrate its usefulness by experiments with four real applications (Ekiga, SSHGuard, ZooKeeper, and Hadoop), and its practicality through microbenchmarks. Furthermore, we develop a mechanical proof for a key portion of PANE, the first for an SDN controller. Unfortunately, network administrators interested in using SDN controllers such as PANE to manage the network face the herculean challenge of migrating existing policy to the new platform. To lessen this challenge, this dissertation introduces Exodus, the first tool for directly translating existing network configurations in languages such as Cisco IOS and Linux iptables to SDN controller software. These controllers are written in Flowlog, a novel, rule-based, tierless language for SDNs we significantly enhance for Exodus. Automatic migration of existing configurations into SDN controllers has exposed several limitations in both today's languages for SDN programming, and OpenFlow itself. This dissertation explores these limits, and provides guidance on SDN migration and necessary switch features."
          ],
          "keyword": [
            "software-defined networks",
            "network configuration",
            "OpenFlow"
          ],
          "primary_title": "Policy Delegation and Migration for Software-Defined Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386312/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386312/"
        },
        {
          "pid": "bdr:386305",
          "object_type": "pdf",
          "abstract": [
            "In the modern digital society, individuals, businesses and governments perform numerous everyday tasks such as shopping, banking and commuting using electronic media. Although these electronic transactions provide efficiency and convenience, they usually overlook the privacy of the users. This thesis studies privacy-enhancing technologies in order to get the best of both worlds: all the benefits of electronic transactions but without sacrificing user privacy. Using efficient cryptographic tools such as digital signatures, zero-knowledge proof systems and encryption schemes, we propose secure protocols that protect user privacy and at the same time are practical. Anonymous credential systems allow users to obtain and demonstrate possession of digital credentials in order to authenticate themselves in a privacy-preserving manner. We first show impossibility results on proving the security of one of the most well-known anonymous credential systems, the one due to Stefan Brands, that is currently being implemented by Microsoft under their credential management project, U-prove. Our impossibility result not only applies to Brands but generalizes to a much broader class of protocols. We then propose Anonymous Credentials Light: the first efficient single-use anonymous credential scheme that is provably secure. Cryptographic e-cash allows secure and private electronic payments and provides similar unforgeability and untraceability as physical cash does. Our Anonymous Credentials Light can be extended to an efficient e-cash scheme that moreover has the nice property of encoding users' attributes in the coins. We provide a smartphone implementation of our proposed scheme and explain how it can be used for efficient and private payments in the public transportation scenario. A limitation of traditional cryptographic e-cash, however, is that it does not allow users to transfer coins to each other. We present the first practical, fully anonymous transferable e-cash scheme that does not depend on a trusted authority to detect double spending. Finally, we study how to revoke users' secret keys or credentials when, for example, a user misbehaves or a secret key was compromised. We propose efficient generic revocation mechanisms that can be used as building blocks for various constructions."
          ],
          "keyword": [
            "private authentication",
            "electronic payments",
            "Cryptography",
            "Privacy"
          ],
          "primary_title": "Efficient Cryptography for Information Privacy",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386305/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386305/"
        },
        {
          "pid": "bdr:386247",
          "object_type": "pdf",
          "abstract": [
            "Many optimization problems involving graphs naturally feature road networks, grids, or other large planar graphs. By exploiting the special structure of planar graphs, it is often possible to design algorithms that are faster or give better results than their counterparts for general graphs. We present an efficient polynomial-time approximation scheme for the Steiner forest problem in planar graphs, a bicriteria polynomial-time approximation scheme for the ball cover problem in planar graphs, a linear-time algorithm for multiple-source shortest paths problem (MSSP) in planar graphs with integer weights that are small on average, and algorithms that use MSSP to compute closeness and betweenness centrality in planar graphs. We report on our implementation of the latter."
          ],
          "keyword": [
            "planar",
            "graph",
            "approximation",
            "Steiner",
            "k-center",
            "shortest paths",
            "centrality",
            "implementation",
            "Algorithms"
          ],
          "primary_title": "Toward Practical Planar Graph Algorithms",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386247/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386247/"
        },
        {
          "pid": "bdr:386143",
          "object_type": "pdf",
          "abstract": [
            "An emerging class of distributed database management systems (DBMS) seek to provide high-performance transaction processing for data-intensive applications while maintaining strong consistency guarantees. These systems achieve this by storing databases in a cluster of shared-nothing, main memory partitions. This partitioning enables them to eschew much of the legacy, disk-oriented architecture that slows down traditional systems, such as heavy-weight concurrency control algorithms, thereby allowing for the efficient execution of single-partition transactions. But many applications cannot be partitioned such that all of their transactions execute in this manner; these multi-partition transactions require expensive coordination that inhibits performance. In this dissertation, we study the problem of scalable transaction execution for modern on-line transaction processing (OLTP) applications. We present the design of H-Store, a distributed, main memory DBMS that is optimized for short-lived, write-heavy transactional workloads. We then present domain-specific applications of optimization and machine learning techniques to improve the performance of fast database systems like H-Store. Our first contribution is an approach for automatically generating a database design for an OLTP application that both minimizes both the amount of cross-node communication and skew in the cluster. We then present a Markov model-based approach for automatically selecting which optimizations to enable at run time for new transaction requests based on their most likely behavior. Finally, we present a method for using these models to schedule speculative transactions and queries whenever a DBMS stalls because of a multi-partition transaction. All together, these allow enable H-Store to support transactional workloads that are beyond what single-node systems can handle."
          ],
          "keyword": [
            "OLTP",
            "Database Systems",
            "H-Store",
            "Zdonik",
            "Databases"
          ],
          "primary_title": "On Scalable Transaction Execution in Partitioned Main Memory Database Management Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386143/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386143/"
        },
        {
          "pid": "bdr:386282",
          "object_type": "pdf",
          "abstract": [
            "Cloud storage has emerged as the next generation of data storage where users can remotely store their data and leave its management to a third party, e.g., Amazon S3, Google Drive or Microsoft Azure. However, the fact that users no longer have physical possession of their data raises new challenges in terms of data privacy. Storing the data in encrypted form is a key component in maintaining privacy against the storage provider. However, encryption alone is not enough since information may be leaked through the pattern in which users access the data. In this thesis, we describe algorithms that allow \\emph{data-oblivious} access to remotely stored data. That is, access patterns of such algorithms depend only on the size of the outsourced data and algorithm input, but not their content. Hence, such algorithms reveal nothing about the data they are processing. We start by describing a general method that obliviously simulates user requests to outsourced data of size n and adds O(\\log n) overhead in the average case, succeeding with very high probability. This method assumes a private workspace of size O(n^{\\epsilon}) on the user side, for any given fixed positive constant \\epsilon, and does not maintain a state between data requests. We then show how to deamortize our method to achieve O(\\log n) overhead in the worst case. Our deamortization technique is general and can be applied to several existing oblivious simulations. The next oblivious simulation technique presented in this thesis improves over the O(\\log n) solution and demonstrates an interplay between system parameters such as latency, bandwidth, and the size of the user's private memory. We show that if a user exchanges messages of size O(n^{1/c} \\log n) with the storage provider, for some constant c>=2, and has access to private memory of the same size, then our method can achieve O(1) access overhead in the worst case with very high probability. Finally, we study application-specific access patterns and look at how to make them oblivious without using the general oblivious simulation methods mentioned above. In particular, we show how one can access and perform a computation in an oblivious fashion on an externally stored graph. We also show that a number of classic graph drawing algorithms can be efficiently implemented in this framework while maintaining user privacy."
          ],
          "keyword": [
            "cloud storage",
            "oblivious RAM (ORAM)",
            "access patterns",
            "oblivious storage",
            "oblivious shuffle",
            "Privacy",
            "Graph algorithms"
          ],
          "primary_title": "Data-Oblivious Algorithms for Privacy-Preserving Access to Cloud Storage",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386282/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386282/"
        },
        {
          "pid": "bdr:320512",
          "object_type": "pdf",
          "abstract": [
            "This dissertation shows that qualitative and quantitative characterization of patterned structures in brain connectivity data obtained using diffusion MRI not only improves the exploration of the intricate space of brain connectivity but also provides clinically meaningful measures, quantifying normal and pathological variation in the brain. To this end, we introduce a set of computational and mathematical models, algorithms, and interactive tools to explore, understand, and characterize diffusion-derived structural brain connectivity. We contribute to all stages of modeling, visualization, and analysis of brain connectivity. In modeling, our contributions are twofold. First, we model the joint distribution of local neural fiber configurations with Markov random fields and infer the most likely configuration with maximum a posteriori estimation. We demonstrate this framework's use in resolving fiber crossings. Second, we introduce new planar map representations of three-dimensional neural tract datasets. These planar representations improve the exploration of brain connectivity by reducing visual and interaction complexity. In visualization, we contribute to structure-preserving color mappings. First, we introduce Boy's surface as a model for coloring 3D line fields and show results from its application in visualizing orientation in diffusion MRI brain datasets. This coloring method is smooth and one-to-one except on a set of measure zero. Second, we propose a general coloring method based on manifold embedding that conveys spatial relations among neural fiber tracts perceptually. We also introduce a new bivariate coloring model, the flat torus, that allows finer adjustments of coloring arbitrarily. We contribute to both local and global analysis of brain connectivity. In local analysis, we introduce a geometric slicing-based coherence measure for clusters of neural tracts. Clustering refinement based on this measure leads to a significant improvement in clustering quality that is not possible directly with standard methods. We also introduce tract-based probability density functions and demonstrate their effective use in nonparametric hypothesis testing and classification. In global analysis, we propose computing the ranks of persistent homology groups in the neural tract space. This captures the effects of diffuse axonal dropout and provides a global descriptor of structural brain connectivity."
          ],
          "keyword": [
            "Computational brain connectivity",
            "diffusion MRI",
            "maximum a posteriori",
            "diffusion tensor",
            "crossing brain fibers",
            "mixture of fiber orientations",
            "tractography",
            "visual analysis",
            "interaction",
            "multiscale",
            "hierarchical clustering",
            "brain map",
            "structure preserving",
            "coloring",
            "line field",
            "real projective plane",
            "Boy's surface",
            "immersion",
            "embedding",
            "perceptual",
            "neural tract",
            "cluster",
            "coherence",
            "clustering",
            "refinement",
            "slicing",
            "Gaussian mixture model",
            "expectation maximization",
            "cycle",
            "homology",
            "persistent homology",
            "homology groups",
            "rank",
            "biomarker",
            "tract based",
            "probability density function",
            "density estimation",
            "histogram",
            "kernel",
            "nonparametric hypothesis testing",
            "classification.",
            "Markov random fields",
            "Diffusion tensor imaging",
            "Abstraction",
            "Multiscale modeling",
            "Hierarchical clustering (Cluster analysis)",
            "Brain mapping",
            "Stability",
            "Social classes",
            "Biochemical markers",
            "Permutations",
            "Classification"
          ],
          "primary_title": "Computational Brain Connectivity Using Diffusion MRI",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320512/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320512/"
        },
        {
          "pid": "bdr:320518",
          "object_type": "pdf",
          "abstract": [
            "Variation in genomes occurs in many forms, from single nucleotide changes to gains and losses of entire chromosomes. Large-scale rearrangements, called structural variants (SVs), are associated with numerous diseases and are common in cancer genomes. However, many SVs in mammalian genomes are found in highly repetitive regions, complicating their detection and characterization. Ongoing development of genomic technologies invite new algorithmic approaches to SV detection.<br/><br/> In this thesis, we present a collection of four algorithms that identify SVs using data from current and emerging genomic technologies. The first algorithm is designed for a technology called array-comparative genomic hybridization (aCGH), which measures the number of copies of DNA segments present in a test genome relative to a known reference genome. We describe a method to identify SVs that are common to a group of individuals, and apply the method to aCGH data from hundreds of cancer patients. We recover an SV in prostate cancer that is known to be biologically important, and we infer a number of novel SVs in brain cancer.<br/><br/> Our other algorithms are designed for DNA sequencing technologies, which measure a broader range of SVs than aCGH data with the tradeoff of higher cost. One DNA sequencing technology, strobe sequencing, yields multiple sequences from a single, contiguous fragment of DNA. While strobes provide longer sequenced portions of DNA compared to other sequencing technologies, the per-base error rate is substantially higher. Our algorithms for SV detection exploit the benefits of multiply-linked DNA sequences while being robust to high sequencing error rates. We describe the first published method for SV detection using strobe sequencing, which finds the smallest number of SVs (relative to a known reference genome) that explain the strobes. We then improve upon our method with a probabilistic algorithm that better models the strobe sequencing data. Finally, we describe a de novo assembly algorithm for strobe sequencing data when a reference genome is unavailable. We assess the performance of these algorithms on simulated and real strobe sequencing data, and conclude that with appropriate algorithms, strobe sequencing compares favorably to other DNA sequencing technologies."
          ],
          "keyword": [
            "structural variation",
            "array-comparative genomic hybridization",
            "DNA sequencing"
          ],
          "primary_title": "Algorithms for Identifying Structural Variants in Human Genome",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320518/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320518/"
        },
        {
          "pid": "bdr:320519",
          "object_type": "pdf",
          "abstract": [
            "Realistic clothing and hair animation are necessary for many applications such as special effects, gaming, and on-line fashion. Thanks to the advances in computer graphics, highly realistic clothing and hair animation is common in recent animated movies by use of Physics Based Simulation (PBS). Clothing and hair modeling/animation are also important to other fields such as computer vision. However, PBS methods create clothing/hair that are specific to a particular body model and there is no good way to invert the PBS process to fit the parameters of the generative model to images. Furthermore, PBS for clothing requires a significant amount of manual work to find the right size for every instance of a body and prepare the clothing for simulation. This makes PBS unsuitable for applications that involve various body shapes, such as virtual fashion and crowd simulation. We believe a new generative clothing model will address these problems.<br/> <br/> In this thesis, we describe a complete process from estimating 3D human body using image evidence to animating the body with data-driven, low-dimensional 3D clothing and hair models. First, we describe a solution to estimating 3D human body shape from a single photograph or painting using multiple image cues including silhouettes, edges, and smooth shading. Second, we explore a 2D clothing model in which the clothing is modeled as a closed contour of points that are offsets from corresponding body contour points. We show the increased accuracy of 2D body shape estimation and clothing type classification using such a model. Third, we focus on modeling the appearance of the 3D body and propose a complete system for simulating realistic clothing on 3D bodies of any shape and pose without manual intervention. Fourth, we present a 3D hair model that performs hair animation in real-time, preserves the key dynamic properties of physical simulation, and gives the animator continuous interactive control over hair styles and external wind. <br/> <br/> The result is a 3D human body model that can be estimated from images and then animated with realistic clothing, hair, and body movement."
          ],
          "keyword": [
            "3D body shape estimation",
            "clothing animation",
            "hair animation"
          ],
          "primary_title": "Virtual Human Bodies with Clothing and Hair: From Images to Animation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320519/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320519/"
        },
        {
          "pid": "bdr:320469",
          "object_type": "pdf",
          "abstract": [
            "Estimating image motion, or optical flow, in scenes with multiple moving objects and segmenting the individual moving objects are two fundamental problems in computer vision and have applications in many fields, including medical imaging, image processing, graphics, and robotics. Despite decades of extensive research effort, current methods still tend to produce large optical flow errors near motion boundaries and in occlusion regions and falsely merge foreground objects with the background.<br/> <br/> A key feature of optical flow methods is an energy term, or prior, that prefers spatially smooth flow fields. In this dissertation, we show that image-dependent and non-local prior models can better preserve motion boundaries than the widely used pairwise Markov Random Field (MRF) models. We also demonstrate that joint motion estimation and segmentation can achieve more accurate results than the separate treatment of each problem.<br/> <br/> First, we formulate fully learnable low-level models of optical flow and learn the models from training data. Our results show that image-dependent, steerable models outperform standard MRF models, especially in recovering motion boundaries. Second, we perform a quantitative analysis of recent practices in optical flow estimation. Median filtering of the flow field is one of the key features of the most accurate methods and we formalize this as a non-local smoothness term that integrates information over a large spatial neighborhood. We further define a weighted non-local smoothness term that uses both image and motion cues to preserve motion boundaries. Third, we develop a layered model to segment moving objects (layers) using image-dependent, continuous support functions. The method orders each layer in depth and explicitly models the occlusions between layers and the temporal consistency of layers. To avoid being trapped in poor local optima, we define a discrete formulation of our objective function and extend graph cuts optimization methods to obtain good initial values for the continuous formulation. The mixed continuous-discrete optimizer can automatically infer the number of layers and their depth ordering for a given scene. Experimental results on benchmark datasets demonstrate the benefits of joint motion estimation and segmentation via the layered approach."
          ],
          "keyword": [
            "optical flow",
            "motion",
            "segmentation",
            "layered model",
            "median filtering",
            "non-local term",
            "Image segmentation"
          ],
          "primary_title": "From Pixels to Layers: Joint Motion Estimation and Segmentation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320469/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320469/"
        },
        {
          "pid": "bdr:386236",
          "object_type": "pdf",
          "abstract": [
            "Realistic modeling of the human body in 3D has many applications ranging from fashion to the production of movies and video games. In this thesis, we pose that modeling the 3D human shape in a data-driven, surface-based way using multiple instances of captured geometry can lead to (a) accurate estimation of human attributes, e.g. anthropometric measurements and (b) realistic representation of human body deformations. Initially, we describe a method for finding sparse pairwise correspondences between 3D triangular meshes of articulated objects, such as humans, in various shapes and poses. We propose a search framework that effectively explores the space of possible correspondences and is more robust to local optima than previous work. Central to this framework are features based on mesh surface paths that are invariant to shape, pose, and resolution. Then, we present an approach for extracting standard anthropometric measurements from 3D human scans. Our approach relies on fitting a deformable 3D body model to scan data; this model-based fitting is robust to scan noise. In addition, it brings a query scan into registration with a database of registered body scans, which facilitates statistical analysis. We show that combining multiple poses yields optimal measurement prediction. Additionally, we perform an extensive evaluation of existing commercial and research systems using the CAESAR dataset. Finally, we propose a method for capturing and modeling the non-rigid intrinsic shape variation of the human body during breathing. We learn a detailed model of body shape deformations due to breathing for different breathing types and provide simple animation controls to render lifelike breathing. We also develop a novel interface for breathing animation using a spirometer, which measures the breathing volume of a “breath actor.” Our approach generates fine-scale body shape deformations due to breathing with greater ease and realism than previously achieved. In this thesis, we introduce application-dependent 3D shape representations that describe realistically the details of the human shape, capture variation in shape and are tied to the anthropometry of the human body."
          ],
          "keyword": [
            "comuter graphics",
            "3D human body modeling",
            "Computer vision"
          ],
          "primary_title": "Modeling the Human Body in 3D: Data Registration and Human Shape Representation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386236/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386236/"
        },
        {
          "pid": "bdr:386274",
          "object_type": "pdf",
          "abstract": [
            "Determining the sequences of alleles co-inherited on a single chromosome, or haplotypes, is fundamentally important in genomics, molecular biology, and genomic medicine. Experimental methods for determining haplotypes are currently labor intensive, expensive, and do not scale. The computation of haplotypes from genome sequencing, or haplotype assembly, employs graph-theoretic and combinatorial algorithms intertwined with statistical models of DNA. The related problem of haplotype reconstruction from a population sample, or haplotype phasing, uses the statistical linkage between neighboring alleles and identical-by-descent (IBD) evolutionary relationships to reconstruct the haplotype sequences. This dissertation introduces graph-theoretic, combinatorial, and statistical algorithms for genome-wide haplotype reconstruction and IBD haplotype tract inference. Specifically, we present: • DELISHUS, a mathematical model and exact polynomial-time algorithm for computing deletion haplotypes in SNP array data. • The HapCompass algorithm for diploid genomes (e.g. humans) which models haplotype reconstruction as local optimizations on the cycle basis of a graph theoretic representation of variant alleles captured by sequence reads. This framework provides an algorithmic design strategy for a range of haplotype reconstruction problems and incorporates population genetics and identity-by-descent theory into the haplotype reconstruction model. • The first model and algorithm for haplotype assembly of polyploid genomes, that is, organisms with more than two sets of homologous chromosomes (common in plant and tumor genomes). • Tractatus, the first theoretically guaranteed exact and linear time algorithm for identical-by-descent multi-tract inference. We compare our approaches with a variety of competing algorithms and investigate the feasibility of genome-wide haplotype reconstruction from computational and experimental perspectives."
          ],
          "keyword": [
            "haplotype",
            "haplotype phasing",
            "haplotype assembly",
            "population genomics",
            "Autism",
            "Graph theory",
            "Algorithms"
          ],
          "primary_title": "Genome-wide algorithms for haplotype assembly, haplotype phasing, and IBD inference",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386274/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386274/"
        },
        {
          "pid": "bdr:386131",
          "object_type": "pdf",
          "abstract": [
            "GPUs are increasingly utilized for scientific and general-purpose workloads as cheap and efficient hardware accelerators. Despite the GPU's popularity, many codes ported to the GPU suffer degraded performance. Unlike current out-of-order CPUs, the GPU architecture is a scalable array of simple in-order multithreaded streaming multiprocessors, facilitating data-parallel execution. For the GPU, efficiently executing general-purpose applications with arbitrary control flow remains particularly challenging, since it lacks hardware branch prediction and speculation. This dissertation addresses these issues by proposing and evaluating a novel analysis and optimization strategy to improve application performance by automatically increasing instruction-level parallelism (ILP) and instructions executed per cycle (IPC) in the presence of control flow. Our static program-based GPU branch prediction analysis accurately identifies GPU program behavior and program's critical path. This analysis guides our \"Warp-Aware Trace Scheduling\", the GPU's first automatic speculative optimization. Warp-Aware Trace Scheduling circumvents the GPU's lack of hardware branch prediction and hardware speculation to adapt microcode Trace Scheduling to the GPU by revisiting and revising each step of Trace Scheduling to attend to branch and warp behavior, avoiding warp divergence and reducing divergence time."
          ],
          "keyword": [
            "GPU",
            "autoparallelization",
            "compilers",
            "static branch prediction",
            "global instruction scheduling",
            "Parallelizing compilers"
          ],
          "primary_title": "Automatically Extracting GPU Instruction-Level Parallelism",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386131/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386131/"
        },
        {
          "pid": "bdr:419368",
          "object_type": "pdf",
          "abstract": [
            "The performance of multithreaded programs is often difficult to understand and predict. Multiple threads use various locking operations, resulting in the parallel execution of some computations and the sequential execution of others. Threads use hardware resources such as a CPU or a hard drive simultaneously, which may lead to their saturation. The result is a complex non-linear dependency between the configuration of a multithreaded program and its performance. To better understand this dependency a performance prediction model is used. Such a model predicts the performance of a system for different configurations. Configurations reflect variations in the workload, program options such as the number of threads, and characteristics of the hardware. Performance models are complex and require a solid understanding of the pogram's behavior. As a result, building models of large applications manually is extremely time-consuming and error-prone. In this work we present an approach for building performance models of multithreaded programs automatically. We employ hierarchical discrete-event models. The higher-level model simulates the data flow through the program using the queueing network. The mid-level model simulates program's threads using probabilistic call graphs. The low-level model simulates program-wide locks and underlying hardware. We extract information necessary for constructing the model using a combination of static and dynamic analyses of the program under study. This includes information about the structure of the program, the semantics of interaction between the program's threads, and resource demands of individual program's components. The discovered information is translated into the discrete-event model of the program. In our experiments we successfully generated performance models of a suite of large multithreaded programs. The resulting models predicted performance of these programs across a range of configurations with a reasonable degree of accuracy."
          ],
          "keyword": [
            "Program analysis",
            "prediction",
            "simulation",
            "Performance",
            "Modeling",
            "Forecasting",
            "Computer simulation"
          ],
          "primary_title": "Automated Performance Modeling of Multithreaded Programs",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419368/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419368/"
        },
        {
          "pid": "bdr:419552",
          "object_type": "pdf",
          "abstract": [
            "We develop statistical methods for analyzing natural images, videos, motion capture (MoCap) sequences, and three-dimensional (3D) representations of articulated objects. Our goal is to discover and characterize regions, objects, actions, and the parts composing them. Such data typically exhibit wide variability in complexity, with some instances containing only a few objects (parts) and others exhibiting complex structure. Further, images and 3D object representations have strong spatial correlations, while MoCap and video sequences additionally exhibit temporal dependencies. Effective models for such data must automatically reason about the number of constituent objects and parts, while simultaneously modeling strong spatio-temporal interactions. Motivated by these challenges, we study and extend flexible Bayesian nonparametric priors. Focusing first on images, we explore a family of models that generalize the Pitman-Yor (PY) process to produce decompositions of images into depth-ordered segments (layers). Spatial correlations are captured through an ordered set of Gaussian processes that encourage piecewise smooth allocation of pixels to segments. We develop variational methods for effective learning and robust inference, and demonstrate competitive performance on standard image segmentation benchmarks. Next, we explore the distance dependent Chinese restaurant process (ddCRP), a distribution over partitions that allows user-specified affinity functions to capture dependencies between data instances. We show that a statistical model endowed with a ddCRP prior, and an expressive likelihood for modeling deformations, produces state-of-the-art segmentations of articulated 3D objects. We then develop a family of hierarchical ddCRP priors that allow dependencies both between data instances and their latent clusters. Coupled with vector auto-regressive likelihoods, this hierarchical ddCRP successfully discovers activities from related MoCap sequences. The performance of the distance dependent models crucially depends on the choice of the affinity functions. Designing functions that capture appropriate domain specific dependencies can be challenging. We develop extensions to the distance dependent models and borrow ideas from the approximate Bayesian computation (ABC) literature to develop algorithms for learning affinity functions from human annotated data. Through extensive experiments on image and video segmentation corpuses, we demonstrate that the learned models consistently outperform their hand-crafted counterparts."
          ],
          "keyword": [
            "Bayesian nonparametrics",
            "Statistical modeling",
            "ddCRP",
            "Hierarchical Bayesian Modeling",
            "MCMC",
            "Expectation Propagation",
            "Activity Discovery",
            "Topic Models",
            "Image segmentation"
          ],
          "primary_title": "Bayesian Nonparametric Discovery of Layers and Parts from Scenes and Objects",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419552/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419552/"
        },
        {
          "pid": "bdr:419551",
          "object_type": "pdf",
          "abstract": [
            "There are two primary issues facing database systems designed for on-line transaction processing (OLTP): scalability and performance. Traditional disk-based OLTP architectures are the result of a now decades-old hardware assumption that main memory can only store a small subset of the data at any given time. In order address the issue of high disk latency, such architectures employ concurrent execution strategies with complex and expensive mechanisms to ensure correctness. Such an architecture addresses the scalability issue by incorporating high-capacity persistent storage, but fails to meet the performance demands of modern OLTP applications. More recently, architectures have been proposed that take advantage of the huge advances in main memory capacity. By designing an OLTP architecture under the assumption that all data would fit in memory, these main memory database architectures offer significant performance advantages over disk-based architectures. However, they are bounded by the capacity of main memory, and make no use of the significantly higher capacity of persistent storage. In this dissertation, we present a novel architecture called anti-caching that addresses both the performance and scaling needs of modern OLTP applications. Anti-caching works as an extension to a main memory architecture by storing frequently accessed data in main memory and moving cold data to the anti-cache on persistent storage. We present several lightweight mechanisms for tracking data accesses and for efficient and asynchronous movement of data. Finally, we consider the use of next-generation hardware in the form of non-volatile memory, and explore its use in existing OLTP architectures. Taken together, the contributions in this dissertation detail an OLTP architecture well-suited for current and future hardware assumptions that addresses both the performance and scalability of modern OLTP applications."
          ],
          "keyword": [
            "anti-caching",
            "OLTP",
            "Databases"
          ],
          "primary_title": "Anti-Caching and the Use of Non-Volatile Memory in OLTP Databases",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419551/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419551/"
        },
        {
          "pid": "bdr:419545",
          "object_type": "pdf",
          "abstract": [
            "This dissertation describes three primary contributions to the field of medical imaging: (1) a mathematical model (\"Blockhead\") of the macroscopic and microscopic structure of the white matter of the human brain; (2) a technique for computing the parameters of such a model from MRI scans of a given subject; and (3) an application of existing statistical tools to compare instances of any different tissue models according to their accuracy and parsimony with respect to MR images of the subject. The Blockhead model has both discrete and continuous parameters. As such the fitting method demonstrates a novel synthesis of techniques from combinatorial and numerical optimization: namely, the inclusion of short gradient-descent steps into the neighborhood of a local-search solver. The thesis of this work is that this multi-scale tissue model admits of an instance that, for certain inputs with simple geometry, has better fit to the input (in the sense of accuracy and parsimony) than current voxel-oriented techniques. Furthermore the fitting method is capable, in some circumstances, of computing this model instance. The dissertation also details the shortcomings of this model and proposes future refinements to better represent realistic tissue geometry."
          ],
          "keyword": [
            "Medical Imaging",
            "Diffusion MRI",
            "Optimization",
            "Tissue Model",
            "Imaging systems in medicine",
            "Tissues--Models"
          ],
          "primary_title": "A Multi-Scale Model of Brain White-Matter Structure and Its Fitting Method for Diffusion MRI",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419545/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419545/"
        },
        {
          "pid": "bdr:386301",
          "object_type": "pdf",
          "abstract": [
            "Analyzing huge datasets becomes prohibitively slow when the dataset does not fit in main memory. Approximations of the results of guaranteed high quality are sufficient for most applications and can be obtained very fast by analyzing a small random part of the data that fits in memory. We study the use of the Vapnik-Chervonenkis dimension theory to analyze the trade-off between the sample size and the quality of the approximation for fundamental problems in knowledge discovery (frequent itemsets), graph analysis (betweenness centrality), and database management (query selectivity). We show that the sample size to compute a high-quality approximation of the collection of frequent itemsets depends only on the VC-dimension of the problem, which is (tightly) bounded from above by an easy-to-compute characteristic quantity of the dataset. This bound leads to a fast algorithm for mining frequent itemsets that we also adapt to the MapReduce framework for parallel/distributed computation. We exploit similar ideas to avoid the inclusion of false positives in mining results. The betweenness centrality index of a vertex in a network measures the relative importance of that vertex by counting the fraction of shortest paths going through that vertex. We show that it is possible to compute a high-quality approximation of the betweenness of all the vertices by sampling shortest paths at random. The sample size depends on the VC-dimension of the problem, which is upper bounded by the logarithm of the maximum number of vertices in a shortest path. The tight bound collapses to a constant when there is a unique shortest path between any two vertices. The selectivity of a database query is the ratio between the size of its output and the product of the sizes of its input tables. Database Management Systems estimate the selectivity of queries for scheduling and optimization purposes. We show that how to bound the VC-dimension of queries in terms of their SQL expressions, and how to use this bound to compute a sample of the database that allow much a more accurate estimation of the selectivity than possible using histograms."
          ],
          "keyword": [
            "vc-dimension",
            "statistical learning",
            "frequent itemsets",
            "betweenness centrality",
            "graph",
            "Sampling",
            "Algorithms"
          ],
          "primary_title": "Sampling-based Randomized Algorithms for Big Data Analytics",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:386301/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:386301/"
        },
        {
          "pid": "bdr:698525",
          "object_type": "pdf",
          "abstract": [
            "Constraint Query Languages assume that the relations in the database represent data implicitly by constraint speciﬁcations. Unlike standard database query languages, Constraint Query Languages allow ﬁnitely represented inﬁnite size objects, such as lines, polytopes etc. extending the applicability of declarative database querying. In this thesis a general framework for Constraint Query Languages is proposed and four cases of using constraints within database logic programs are analyzed. Each case is the union of a traditional database query language and a logical theory, in which the constraint speciﬁcations are expressed: either as polynomial equations, rational order constraints, integer order constraints, or boolean equations. For each case, a closed form evaluation is described, that is, the output relation of any query is shown to be evaluable in ﬁnite time and representable using the same type of constraints as the input. We derive upper and lower bounds on the time of query evaluation in terms of the size of the database. Several subcases of Constraint Query Languages are also shown to be evaluable in highly parallel fashion. The problem of query containment, an important part of many query optimization algorithms, is examined in detail for one case of Constraint Query Languages."
          ],
          "keyword": [
            "Query languages (Computer science)",
            "Computer scienc",
            "Querying (Computer science)"
          ],
          "primary_title": "Constraint query languages",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:698525/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:698525/"
        },
        {
          "pid": "bdr:76",
          "object_type": "pdf",
          "abstract": [
            "The confluence of ubiquitous, high-performance networking and increased availability of online information has led to the emergence of a new class of large-scale stream processing and dissemination applications. These applications often exhibit diverse logic and performance requirements, yet they all require common facilities, which include construction of an overlay network, routing and processing logic, and membership management. In contrast to existing approaches that provide custom, point solutions to point applications, we introduce a general-purpose infrastructure that provides these core functionalities and can be easily extended for a broad spectrum of target applications. Extensibility is the central design consideration for our infrastructure, which can be customized to support diverse processing logic, stream types, and performance targets through a set of methods that encapsulate application-specific behavior and a cost model for defining the desired QoS and resource-utilization metrics and constraints. Given these specifications, the system automatically creates and optimizes a data stream acquisition, processing and dissemination overlay network. Its optimization is driven by metric-independent operations, which can refine the structure of the overlay network as well as efficiently distribute processing across the network. In this dissertation, we first describe the basic concepts and models used by our infrastructure and present its generic optimization framework. We study different types of optimization techniques we have developed for supporting stream dissemination and stream processing applications. Finally, we evaluate the performance of our framework through results we have obtained by a prototype deployment on the PlanetLab testbed as well as LAN emulation and simulation experiments."
          ],
          "keyword": [
            "Stream Processing",
            "Stream Dissemination",
            "Overlay Networks"
          ],
          "primary_title": "An Extensible Overlay Infrastructure for Wide-Area Stream Processing and Dissemination",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:76/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:76/"
        },
        {
          "pid": "bdr:918762",
          "object_type": "pdf",
          "abstract": [
            "Coreference Resolution is a fundamental natural language processing (NLP) problem, as it attempts to resolve which underlying discourse objects refer to one another. Further, it serves as an essential component of many other core NLP tasks, including information extraction, question-answering, document summarization, etc. However, decades of research have primarily focused on resolving \\textit{entities} (e.g., people, locations, organizations), with significantly less attention given to \\textit{events} --- the actions performed. This dissertation focuses on improving event coreference. We first detail existing research, while addressing potential weaknesses in current systems: the reliance on dozens of hand-engineered lexical features, and agglomerative-based clustering that is limited to mention-to-mention comparisons. We develop a state-the-art relational-based model which uses almost no features, along with a neural clustering approach that is more holistic than existing mention-based clustering approaches. Last, we research the benefits of including entity information and further this by resolving both entities and events. Our model is novel in demonstrating a symbiotic relationship."
          ],
          "keyword": [
            "Machine Learning",
            "Natural language processing (Computer science)",
            "Artificial intelligence",
            "Computer science",
            "coreference resolution",
            "Computational linguistics",
            "Discourse analysis"
          ],
          "primary_title": "Cross-Document Coreference Resolution for Entities and Events",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918762/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918762/"
        },
        {
          "pid": "bdr:65",
          "object_type": "pdf",
          "abstract": [
            "Most protocol analyses only address security properties. However, other properties are important and can increase our understanding of protocols, as well as aid in the deployment and compilation of implementations. We investigate such analyses. Unfortunately, existing high-level protocol implementation languages do not accept programs that match the style used by the protocol design community. These languages are designed to implement protocol roles independently, not whole protocols. Therefore, a different program must be written for each role. We define a language, WPPL, that avoids this problem. It avoids the need to create a new tool-chain, however, by compiling protocol descriptions into an existing, standard role-based protocol implementation language. Next, we investigate two families of analyses. The first reveals the implicit design decisions of the protocol designer and enables fault-tolerance in implementations. The second characterizes the infinite space of all messages a protocol role could accept and enables scalability by determining the session state necessary to support concurrency. Our entire work is formalized in a mechanical proof checker, the Coq proof assistant, to ensure its theoretical reliability. Our implementations are automatically extracted from the formal Coq theory, so they are guaranteed to implement the theory."
          ],
          "keyword": [
            "Program analysis",
            "Cryptographic protocols",
            "Strand spaces",
            "Coq",
            "Theorem proving"
          ],
          "primary_title": "Static Analyses of Cryptographic Protocols",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:65/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:65/"
        },
        {
          "pid": "bdr:419462",
          "object_type": "pdf",
          "abstract": [
            "The purpose of this thesis is to better understand how to make good decisions in markets. Decision-making in markets is notoriously hard: a buyer or seller must choose from many possible actions in a stochastic, partially-observable, multi-agent environment, and must make these choices repeatedly over time. To make decisions in practice, buyers and sellers often simplify the problem with abstractions. A common abstraction, central to this thesis, is one in which agents make various predictions about the current and future states of the world, and then optimize with respect to those predictions. But this begs the question: what prediction and optimization abstractions lead to effective decisions? In this thesis, I present case studies in which I designed, implemented, and analyzed performance of autonomous agents in seven particular market domains. These domains include trading agent competitions, simpler auction domains, and real-world online advertising domains. For each market domain, I formalize a game-theoretic (or in some special cases, decision-theoretic) model for making decisions in that domain. I characterize the resulting problem structure that can potentially be exploited by abstractions. I develop autonomous agents specialized for making decisions in each domain, and describe these agents in terms of their prediction and optimization abstractions (and algorithms for solving those abstractions). I demonstrate the effectiveness of these abstractions through empirical game-theoretic analyses, evaluation of TAC tournament performance, and worst-case bounds on regret for making different problem abstractions. As a step towards understanding what problem structure exists and what abstractions are effective across market domains, I developed a working taxonomy of prediction and optimization abstractions that were frequently effective in these particular market domains."
          ],
          "keyword": [
            "sequential decision-making",
            "Game theory",
            "Auction theory",
            "Artificial intelligence"
          ],
          "primary_title": "Prediction and Optimization Abstractions in Market Games",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419462/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419462/"
        },
        {
          "pid": "bdr:419455",
          "object_type": "pdf",
          "abstract": [
            "Hot environments pose a risk of heat illness for many professions especially when heavy workloads or protective clothing are necessary. Modern wearable sensors may be able to mitigate risk of heat illness and improve performance if they are able to track health state and provide feedback to the user. However, effective algorithms and models to make use of wearable sensor information are lacking. We present two contributions: 1) a method for health state estimation of the latent human body core temperature from physiological sensors, and 2) models for policy estimation to provide automated advice to reduce thermal-work strain and improve physiological performance over a course of prescribed work. Continuous measurement of core temperature, a requisite of thermal-work strain health state, has been an open physiology problem. We show that the physiological dependencies of the human thermoregulatory system can be cast into a dynamic Bayesian network model that allows us to estimate core body temperature from wearable physiological sensors. We simplify this model to use only an input of heart rate which is collected by many commercial sensors. This approach is validated across different combinations of temperature, hydration, clothing, and acclimation states, and shows similar comparison accuracy to accepted laboratory measures. We finally demonstrate the use and effectiveness of the algorithm from experimental trials during a first responder live training event. We also present a Markov decision process that uses health state estimates to optimize individual pacing strategies to reduce the overall level of thermal-work strain. We describe the estimation of real world activity objectives and thermal-work strain constraints as a reinforcement learning problem. Using a dynamical simulation of physiology, pacing estimates from this model are shown to reduce overall thermal-work strain. Our health state and policy estimation contributions were evaluated in the context of an implementation to compare human self-guided pace and policy guided pace. The results show that the policy allowed individuals to complete the task with meaningfully lower thermal-work strain. We demonstrate that real-time feedback from our model was able to match the thermoregulatory efficiency of a well-trained athlete."
          ],
          "keyword": [
            "physiological monitoring",
            "core body temperature estimation",
            "human performance optimization",
            "Patient monitoring"
          ],
          "primary_title": "Human Thermal-Work Strain Performance Optimization from Wearable Physiological Sensors",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419455/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419455/"
        },
        {
          "pid": "bdr:419436",
          "object_type": "pdf",
          "abstract": [
            "Computer architecture has recently seen an explosion of innovation that has enabled more parallel execution, while parallel software systems have been making strides in providing more simplified programming models. The number of computing cores used in every area of the software ecosystem continues to increase, and parallelism within programs is now ubiquitous. Ideally, performance would scale linearly with the number of cores, but that is rarely the case in practice. Communication and synchronization between cores running the same application are often necessary, but usually come at a high cost. This results in reduced scalability and a significant drop in performance. In this context, parallel software needs to provide more simplified programming patterns and tools that enable a higher potential for parallelism without increasing the burden on the programmer. This thesis discusses new techniques to simplify writing efficient parallel code by leveraging novel architectural features from many current systems. First, we describe various programming abstractions, such as delegation, elimination, combining and transactional memory, which improve scalability and performance of concurrent programs. Next, we show how to use and integrate these abstractions to write scalable concurrent algorithms, such as stacks and priority queues. Finally, we describe how to further improve these abstractions. In particular, we present new transactional memory algorithms that use Intel’s new extension to the x86 instruction set architecture, called Restricted Transactional Memory, to simplify general synchronization. Developers can use all of these abstractions as building blocks to create efficient code that is able to scale on very diverse platforms, with minimal specialized knowledge of parallel programming."
          ],
          "keyword": [
            "concurrent algorithms",
            "concurrent data structures",
            "NUMA-aware data structures",
            "transactional memory"
          ],
          "primary_title": "Concurrent Algorithms for Emerging Hardware Platforms",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419436/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419436/"
        },
        {
          "pid": "bdr:419434",
          "object_type": "pdf",
          "abstract": [
            "In this thesis we address the problem of building shape models of the human body, in 2D and 3D, which are realistic and efficient to use. We focus our efforts on the human body, which is highly articulated and has interesting shape variations, but the approaches we present here can be applied to generic deformable and articulated objects. To address efficiency, we constrain our models to be part-based and have a tree-structured representation with pairwise relationships between connected parts. This allows the application of methods for distributed inference based on message passing. To address realism, we exploit recent advances in computer graphics that represent the human body with statistical shape models learned from 3D scans. We introduce two articulated body models, a 2D model, named Deformable Structures (DS), which is a contour-based model parameterized for 2D pose and projected shape, and a 3D model, named Stitchable Puppet (SP), which is a mesh-based model parameterized for 3D pose, pose-dependent deformations and intrinsic body shape. We have successfully applied the models to interesting and challenging problems in computer vision and computer graphics, namely pose estimation from static images, pose estimation from video sequences, pose and shape estimation from 3D scan data. This advances the state of the art in human pose and shape estimation and suggests that carefully defined realistic models can be important for computer vision. More work at the intersection of vision and graphics is thus encouraged."
          ],
          "keyword": [
            "human body model; shape model; computer vision"
          ],
          "primary_title": "Shape Models of the Human Body for Distributed Inference",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419434/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419434/"
        },
        {
          "pid": "bdr:419504",
          "object_type": "pdf",
          "abstract": [
            "Cancer is a disease resulting from genomic mutations that occur during an individual's lifetime and cause the uncontrolled growth of a collection of cells into a tumor. These mutations range from single nucleotide variants to larger rearrangements such as copy number aberrations or structural variants that duplicate, delete or rearrange entire segments of DNA. As we enter the era of personalized medicine, where a patient's treatment may be tailored to their specific genomic architecture, accurate identification and interpretation of the set of mutations within each patient's genome is increasingly important. Despite numerous recent advances in DNA sequencing technologies, many challenges still exist for measuring genomic mutations - especially for cancer genomes. For example, tumors often exhibit intra-tumor heterogeneity where individual cells in a single tumor contain different complements of mutations. Additionally, cancer genomes are highly rearranged and often contain complex rearrangement patterns that amplify, delete or interweave distant regions of the reference genome. These challenges confound the identification and interpretation of the complete set of mutations in a tumor. Therefore, we have developed a suite of algorithms that directly address these challenges. With respect to intra-tumor heterogeneity, we describe several algorithms that infer the composition of heterogeneous tumors from both single and multi-sample datasets. In the case of multi-sample data, our approach is also able to infer the evolutionary history of the tumor sample. With respect to complex rearrangements, we present algorithms to find the most likely set of rearrangements present in a cancer genome, and to determine whether such a set of rearrangements occurred simultaneously or as part of a sequence of individual events. We demonstrate the advantages of our methods over competing algorithms using both simulated and real DNA sequence data. This collection of algorithms forms an initial step towards enabling improved characterization of genomic mutations in tumor samples."
          ],
          "keyword": [
            "DNA sequencing",
            "Cancer",
            "Algorithms"
          ],
          "primary_title": "Computational Characterization of Rearrangements and Heterogeneity in Cancer",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419504/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419504/"
        },
        {
          "pid": "bdr:419483",
          "object_type": "pdf",
          "abstract": [
            "We present several novel constructions—combining cryptography, error correcting codes (ECCs), and data compression—that find ready application in enhancing security and fault-tolerance in cloud storage. We demonstrate this by presenting a simple (yet novel) secure cloud storage scheme (which can be used on top of any cloud service provider) that provides strong guarantees of integrity and fault-tolerance, and we show the different enhancements possible using our constructions. Our constructions provably achieve strong theoretical properties and are quite practical as well. First, we consider the problem of combining data compression with encryption to provide a primitive that performs both operations at once. This work provides the first formal definitions of security for schemes that combine compression and encryption. We present two compressing ciphers that are the first to provably achieve these strong guarantees of privacy and security. Moreover, one construction is quite practical and provides data compression ratios and speeds comparable to standard algorithms, as demonstrated with a detailed set of experiments. Second, we utilize cryptographic primitives to enhance erasure codes to withstand adversarial corruption of the encoded data. As part of this, we present a new adversarial model for ECCs which is more powerful than previously considered. We then provide two constructions, called authenticated error correcting codes, that transform an erasure code into an ECC and are provably secure in our model. The first scheme combines digital signatures and list decoding while the second uses a message authentication code (MAC), a non-malleable cipher, and a pseudorandom permutation. Finally, we present cryptographically enhanced LT codes (a fast, rateless erasure code) which are able to provide error correction over an adversarial channel. LT codes encode data via a sparse bipartite graph where each output symbol is the XOR of a random subset of the message symbols. All prior work on LT codes only considered random erasures or random errors (e.g., additive white Gaussian noise) and would fail under adversaries that exploit the encoding graph. We define a new framework for analyzing the security of rateless codes and provide three provably secure constructions: (1) a basic scheme that is used as a subroutine in the other schemes; (2) a scalable, block-oriented fixed-rate scheme; and (3) a scalable, block-oriented rateless scheme. All of these schemes maintain both asymptotic and practical efficiency—the latter we demonstrate experimentally."
          ],
          "keyword": [
            "data compression",
            "error correcting codes",
            "rateless codes",
            "secure storage",
            "provable security",
            "Cryptography",
            "Data compression (Computer science)",
            "Error-correcting codes (Information theory)"
          ],
          "primary_title": "Secure Data Compression and Error Correcting Codes for Networks and Cloud Storage",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:419483/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:419483/"
        },
        {
          "pid": "bdr:674362",
          "object_type": "pdf",
          "abstract": [
            "Human communication is naturally multimodal. On the web, images frequently appear alongside text: for example, product images and descriptions on shopping websites, or social media users commenting on an image or a video. Image captions can serve many purposes: describing the salient content of an image, giving background information that is relevant to understanding the image, and allowing for images to be indexed and retrieved on search engines. Automatic image captioning is a challenging task involving several open problems in the fields of Natural Language Processing (NLP) and Computer Vision (CV). This thesis presents work toward image captioning methods that learn from weakly-supervised examples of previously captioned images. These approaches employ text-to-text natural language generation techniques, which generate image captions by adapting text from captions of visually similar images. Using automatic and human evaluations, we demonstrate that our models can produce coherent and informative captions of images. The work in this thesis will help enable the development of data-oriented image captioning systems which can be used to generate captions that describe the same relevant features that are described by humans, even in specific domains with few CV resources."
          ],
          "keyword": [
            "natural language",
            "Computer vision",
            "Natural language processing",
            "Computer science"
          ],
          "primary_title": "Data-driven Image Captioning",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674362/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674362/"
        },
        {
          "pid": "bdr:674121",
          "object_type": "pdf",
          "abstract": [
            "The tools and language of combinatorial topology have previously been very successful in characterizing distributed task solvability when processes fail only by crashing. In this work, the approach is extended to systems where processes fail arbitrarily, even maliciously, characterizing what the literature calls \"Byzantine failures\". Distributed tasks are formalized in terms of a pair of combinatorial structures called simplicial complexes: one models process inputs (the input complex), and another models process outputs (the output complex). A map between the input complex and output complex defines the task semantics. This thesis establishes necessary and sufficient solvability conditions for Byzantine tasks, in synchronous and asynchronous systems. For asynchronous systems, a Byzantine task is shown to be solvable if and only if a specific crash-failure task, suitably defined in terms of the Byzantine task, is solvable as well. For asynchronous colorless tasks, an important subclass of problems that includes consensus and k-set agreement, the above characterization reduces to particularly succinct and elegant forms. The frontier between possible and impossible for colorless tasks is delineated by pivotal problems such as multidimensional epsilon-approximate agreement and barycentric agreement, for which we present protocols with maximum resilience. For synchronous systems, the information dissemination throughout the rounds is modeled by simplicial complexes called protocol complexes, and certain topological properties can characterize ambiguity caused by Byzantine failures. This approach ultimately demonstrates that Byzantine processes can potentially impose one extra round of ambiguity compared to crash-failure systems in regard to solving set agreement problems - and at most that in some settings. In essence, this work shows how the language of combinatorial topology, when aligned with traditional algorithmic tools in distributed computing, provides a robust framework to study task solvability not only across different communication models, synchronous and asynchronous, but also across different failure models, crash and Byzantine."
          ],
          "keyword": [
            "byzantine systems",
            "approximate agreement",
            "colorless tasks",
            "Combinatorial topology"
          ],
          "primary_title": "Byzantine Computability and Combinatorial Topology",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674121/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674121/"
        },
        {
          "pid": "bdr:674147",
          "object_type": "pdf",
          "abstract": [
            "This dissertation makes contributions to the field of data visualization related to evaluating visualization systems with novel methods that focus on analysts' thoughts and behaviors. Lab-based evaluations that use only benchmark tasks are typical but often do not capture realistic visualization analysis scenarios or provide information about how people perform analyses. This work addresses the problem that alternative evaluation methods more directly related to analysts' cognitive processes - including insight-based evaluation, human-performance modeling, and modeling visual attention - are often difficult to design and execute. A long-term vision is to systematize these \"thoughtful\" evaluation approaches so that recruiting participants and testing visualizations is as easy as pressing a button. I present three approaches that make it easier to gather and draw conclusions from empirical evidence about where people gaze, how they interact, and what they discover during visualization analysis. These methods are motivated by design problems in the areas of brain-network visualization, spatiotemporal intelligence analysis, and tree diagram layouts. First, I present a method for comparing alternative visualization designs that incorporates both predefined tasks and exploration in a within-subjects design. I demonstrate the method in a case study of network layouts designed to help analysts make sense of a corpus of geotagged documents. Second, I present a method for semi-automatically predicting task completion times for a visualization system using interaction logs, then I validate the method in a case study of brain-network visualization. Finally, I present and evaluate a method for crowdsourcing gaze-location estimates during visualization analysis tasks. Gaze estimates using the method are qualitatively and quantitatively similar to data collected from controlled eye-tracking studies, which are less accessible than using the method. Together, these methods make it easier for visualization developers to understand how analysts accomplish tasks or discover insights about their data using visualization. Limitations of these approaches are discussed along with opportunities for novel evaluation methods for visualization."
          ],
          "keyword": [
            "data visualization",
            "evaluation",
            "user studies",
            "crowdsourcing",
            "qualitative methods",
            "quantitative methods",
            "Information visualization",
            "Human computation",
            "Modeling",
            "Qualitative research",
            "Quantitative research"
          ],
          "primary_title": "Methods for Evaluating Visualizations Using Practical Models of Insight, Interaction, and Gaze",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674147/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674147/"
        },
        {
          "pid": "bdr:919067",
          "object_type": "pdf",
          "abstract": [
            "Robotic perception often fails on reflective and transparent surfaces. We will describe a new method of passive RGB sensing which uses a calibrated camera located on the end effector robotic arm to record a dense light field over a planar surface on a fixed trajectory, turning the robot into a time slice light field camera. We show how to use this light field to render synthetic 2D orthographic projections and to perform 3D reconstruction of the imaged environment. The synthesized photographs suffer from reduced sensor noise, chromatic aberration, and specular highlights when compared to the images used to compose them. We then describe a novel graphical model over the light field data collected by the robot and show that, using this approach, a robot can robustly perform object segmentation, classification, pose estimation, and grasp inference on objects in a static scene. Next we will show how our system enables a Baxter robot to autonomously separate previously unencountered objects from an input pile, collect images of and generate models for those objects, and remove the objects to an output pile. Light field photography performed with an eye in hand camera can enable a robot to perceive and manipulate most rigid household objects, even those with pathological non-Lambertian surfaces. We will support this position with three contributions. The first is the graphical model which describes the synthetic photography we use on our robots, which allows them to collect light fields, synthesize 2D photographs from those light fields, and perform various inference tasks with those synthetic photographs. The second is a collection of methods which form a perception and planning system. This system enables our robots to perform robust pick and place in challenging environments, achieving state-of-the-art results. The third is the reactive programming environment Ein, which coordinates the hand and eye movements of the robot to control its behavior through alternate and simultaneous perception and movement."
          ],
          "keyword": [
            "Computer Vision",
            "Robotics",
            "light fields",
            "synthetic aperture photography"
          ],
          "primary_title": "Light Fields and Synthetic Aperture Photography for Robotic Perception",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919067/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919067/"
        },
        {
          "pid": "bdr:919049",
          "object_type": "pdf",
          "abstract": [
            "Combinatorial and algebraic topology have both provided useful mathematical tools to the field of distributed computing, giving researchers a rigorous mathematical model in which one can prove elegant theorems about task solvability and computability. An early result in the field is the impossibility of deciding whether an arbitrary task is solvable, using a class of tasks called loop agreement. The first part of this thesis extends the original work by describing a natural way in which loop agreement tasks can be composed in parallel, additionally considering a category theoretic perspective of loop agreement. In previous work, Herlihy and Shavit formulated a topological characterization of wait-free task solvability, called the asynchronous computability theorem. However, the original proof relied on an intricate geometric argument with little insight on algorithmic intuition. This thesis develops a distributed protocol, called the convergence algorithm, for solving a chromatic simplex agreement task on simplicial complexes, and from this result, the original asynchronous computability theorem follows. More significantly though, the convergence algorithm highlights the importance of link-connectivity, a property of simplicial complexes, and its relation to more general models of fault-tolerance beyond wait-freedom. The asynchronous computability theorem is then generalized from wait-freedom to the t-resilient model, which permits up to t failures. To do so, the delayed snapshot is introduced, serving as a building block for t-resilient protocols. Using the convergence algorithm, it is shown that only one delayed snapshot, and hence one synchronization barrier, is ever required in implementing any t-resilient protocol, after which processes may run wait-free. This snapshot protocol is further extended to a model of fault-tolerance in which processes run in the presence of an adversary. It is demonstrated using this snapshot that one synchronization barrier may not suffice for tasks solvable against an adversary."
          ],
          "keyword": [
            "Fault-tolerant computing",
            "Electronic data processing--Distributed processing",
            "Algebraic topology"
          ],
          "primary_title": "Fault-Tolerant Distributed Computability",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919049/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919049/"
        },
        {
          "pid": "bdr:9zbjf2uf",
          "object_type": "pdf",
          "abstract": [
            "With few exceptions, robots today are unable to quickly acquire new manipulation skills in the real world. The modern data-driven approach to skill learning is thwarted in practice by the sample complexity of learning algorithms and a lack of diverse, high-quality data. This thesis develops algorithms and frameworks that leverage structured models of perception and action to improve the efficiency of robot skill learning. I first describe our work on modeling articulated objects, posing a new problem formulation and introducing systems capable of autonomously manipulating novel objects. I subsequently show how these models can be used to bootstrap motor skill learning. The thesis then turns to study structure in behavior, illustrating a novel class of policies that are efficiently learnable and readily composable. Together, these methods present a path toward skill learning in real-time in the real world."
          ],
          "keyword": [
            "Computer Vision",
            "Reinforcement learning",
            "Robotics",
            "Robotic manipulation"
          ],
          "primary_title": "Exploiting Structure for Efficient Robotic Manipulation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:9zbjf2uf/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:9zbjf2uf/"
        },
        {
          "pid": "bdr:caa8g3xc",
          "object_type": "pdf",
          "abstract": [
            "Large pre-trained transformer-based language models (PTLMs) have recently dominated the state-of-the-art in Information Retrieval tasks such as web search and question answering. Despite the advantages such models offer with respect to utilizing term context for building better query and document representations, their training currently relies on point-wise or pair-wise similarity learning, and overlooks the effect of ranking context, that is, of jointly scoring a large set of documents closely related to the query. The latter setting better reproduces the target objective of comparing a query against a large collection of documents and had been found beneficial in pre-PTLM Learning-to-Rank literature. In the present work, we first expressly investigate the effect of ranking context and its constituent parts: (1) jointly scoring a large number of candidates, (2) using retrieved (query-specific) instead of random negatives, and (3) a fully list-wise loss. To this end, we introduce COntextual Document Embedding Reranking (CODER), a highly efficient and generic fine-tuning framework that for the first time enables incorporating context into transformer-based language models used in state-of-the-art dense retrieval. CODER acts as a lightweight performance enhancing framework that can be applied to virtually any existing dual-encoder model. We next explore the potential that CODER offers in directly optimizing retrieval for essentially context-dependent properties, such as ranking fairness. We find that, compared to the existing alternatives for deep neural retrieval architectures, our end-to-end differentiable and efficient approach based on CODER can attain much stronger bias mitigation (fairness). At the same time, for the same amount of bias mitigation, it offers significantly better relevance performance (utility). Crucially, our method allows for a more finely controllable and predictable intensity of bias mitigation. Lastly, we seek to enhance the ranking context itself by addressing the problem of sparse relevance annotation in modern large-scale retrieval datasets. To mitigate penalizing the model in case of false negatives during training, we propose evidence-based label smoothing, i.e., propagating relevance from the ground-truth documents to unlabeled documents that are intimately related to them. To that end, we leverage the concept of reciprocal neighbors, moving beyond geometric similarity and exploiting local connectivity in the shared representation space. We find that using the CODER framework to fine-tune retrievers based on the recomputed labels substantially improves ranking effectiveness."
          ],
          "keyword": [
            "Natural language processing (Computer science)",
            "Deep learning (Machine learning)",
            "Large Language Models",
            "Information retrieval"
          ],
          "primary_title": "Improving Information Retrieval through Contextual Ranking with Large Language Models",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:caa8g3xc/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:caa8g3xc/"
        },
        {
          "pid": "bdr:vpgen8xp",
          "object_type": "pdf",
          "abstract": [
            "Conversational assistive robots have the potential to help human users accomplish a wide range of daily tasks, such as cooking meals, performing exercises, or operating machines. However, for the robot to interact effectively, it must understand the state of the world and the user's goals through their interactions with the environment over extended durations. The primary challenge is the uncertainty involved in estimating three critical factors: modeling the world, understanding the user, and defining the task. Due to the robot's limited ability to discern human intentions, noisy sensors, and variations in human activity execution, the perception of the environment, user, and tasks can be limited. This problem compounds as the robot accumulates more noisy observations about the user and environment over extended periods. My dissertation bridges the gap in enabling the robot to engage in accurate real-time inference and modeling of the world, user, and task state over extended periods, spanning days to weeks and months. In the first part of my dissertation, I focus on modeling and inferring the state of the world, user, and tasks within multi-step, long-horizon tasks. Our research enables robots to handle uncertainty effectively for tasks extending over minutes of interaction. Subsequently, I delve into my research on the world state estimation of objects within the environment over several days of interaction. Finally, I demonstrate progress by validating the deployment of Human-Robot Interaction (HRI) features through simulated human behavior spanning days to weeks. This process helps assess the long-term efficacy of such features. In my future work, I plan to extend this research to several months of human-robot interaction time. In conclusion, my research equips personal social robots with enhanced situational awareness for long-term social interactions."
          ],
          "keyword": [
            "human-robot interaction",
            "Robotics",
            "Social Robots"
          ],
          "primary_title": "Long-Term Autonomy in Socially Assistive Robots",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:vpgen8xp/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:vpgen8xp/"
        },
        {
          "pid": "bdr:z4vf28zy",
          "object_type": "pdf",
          "abstract": [
            "This dissertation studies the canonical tasks of database compression and similarity search, and demonstrates how advanced deep learning models can be used to develop effective solutions that advance the state-of-the-art approaches. We first tackle the issue of relational-data compression, leveraging primarily the high-level value correlations among the columns of relational tables to achieve lossy compression with application-defined error bounds (DeepSqueeze). We then study JSON-data compression by learning the inherent hierarchical patterns in JSON documents, and using these patterns to achieve lossless compression (JPress). Finally, we introduce methods to learn user-specific notions of similarity to support custom search in time-series data (Flexim)."
          ],
          "keyword": [
            "Data compression (Computer science)",
            "Deep Learning",
            "pattern similarity",
            "Time series search",
            "Semantic compression"
          ],
          "primary_title": "Deep Models for Database Compression and Search",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:z4vf28zy/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:z4vf28zy/"
        },
        {
          "pid": "bdr:jcphtbmm",
          "object_type": "pdf",
          "keyword": [
            "Computer networks"
          ],
          "primary_title": "CDPlane: A Dynamic Content Distribution Plane for the Next Billion Users.",
          "abstract": [
            "The digitization of the economy has had a revolutionary impact on society, and, today, people use digital web services to conduct essential daily activities, such as business, work, education, or communication. Consequently, the performance and availability of a web service has a direct impact on society, ranging from impacting business revenue to affecting people’s quality of life. To ensure performant delivery of service workloads (e.g., website code, media objects) and robust availability, service providers typically use Content Distribution Networks (CDNs) that operate geographically distributed server deployments to connect vast areas of the world. At a high-level, a CDN’s web stack comprises several layers, ranging from service code (e.g., HTTP web), networking layers (e.g., transport and application protocols), to the client-side applications (e.g., website JavaScript processed by a generic browser). While maximizing the performance and availability is a key goal, CDNs also aim to minimize the complexity of their infrastructures and traditionally use generalizable solutions, such as same network stack or website design for different services or users, to reduce the CDN operator and web developer effort cost of designing and optimizing the various layers. Today, Hyperscalers, such as planet-scale CDN infrastructures from Meta, Google and Microsoft, provide diverse web services to billions of users across the world and, with the growth in the scale, a new set of challenges have emerged towards maximizing the performance and availability goals. Hyperscalers are expected to host tens of services with diverse natures (e.g., TCP, QUIC, publish/subscribe), and serve a heterogeneous user-base with diverse last-mile connectivity and device types. Further, the service code experiences a high churn rate and is updated up to tens of times a day. These dimensions directly impact the degree of user satisfaction, commonly called Quality-of-Experience (QoE), as web performance is sensitive to network and devices, while minimizing downtime during service restarts is sensitive to the nature of their protocols. While this heterogeneity is the product of end-user’s choice of connectivity/device and developer’s choice of service/website design; from the CDN’s perspective, the diverse user population and the different implementation of the services end up sharing the same web server stack, network and device resources. Yet, the CDN is required to be consistently performant and available, while maintaining the simplicity of its infrastructure. Effectively, with the limited set of choices at the server-side, i.e., a server stack that can make different choices for the different web stack layers, the CDN is required to optimize QoE for the vast range of heterogeneity that exists outside its purview. Inherently, a tussle exists between the two contrasting goals of maximizing “QoE” and managing “least-cost”, defined as the smallest number of choices a CDN can make to improve QoE without introducing additional overheads. For instance, while the least-cost approach might be to make no choice and simply used a generalized stack, it may not address the domain-specific needs of the diverse user-base and result in suboptimal QoE. On the other extreme, a CDN may strive to make the optimal domain-specific choices for every individual connection, ranging from tuning the networking stack, adding domain specific enhancements to manage the consistency of diverse protocols during service updates, to re-designing the website to optimize their resource usage. However, a lack of algorithmic, protocol and system support in the traditional CDN design hinders the practicality of introducing such dynamic flexibility in a low-cost manner: (i) lack of algorithmic support to address the network volatility and the high last-mile/device dimensionality of the Internet makes it challenging to automatically tune protocols in a principled and low-overhead manner, (ii) limitations with kernel and protocols to address the domain-specific needs of non-TCP protocols make it challenging to prevent disruptions for the diverse services, and (iii) limitations with the existing analytics techniques to understand the client-side resource dynamics make it challenging for the developers to identify the key targets to improve in their website design. In light of these challenges, the space of making the right design choice for the different layers is vast, contributing to the high-cost of improving the QoE for the diverse masses. This dissertation explores the CDN design space to improve the QoE vs least-cost trade-off and proposes “CDPlane: a collection of data-path and control-path components for a flexible CDN”. While a CDN may not have direct control over the source of heterogeneity, the eventual impact of heterogeneity manifests itself in the form of different types of states across the layers, e.g., transport network properties of a connection, application-level state for diverse services, client-side resource allocations for the page load. CDPlane proposes algorithmic, systems, and analytics design components that extract and leverage the state to (i) auto-tune network stack for diverse users by selecting the most suitable protocol configurations based on a connection’s transport and application state, (ii) propose system and protocol abstractions to preserve the connection state during the application update restarts, and (iii) propose a measurement technique to gain memory state visibility into the JavaScript’s interactions with browser layers to identify the key culprits behind a website design’s memory overhead. While the domain-specific design of the algorithm to combat network dynamics eliminates the cost of manually curating networking stack configurations, the proposed system and protocol abstractions for disruption-free updates only extend the traditional designs and do not require a specialized redesign of services. Further, the analytics technique directly identifies the source of memory bloat and uncovers potential avenues for optimizations, reducing the developer effort required to inspect the large space of JavaScript functions, libraries, and their interactions with the browser components. Taken together, these building blocks extend the traditional design to improve QoE for the diverse services and user-base, and optimize the tussle in a low-cost manner."
          ],
          "uri": "https://repository.library.brown.edu/studio/item/bdr:jcphtbmm/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:jcphtbmm/"
        },
        {
          "pid": "bdr:vtz27y22",
          "object_type": "pdf",
          "abstract": [
            "Computational models of language that are grounded in some world can learn the meaning of words in the world they exist in. A typical goal in grounded language learning is to learn to map language to some component of an agent's decision process, for example, goals or rewards. This thesis puts forth a number of advances in methods that take in natural language, and learn to ground it to components of decision processes in reinforcement learning tasks. Unlike previous work, the models we build ground natural language to symbolic representations that can integrate with all components of an agents model, exploring the benefits of different types of symbolic languages that serve as an intermediate representation. Further, we explore how we might transfer grounded knowledge to pretrained text-only models while keeping the previously learned textual knowledge and parameters intact. The primary contributions of this thesis are to develop models that 1) convert natural language instructions to symbolic task representations that allow planning with temporal constraints, 2) allow different parts of natural language to update different parts of a reinforcement learning system and 3) allow us to transfer ungrounded textual models to learn grounded concepts. This thesis concludes by discussing the steps forward which outline the goal of grounding language models in world models."
          ],
          "keyword": [
            "nature language processing"
          ],
          "primary_title": "Grounding Language Models in World Models",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:vtz27y22/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:vtz27y22/"
        },
        {
          "pid": "bdr:6zu72eyk",
          "object_type": "pdf",
          "abstract": [
            "Over the last decade, Python emerged as the language of choice for data processing and model building. Yet, the developer productivity of Python comes at the cost of slow execution speed and high demand for resources. Compiling Python to efficient, optimized machine code could help, but writing a general-purpose Python compiler is inherently difficult. Python’s dynamic typing, dynamic dispatch, and object format all impede standard compiler optimizations, and efforts to write compilers over two decades have failed to produce substantial speedups for practical data science workloads. This dissertation introduces two specialized compilers that leverage the high-level structure of a query to compile Python to efficient code for parallel data science workloads. The first exploits the domain-specific program structure of data science pipelines and generates code specialized to a sample of the input data, which allows making assumptions about types and common-case behavior. This results in a novel data-driven compilation approach that produces highly efficient machine code. Our prototype data analytics system, Tuplex, demonstrates that data-driven compilation beats state-of-the-art systems by a factor of 5× to 91× on realistic workloads. We then explore the idea of hyperspecialization, which generates custom code for disjoint data partitions to avoid sampling errors and improve overall efficiency for heterogeneous datasets. In a distributed setting using serverless functions to avoid sampling errors and improve overall efficiency. Building on Tuplex, we show that this next-generation system, Viton, benefits from hyperspecialization for realistic workloads and reduces cost and runtime by a factor of 2× to 3×. Viton uses serverless Lambda functions to achieve massive parallelism at low latencies together with a more advanced data-driven compilation approach backed by hyperspecialization."
          ],
          "keyword": [
            "Big data",
            "Query Processing",
            "data science",
            "Data analytics",
            "Compilers (Computer programs)"
          ],
          "primary_title": "Efficient Data Analytics Using Speculative Compilation Techniques",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:6zu72eyk/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:6zu72eyk/"
        },
        {
          "pid": "bdr:wpyz5bhg",
          "object_type": "pdf",
          "abstract": [
            "At present, there are many blockchains, each competing with the others to gain market share and dominance. It is expected that multiple blockchains will continue to coexist in the future. In this blockchain universe, no single chain will rule them all, making the ability of those chains to communicate become essential. Particularly, as people utilize multiple blockchains and engage in cross-chain transactions, it becomes crucial to ensure their feasibility. This thesis explores methods for enabling cross-chain transactions, including the following key contributions: (1) proposing protocols for people to trade across multiple blockchains and protect participants’ financial interests if there exist uncooperative participants; (2) designing protocols for fault-tolerant cross-chain transactions such that even if some participants become uncooperative, the trade can still succeed; (3) providing protocols to trade cross-chain options; (4) automating the process of designing cross-chain protocols for cross-chain transactions."
          ],
          "keyword": [
            "Blockchains (Databases)"
          ],
          "primary_title": "Enabling Cross-Chain Transactions",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:wpyz5bhg/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:wpyz5bhg/"
        },
        {
          "pid": "bdr:f4dafjdd",
          "object_type": "pdf",
          "abstract": [
            "Decentralized finance (or DeFi) has become a booming area of applied distributed computing. Over the course of the last several years, many mechanisms and distributed protocols have emerged as the backend for new financial applications in the DeFi setting. One of the most popular mechanisms, an automated market maker (AMM), is an automaton that has custody of several pools of assets, and is capable of trading those assets with clients at rates set by a mathematical formula. Unlike traditional “order book” traders, AMMs trade directly with clients, and do not need to match up (and wait for) compatible buyers and sellers. Today, AMMs such as Uniswap, Bancor, and others have become one of the most popular ways to trade electronic assets on a single blockchain. Additionally, for assets managed across distinct blockhains (e.g. trading ETH for BTC), many cross-chain protocols have been proposed to synchronize multiple blockchain to ensure reasonable trade outcomes for parties involved. These have become the main vehicles for mutually distrusting parties to seamlessly exchange assets between blockchains. The goal of this thesis is to develop a mathematical theory that can serve as a guide to understand to what extent AMMs and cross-chain protocols can be used to solve various trade problems arising in the decentralized financial setting. This thesis introduces: (1) An axiomatic characterization of desirable properties any AMM should satisfy. (2) The first definition of what it means to compose AMMs in an appropriate way, illustrating both the expressive power and limitations of networks of AMMs. (3) A discussion of the tradeoffs an AMM design imposes on both the traders and AMM investors, or liquidity providers. (4) Several novel cross-chain protocols for trading financial options. (5) A consensus hierarchy for the cross-chain synchronization setting."
          ],
          "keyword": [
            "Distributed Computing",
            "Game theory"
          ],
          "primary_title": "Trading Mechanisms in an Adversarial Economy",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:f4dafjdd/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:f4dafjdd/"
        },
        {
          "pid": "bdr:7sfgkb7m",
          "object_type": "pdf",
          "abstract": [
            "For machines to communicate naturally with humans in the real world, they need to connect the meaning of words to objects and actions in the world. This includes verbs like toss vs. throw and slide vs. roll, for which there is a nuanced difference in the physical dynamics of the verb. Ideally, in the future, a robot would be able to understand this difference, as a human would. So, to what extent do existing visually-grounded models capture this difference? What about models that learn from ground-truth trajectory data, i.e. the positions and rotations of objects over time? This thesis investigates these questions. The primary contributions of this work are 1) developing two virtual environments that allow parallel trajectory and visual data collection, 2) building models that represent verbs in terms of trajectory data, and 3) comparing these representations to existing visually-grounded representations, giving insight into how future models may understand physical dynamics of verb meaning, which may then be applied to downstream tasks like instruction following. This thesis finds that while trajectory-based models are able to successfully capture physical dynamics of verb meaning, visual inputs are able to do so comparably well, suggesting that conventional visual modalities are sufficient for capturing verb meaning. However, it isn’t clear whether the picture will remain the same with increased scale, pointing toward a need for further research."
          ],
          "keyword": [
            "Natural language processing (Computer science)"
          ],
          "primary_title": "Modeling Verb Meaning with Trajectories",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:7sfgkb7m/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:7sfgkb7m/"
        },
        {
          "pid": "bdr:p27ge32g",
          "object_type": "pdf",
          "abstract": [
            "Indoor scenes and the shapes that comprise them play a central role in our daily experiences. As virtual 3D representations of these environments become increasingly important, the value of generative models capable of producing new 3D shapes and scenes also grows. In this dissertation, we explore the possibilities of combining deep learning with large datasets of shapes and scenes to create powerful generative models of 3D shapes and scenes. We focus on autoregressive generative models, models that sequentially predict the individual components until completion. We make the following key contributions: (1) we formulate autoregressive generation of shapes, room-level scenes, and floor-level scenes: representing inputs, factorizing outputs, combining individual components and improving compatibility between these components; (2) we introduce a two-level paradigm that separate high-level semantics from low-level details, making the models more interpretable and controllable; (3) we explore strategies to encourage these models to learn more generalizable rules. Together, these contributions enable the design of generative models that generate 3D shapes and scenes with better quality, complexity, diversity, and controllability."
          ],
          "keyword": [
            "Artificial intelligence",
            "Neural networks (Computer science)",
            "Computer graphics",
            "Generative Models",
            "Probabilistic Reasoning",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Learning Autoregressive Generative Models of 3D Shapes and Scenes",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:p27ge32g/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:p27ge32g/"
        },
        {
          "pid": "bdr:ran6bq93",
          "object_type": "pdf",
          "abstract": [
            "In the era of big data, distributed data stores have become an indispensable necessity. Almost all these data stores store sensitive data such as our voicemails, messages, emails, shopping data, health records, etc. Unfortunately, with an increasing number of data breaches on a regular basis, privacy has become a major concern. Encryption is often proposed as a solution to privacy issues, however, naively applied encryption leads to brittle systems that fail to provide satisfactory privacy guarantees; it is only through careful system-wide analysis that complicated systems can achieve provable privacy. This thesis formalizes the use of end-to-end encryption (where data is kept encrypted at all times) in distributed hash tables (DHTs) and key-value stores (KVSs). Both are fundamental in the design of storage systems we use today. For example, Amazon’s Dynamo KVS underlies its shopping cart, Facebook’s Cassandra KVS supports Messenger and Google’s Bigtable KVS manages data in Gmail. Integrating end-to-end encryption into these basic building blocks would therefore allow us to support privacy-preserving systems which would greatly increase the confidentiality of our data. In particular, we introduce the notion of encrypted DHTs and encrypted KVSs and provide security definitions that capture the security properties one would desire from such encrypted systems. We then isolate the key properties (such as load balancing, equivocation) needed from the plaintext DHTs and KVSs to have secure constructions. Finally, we give constructions of encrypted DHTs and encrypted KVSs and formally analyze their security under our security definitions. Also, to show that the required properties are indeed achievable in practice, we study common DHTs and KVSs and show that they satisfy all the requirements."
          ],
          "keyword": [
            "Secure Distributed Hash Tables"
          ],
          "primary_title": "Encrypted Distributed Storage Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ran6bq93/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ran6bq93/"
        },
        {
          "pid": "bdr:b8zx4qdu",
          "object_type": "pdf",
          "keyword": [
            "Artificial intelligence",
            "Autonomous robots",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Learning Task-Specific Grasps",
          "abstract": [
            "Grasping is one of the most important open problems in robotics; the very point of a robot is to exert force on the world to achieve a goal, and most such exertions require the robot to execute a grasp first. For a home robot to be effective, it must load a dishwasher with breakable plates; for a repair robot to be effective, it must operate tools; for a caretaker robot to be effective, it must perform chores for those with illnesses. All of these activities require manipulating objects, which in turn requires grasping them effectively. Additionally, to be useful, the robot must be able to perform these tasks on objects it has never seen before, in applications where manipulation failures can be very costly. Deploying a robot to such an environment, where exact operating conditions are unknown and vary between instances, is therefore challenging because systems and algorithms developed in a lab may perform poorly when introduced to a novel environment. A robot must quickly learn to manipulate new objects it encounters using limited prior knowledge. In this dissertation, I examine robot grasping in three contexts. First, I propose a general grasp detection system that enables a multi-finger gripper to use multiple types of grasps to pick objects of varying sizes from dense clutter. For example, precision grasps are necessary for precisely picking small objects from the surface of a table using fingertips, while power grasps stably hold large objects by enveloping them with the gripper’s fingers. Given a visual representation of the scene, the system proposes a set of potential candidate grasp poses. These poses are evaluated using a neural network model that takes as input point clouds centered at a grasp pose and returns the probabilities that a grasp of each type would succeed at the given pose. This system is trained using a dataset generated in simulation and evaluated on a real robot. Explicitly modeling grasp type boosted the system’s object removal rate by 8.5% over the highest performing baseline. Next, I propose a framework for specializing a generic grasp detector to a task-oriented grasp detector. A generic grasp detector detects a stable grasp, which is sufficient for picking up an object but may not be sufficient for manipulating it. For example, a stable grasp very close to the fulcrum of a door handle will make it hard to turn, while grasping far from the fulcrum will make it easier. A task-oriented grasp detector is a classifier that predicts which grasp poses serve as initial states that enable a given manipulation controller to complete a task. As these classifiers are instance dependent, they cannot be trained in simulation and transferred to the real world. Instead, they must be trained directly in the task for which they are required. To this end, I introduce the Augmented Task-Oriented Grasp Detection Network (ATOG), which learns to predict which grasp poses allow a robot to successfully manipulate an object from a single-digit-sized training set. ATOG achieves this via a deep architecture built on an existing network that has been pre-trained to predict general grasp stability. Given a partial point cloud containing the local geometry around a grasp pose and the pose’s relation to the object, ATOG predicts whether the grasp will enable the robot to successfully execute a motor skill. I evaluate ATOG in four simulated domains; it outperforms the nearest baseline by up to 6.5%. Finally, I propose a learning algorithm that learns a task-oriented grasp detector for a given task while simultaneously learning the manipulation policy that the grasp must enable. Learning a policy to control a robot to perform a specific task is difficult because of the large action space and potentially sparse reward signal. This learning process can be simplified by bootstrapping the policy with a grasp controller and learning after a grasp has been executed. For instance, a robot would execute a grasp on the back side of the handle of a hammer, then learn a control policy that raised the hammer over a nail and struck the head down onto the nail. Though bootstrapping with a grasp controller simplifies the policy learning process, a task-oriented grasp classifier still must learn which grasp poses enable the policy to succeed. This joint learning problem is challenging due to the entanglement between the task-oriented grasp detector and the manipulation policy, which changes over time as it is learned; selecting different grasps changes the initial states of the manipulation policy, while a grasp pose that one policy fails the task from could enable an updated policy to complete the task. My proposed Grasp-Aware Reinforcement-Learning Agent (GARLA) overcomes a key obstacle to robot learning with grasping, enabling a robot to quickly learn both how to manipulate an object and where to grasp the object to begin the manipulation. With GARLA, a robot could be deployed to a novel environment and learn to manipulate novel objects within a small number of attempts."
          ],
          "uri": "https://repository.library.brown.edu/studio/item/bdr:b8zx4qdu/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:b8zx4qdu/"
        },
        {
          "pid": "bdr:nyajbec3",
          "object_type": "pdf",
          "abstract": [
            "Future collaborative robots must be capable of finding objects. As such a fundamental skill, we expect object search to eventually become an off-the-shelf capability for any robot, similar to e.g., object detection, SLAM, and motion planning. However, existing approaches either make unrealistic compromises (e.g., reduce the problem from 3D to 2D), resort to ad-hoc, greedy search strategies, or attempt to learn end-to-end policies in simulation that are yet to generalize across real robots and environments. This thesis argues that through using Partially Observable Markov Decision Processes (POMDPs) to model object search while exploiting structures in the human world (e.g., octrees,correlations) and in human-robot interaction (e.g., spatial language), a practical and effective system for generalized object search can be achieved. In support of this argument, I develop methods and systems for (multi-)object search in 3D environments under uncertainty due to limited field of view,occlusion, noisy, unreliable detectors, spatial correlations between objects,and possibly ambiguous spatial language (e.g., \"The red car is behind Chase Bank\"). Besides evaluation in simulators such as PyGame, AirSim, and AI2-THOR, I design and implement a robot-independent, environment-agnostic system for generalized object search in 3D and deploy it on the Boston Dynamics Spot robot, the Kinova MOVO robot, and the Universal Robots UR5e robotic arm, to perform object search in different environments. The system enables, for example, a Spot robot to find a toy cat hidden underneath a couch in a kitchen area in under one minute. This thesis also broadly surveys the object search literature,proposing taxonomies in object search problem settings, methods and systems."
          ],
          "keyword": [
            "Machine Learning",
            "Decision making",
            "Reinforcement learning",
            "human-robot interaction",
            "Artificial intelligence",
            "Computational science and engineering",
            "Robotics",
            "Computer science",
            "Automation",
            "Sequential Decision Making",
            "POMDP",
            "Partially Observable Markov Decision Proess",
            "Robot Planning",
            "Robots--Control systems--Planning",
            "Object Search",
            "Generalized Artificial Intelligence",
            "Generalized Robotics",
            "Systems engineering",
            "Robotic Systems",
            "Autonomous robots",
            "Target Search"
          ],
          "primary_title": "Generalized Object Search",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:nyajbec3/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:nyajbec3/"
        },
        {
          "pid": "bdr:11078",
          "object_type": "pdf",
          "abstract": [
            "Transactional memory (TM) has become increasingly popular in recent years as a promising programming paradigm for writing correct and scalable concurrent programs. Despite its popularity, there has been very little work on how to debug and profile transactional programs. This dissertation addresses this situation by exploring the debugging and profiling needs of transactional programs, explaining how the tools should change to support these needs, and implementing preliminary infrastructure to support this change."
          ],
          "keyword": [
            "Concurrent programming",
            "transactional memory",
            "debugging",
            "performance profiling",
            "Debugging in computer science--Computer programs"
          ],
          "primary_title": "Debugging and Profiling of Transactional Programs",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11078/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11078/"
        },
        {
          "pid": "bdr:11123",
          "object_type": "pdf",
          "abstract": [
            "Peer-to-peer (P2P) systems, and client-server type storage and computation outsourcing constitute some of the major applications that the next generation cloud schemes will address. Since these applications are just emerging, it is the perfect time to design them with security and privacy in mind. Furthermore, considering the high-churn characteristics of such systems, the cryptographic protocols employed must be efficient and scalable. This thesis shows that cryptography can be used to efficiently and scalably provide security and privacy for the next generation cloud systems.We start by describing an efficient and scalable fair exchange protocol that can be used for exchanging files between participants of a P2P file sharing system. In this system, there are two central authorities that we introduce: the arbiter and the bank. We then try distributing these entities to reduce trust assumptions and to improve performance. Our work on distributing the arbiter leads to impossibility results, whereas our work on distributing the bank leads to a more general cloud computation result showing how a boss can employ untrusted contractors, and fine or reward them. We then consider cloud storage scenario, where the client outsources storage of her files to an untrusted server. We show how the client can challenge the server to prove that her file is kept intact, even when the files are dynamic. Next, we provide an agreement protocol for a dynamic message, where two parties agree on the latest version of a message that changes over time. We then apply this agreement protocol to the cloud storage setting and show how a judge can arbitrate between the client and the server officially based on the agreed-upon message and the proof sent by the server. Lastly, we show that all our solutions are efficient and scalable by presenting results from the cryptographic library we implemented."
          ],
          "keyword": [
            "security",
            "peer-to-peer networks",
            "electronic cash",
            "Cryptography",
            "Computer security",
            "Privacy",
            "Cloud computing",
            "Peer-to-peer architecture (Computer networks)",
            "Stored-value cards"
          ],
          "primary_title": "Efficient Cryptography for the Next Generation Secure Cloud",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11123/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11123/"
        },
        {
          "pid": "bdr:11230",
          "object_type": "pdf",
          "abstract": [
            "A well-written text follows an overall structure, with each sentence following naturally from the ones before and leading into the ones which come afterwards. We call this structure \"coherence\"; without it, a document becomes a confusing series of non sequiturs. Understanding the principles that make a text coherent is an important goal of natural language processing. These principles can be applied to the design of systems that create new documents, like summaries, or make changes to existing documents.Coherence is a universal principle of language, but typical approaches to evaluation focus on the application of multidocument summarization. We test the generality of our models by applying them to a new task, chat disentanglement, in which we distinguish independent conversational threads in a crowded chat room. To study this task, we create our own corpus and evaluation metrics, propose a baseline model with basic coherence features, and then test the performance of our own and others' more sophisticated models of local coherence.We present evidence that despite the significant differences between this task setting and conventional summarization-inspired evaluations, many of these models generalize fairly well, improving over the baseline. Problems with lexicalized models are mostly the fault of insufficient in-domain training data, rather than representing weaknesses in the models themselves. Thus we conclude that many of the same basic principles are used to create coherence throughout English discourse, and that simple local models can be used to describe them."
          ],
          "keyword": [
            "discourse",
            "coherence",
            "internet chat",
            "Natural language processing",
            "Written communication"
          ],
          "primary_title": "Generalizing Local Coherence Modeling",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11230/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11230/"
        },
        {
          "pid": "bdr:25",
          "object_type": "pdf",
          "abstract": [
            "Recently, there has been significant interest in applications where high-volume, continuous data streams need to be processed with low latency. Such applications include financial market monitoring, network monitoring, intrusion detection, call analysis, battlefield monitoring, asset tracking, and ecosystem monitoring. Since these applications monitor real-time events, the value of a result decays rapidly over time. Therefore, low-latency processing is a key requirement. Stream processing systems enable efficient implementation of the aforementioned applications. Currently, many such systems are geared toward distributed processing because a large number of applications inherently involve geographically dispersed data sources and the processing capability of a system improves as more servers are used. However, the more computation and communication resources, the higher the odds of failure. In stream processing, a failure prevents low-latency processing because it blocks the flow of data streams. To make matters worse, it may also result in losing data essential to producing correct results. In this dissertation, we propose various techniques that realize both reliable and timely processing of data streams in the face of server and network failures. We first discuss our basic recovery approaches, while comparing them in terms of recovery speed, CPU and network utilization, as well as their relationship to various recovery semantics. Next, we describe a fast recovery technique for commodity server clusters. In this technique, operators on each server are backed up on different servers and thus can be recovered in parallel. This technique assigns backup servers and schedules checkpoints in a manner that maximizes the recovery speed. Finally, we discuss our approach for Internet-scale stream processing. In this approach, multiple operator replicas send outputs to downstream replicas, allowing each replica to use whichever data arrives first. To further reduce latency, replicas run without coordination, possibly processing data in different orders. Despite this relaxation, the approach guarantees that applications always receive the same results as in the non-replicated, failure-free case. It also deploys replicas at locations that effectively improve performance and availability. Our experimental results demonstrate the effectiveness of the approaches above. These results were obtained from a server cluster at Brown University and a worldwide network testbed called PlanetLab."
          ],
          "keyword": [
            "Stream Processing",
            "High Availability",
            "Fault Tolerance",
            "Networking",
            "Replication",
            "Fault tolerance (Engineering)"
          ],
          "primary_title": "Fast and Highly-Available Stream Processing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:25/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:25/"
        },
        {
          "pid": "bdr:240",
          "object_type": "pdf",
          "abstract": [
            "Current efforts in syntactic parsing are largely data-driven. These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures. Labeled data typically requires expert annotators which makes it both time consuming and costly to produce. Furthermore, once training data has been created for one textual domain, portability to similar domains is limited. This domain-dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain. The simplest approach to this task is to assume that the target domain is essentially the same as the source domain. No additional knowledge about the target domain is included. A more realistic approach assumes that only raw text from the target domain is available. This assumption lends itself well to semi-supervised learning methods since these utilize both labeled and unlabeled examples. This dissertation focuses on a family of semi-supervised methods called self-training. Self-training creates semi-supervised learners from existing supervised learners with minimal effort. We first show results on self-training for constituency parsing within a single domain. While self-training has failed here in the past, we present a simple modification which allows it to succeed, producing state-of-the-art results for English constituency parsing. Next, we show how self-training is beneficial when parsing across domains and helps further when raw text is available from the target domain. One of the remaining issues is that one must choose a training corpus appropriate for the target domain or performance may be severely impaired. Humans can do this in some situations, but this strategy becomes less practical as we approach larger data sets. We present a technique, Any Domain Parsing, which automatically detects useful source domains and mixes them together to produce a customized parsing model. The resulting models perform almost as well as the best seen parsing models (oracle) for each target domain. As a result, we have a fully automatic syntactic constituency parser which can produce high-quality parses for all types of text, regardless of domain."
          ],
          "keyword": [
            "constituency parsing",
            "self-training",
            "semi-supervised methods",
            "domain adaptation"
          ],
          "primary_title": "Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:240/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:240/"
        },
        {
          "pid": "bdr:109",
          "object_type": "pdf",
          "abstract": [
            "The need to manage uncertain data arises in many applications. Some examples include data cleaning, data integration, sensor networks, pervasive computing, and scientific data management. Uncertainty can arise in both measured and predicted values, thus, requiring a statistical treatment. Predictive queries based on historic data are also useful in a range of domains like proactive system management, inventory planning, adaptive query processing, sensor data management, and financial planning. In this thesis, I describe a series of techniques that I developed to answer various kinds of queries, including JOIN and top-k queries, on uncertain data. I also explore methods for effectively processing predictive queries. For example, I discuss the use of skip-lists for building models that answer predictive queries on different time horizons. These techniques are instrumental for a workable architecture for query processing on uncertain data. I show experimental results that indicate that these techniques are practical."
          ],
          "keyword": [
            "data management",
            "uncertain data",
            "distribution",
            "query processing",
            "top-k queries",
            "predictive queries",
            "Database management"
          ],
          "primary_title": "Query Processing on Uncertain Data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:109/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:109/"
        },
        {
          "pid": "bdr:202",
          "object_type": "pdf",
          "abstract": [
            "We apply computer science techniques to try to solve a selection of problems that arise in economics and electronic commerce. The problems we address and our results are summarized below. The first problem is from the field of Mechanism Design. The goal is to find a procedure for allocating identical items among agents with private values in the manner that maximizes the total utility of the agents. We approach this problem computationally: solutions are found algorithmically rather than through mathematical derivations. Our computational approach yields a nearly optimal solution greatly improving prior results. In the case with 3 agents and 2 items, we were able to find a provably optimal solution. Next, we address a game-theoretic problem of finding Nash Equilibria in auctions. We investigate when a computational procedure finds an equilibrium in first and second price auctions with discrete bids and values. The rest of the thesis is devoted to automated decision making in electronic commerce domains. Three domains are considered: sponsored search, supply chain management, and simultaneous auctions. The last two domains are studied in the context of the SCM and Travel divisions of the Trading Agent Competition (TAC). Our contributions to automated decision making are both practical and theoretical. On the practical side, the bidding strategy we designed for sponsored search auctions is currently being used by a large advertiser. Our work on TAC Travel culminated in winning the competition in 2006. In the TAC SCM competition, the agent we built was among the top 5 out of over 20 agents almost every year of the competition. For theoretical contributions, we characterized optimal strategies for bidding in simultaneous auctions when prices are known and complemented this analysis with an empirical comparison of different strategies. We identified that bidding decisions in TAC SCM can be modeled as a non-linear knapsack problem and proved the asymptotic optimality of a greedy algorithm for solving a class of non-linear knapsack problems."
          ],
          "keyword": [
            "mechanism design",
            "automated decision making",
            "trading agents",
            "Game theory"
          ],
          "primary_title": "Select Problems at the Intersection of Computer Science and Economics",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:202/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:202/"
        },
        {
          "pid": "bdr:206",
          "object_type": "pdf",
          "abstract": [
            "Information retrieval (IR) has become a ubiquitous technology for quickly and easily finding information on a given topic amidst the wealth of digital content available today. This dissertation addresses search for written and spoken natural language documents, including news articles, Web pages, and spoken interviews. Effective model estimation is identified as a key problem, and several novel estimation techniques are presented and shown to significantly enhance search accuracy. While search is typically performed via a few carefully chosen keywords, formulating effective keyword queries is often unintuitive and iterative, particularly when seeking complex information. As an alternative to keyword search, this dissertation investigates search using ``natural'' queries, such as questions or sentences a person might naturally articulate in communicating their information need to another person. By moving toward supporting natural queries, the communication burden is shifted from user query formulation to system interpretation of natural language. The challenge in enacting such a shift is enabling automatic IR systems to more effectively cope with natural language. To this end, several new estimation techniques for modeling natural queries are described. In comparison to a maximum likelihood baseline, 15-20% relative improvement in mean average precision (MAP) is demonstrated without use of query expansion. When an IR system discovers or is provided one or more feedback documents exemplifying a user's information need, there is further opportunity to improve search accuracy by exploiting document contents for query expansion. However, since documents typically discuss multiple topics varying in importance and relevance to any information need, the system must again be able to effectively interpret verbose natural language. Consequently, an estimation method for leveraging such documents is presented and shown to yield state-of-the-art search accuracy. Depending on the base model employed, 15-85% relative MAP improvement is achieved. When modeling higher-order lexical features or searching smaller document collections like cultural history archives, sparsity become particularly problematic for estimation. To cope with such sparsity, additional estimation methods are described which yield 5-20% relative improvement in MAP accuracy across varying conditions of query verbosity."
          ],
          "keyword": [
            "supervised learning",
            "estimation",
            "regression",
            "Information retrieval",
            "Natural language processing",
            "Machine learning"
          ],
          "primary_title": "Beyond keywords: finding information more accurately and easily using natural language",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:206/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:206/"
        },
        {
          "pid": "bdr:205",
          "object_type": "pdf",
          "abstract": [
            "We consider autonomous robots as having associated control policies that determine their actions in response to perceptions of the environment. Often, these controllers are explicitly transferred from a human via programmatic description or physical instantiation. Alternatively, Robot Learning from Demonstration (RLfD) can enable a robot to learn a policy from observing only demonstrations of the task itself. We focus on interactive, teleoperative teaching, where the user manually controls the robot and provides demonstrations while receiving learner feedback. With regression, the collected perception-actuation pairs are used to directly estimate the underlying policy mapping. This dissertation contributes an RLfD methodology for interactive, mixed-initiative learning of unknown tasks. The goal of the technique is to enable users to implicitly instantiate autonomous robot controllers that perform desired tasks as well as the demonstrator, as measured by task-specific metrics. With standard regression techniques, we show that such ``on-par'' learning is restricted to policies typified by a many-to-one mapping (a unimap) from perception to actuation. Thus, controllers representable as multi-state Finite State Machines (FSMs) and that exhibit a one-to-many mapping (a multimap) cannot be learnt. To be able to do so we must address the three issues of model selection (how many subtasks or FSM states), policy learning (for each subtask), and transitioning (between subtasks). Previous work in RLfD has assumed knowledge of the task decomposition and learned the subtask policies or the transitions between them in isolation. We instead address both model selection and policy learning simultaneously. Our presented technique uses an infinite mixture of experts and treats the multimap data from an FSM controller as being generated from overlapping unimaps. The algorithm automatically determines the number of unimap experts (model selection) and learns a unimap for each one (policy learning). On data from both synthetic and robot soccer multimaps we show that the discovered subtasks can be used (switched between) to reperform the original task. While not at the same level of skill as the demonstrator, the resulting approximations represent significant improvement over ones for the same tasks learned with unimap regression."
          ],
          "keyword": [
            "Regression",
            "Robotics",
            "Machine learning"
          ],
          "primary_title": "Teaching Old Dogs New Tricks: Incremental Multimap Regression for Interactive Robot Learning from Demonstration",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:205/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:205/"
        },
        {
          "pid": "bdr:268",
          "object_type": "pdf",
          "abstract": [
            "Dataflow programming provides an elegant model for computing with values that change over time. While many domain-specific dataflow languages have been developed, this dissertation presents a strategy for extending existing general-purpose call-by-value languages with a notion of dataflow evaluation. This strategy has been realized in the language FrTime, an extension of PLT Scheme that runs in the DrScheme programming environment. The primary challenges in developing FrTime have been to resolve interactions between its notion of dataflow and various features of PLT Scheme. This dissertation discusses the design, semantics, and implementation of FrTime, along with several applications."
          ],
          "keyword": [
            "dataflow programming",
            "higher-order functional programming"
          ],
          "primary_title": "Integrating Dataflow Evaluation into a Practical Higher-Order Call-by-Value Language",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:268/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:268/"
        },
        {
          "pid": "bdr:281",
          "object_type": "pdf",
          "abstract": [
            "Non-interactive zero-knowledge (NIZK) proofs can be an extremely powerful tool, allowing one to prove a statement in a single message without revealing any information besides the truth of the statement. Blum et al showed that NIZK proof systems exist for all languages in NP. However, in practice, NIZK proofs are rarely used, because existing protocols are extremely inefficient. Here we examine some useful languages for which we can give efficient proof system. We define two useful building blocks: one for proving that a message has been signed, and a second for proving that a value has been chosen according to a pseudorandom function. We give applications of these building blocks to anonymous credential systems, to electronic cash, and to the design of other efficient NIZK proofs systems."
          ],
          "keyword": [
            "cryptography zero-knowledge"
          ],
          "primary_title": "Efficient Non-Interactive Zero-Knowledge Proofs for Privacy Applications",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:281/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:281/"
        },
        {
          "pid": "bdr:283",
          "object_type": "pdf",
          "abstract": [
            "Regret-minimizing algorithms are an important class of machine learning algorithms that are closely related to game theoretic equilibria (e.g., Nash equilibria, correlated equilibria). In this work we present a general framework in which we derive a class of \"no-regret\" learning algorithms that converge to a corresponding set of equilibria. We show how to extend these algorithms to the more difficult naive setting. We also extend our framework to the case of convex games, providing the basis for a class of algorithms which can be exponentially faster than previously known convex algorithms. Finally, we apply our framework to the class of extensive-form games to derive the first class of algorithms that learn the set of extensive-form correlated equili"
          ],
          "keyword": [
            "equilibria",
            "Game theory",
            "Learning",
            "Regret"
          ],
          "primary_title": "No-Regret Learning and Game-Theoretic Equilibria",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:283/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:283/"
        },
        {
          "pid": "bdr:282",
          "object_type": "pdf",
          "abstract": [
            "In chemistry, molecules are drawn on paper and chalkboards as diagrams consisting of lines, letters, and symbols which represent not only the atoms and bonds in the molecules but concisely encode cues to the 3D geometry of the molecules. Recent efforts into pen-based input methods for chemistry software have made progress at allowing chemists to input 2D diagrams of molecules into a computer simply by drawing them on a digitizer tablet. However, the task of interpreting these parsed sketches into proper 3D models has been largely unsolved due to the difficulty in making the models satisfy both the natural properties of molecule structure and the geometric cues made explicit in the drawing. This dissertation presents a set of techniques developed to solve this model construction problem within the context of an educational application for chemistry students. Our primary contribution is a framework for combining molecular structure knowledge and molecule diagram understanding via augmenting molecular mechanics equations to include drawing-based penalty terms. Additionally, we present an algorithm for generating molecule models from drawn diagrams which leverages domain-specific and diagram-driven heuristics. These heuristics make our process fast and accurate enough for molecule diagram drawing to be used as an interactive technique for model construction on modern Tablet PC computers."
          ],
          "keyword": [
            "ChemPad",
            "ink-modified molecular mechanics",
            "conformation generation",
            "pen-based interfaces",
            "Tablet PC",
            "organic chemistry",
            "chemistry education",
            "Chemistry, Organic"
          ],
          "primary_title": "Interpretation of Molecule Conformations from Drawn Diagrams",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:282/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:282/"
        },
        {
          "pid": "bdr:11342",
          "object_type": "pdf",
          "abstract": [
            "Constraint Programming is a declarative programming paradigm for solving hard combinatorial problems. Its model consists of a set of decision variables, and a set of constraints specifying relations among variables. Variables can be any combinatorial objects. The model�s high degree of expressiveness enables solvers to exploit the problem semantics and achieve effective search. Constraint Programming over finite domain variables has been extensively studied since 80s. The research direction has been advanced to complex combinatorial objects, such as sets, graphs, and permutations.This thesis considers constraint programming over set variables, since sets are natural and fundamental combinatorial objects to model a wide range of configuration problems naturally. However, set variables raise fundamentally novel representation issues, since their domains often contain an exponential number of sets. How to represent this exponential number of sets and how to use this representation for pruning the search space effectively is the fundamental open issue in constraint programming over sets.We study the length-lex representation that was shown to offer some theoretical advantages over earlier attempts. The contribution is threefold: first, we illustrate that there exist generic, efficient, polynomial filtering algorithms over sets; second, we demonstrate that sometimes it is beneficial to shift some of the exponential behavior from the search component to the filtering components, and from the constraint-propagation algorithm to propagators, in which the constraint semantics is exploited; third, we show experimentally that the length-lex domain is efficient and robust. In particular, a prototypical implementation of length-lex is several orders of magnitude faster than earlier techniques on standard benchmarks."
          ],
          "keyword": [
            "Constraint Programming",
            "Constraint programming (Computer science)",
            "Combinatorial optimization",
            "Logic programming"
          ],
          "primary_title": "The Length-Lex Representation for Constraint Programming over Sets",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11342/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11342/"
        },
        {
          "pid": "bdr:11324",
          "object_type": "pdf",
          "abstract": [
            "Applications must embrace parallelism in order to increase performance on today's ubiquitous multicore processors. Unfortunately, writing correct parallel applications is notoriously hard, partly because the dominant parallel programming model uses threads with shared state and locks for synchronization, a model that is subject to a variety of subtle bugs that can be hard to reproduce.This dissertation advocates a programming model that enables the safe and incremental addition of parallelism to an application designed for serial execution. This model is enabled by a system composed of elyze, a static program analyzer, and multivent, a runtime scheduler. Together, elyze and multivent ensure that an application's code segments run in parallel only when they may do so safely, and guide the programmer in making changes to increase those opportunities.The system has been applied to two real world server applications written in C: thttpd, a web server, and tor, the onion router. thttpd shows an improvement in performance of up to 15%. An elaborate (and defective) thread pool mechanism can be removed from tor without compromising its performance. In both cases, the static and dynamic analyses performed by elyze and multivent guide the programmer in enabling significant parallelism and increased performance, without the need for complex reasoning about concurrent behavior."
          ],
          "keyword": [
            "concurrency",
            "parallelism",
            "safety",
            "Threads",
            "Synchronization"
          ],
          "primary_title": "Safe Parallelism for Servers",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11324/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11324/"
        },
        {
          "pid": "bdr:11185",
          "object_type": "pdf",
          "abstract": [
            "The human genome exhibits a rich structure resulting from a long history of genomic changes, including single base-pair mutations and larger scale rearrangements such as inversions, deletions, translocations, and duplications. The number and order of the genomic changes that resulted in the present-day human genome is not known, but can sometimes be inferred by comparison to the genomes of other species. In particular, genome rearrangements are modeled as operations on signed strings of characters representing blocks of conserved sequences. Genome rearrangement distance measures quantify the similarity between two or more genome sequences by counting the minimum, or most likely, number of rearrangement operations needed to transform one sequence into another. The development of efficient algorithms for computing genome rearrangement distances has been instrumental both in computing phylogenies for sets of known genetic sequences (such as gene families or the whole genomes of present-day species) and in constructing ancestral genome sequences.In this thesis, we develop algorithms to study recent genome rearrangements in human and cancer genomes. We introduce a novel measure, called duplication distance, to quantify the similarity between two genomic regions containing segmental duplications. We give an efficient algorithm to compute the duplication distance between a pair of signed strings and provide several generalizations of duplication distance that also measure inversions and deletions. We demonstrate the utility of the duplication distance measure in constructing the evolutionary history of segmental duplications in the human genome using both parsimony and likelihood techniques. Further, motivated by recent cancer genome sequencing studies, we present a new algorithm for the block ordering problem of inferring a whole genome sequence from a partial assembly by maximizing its similarity to another genome."
          ],
          "keyword": [
            "segmental duplications",
            "evolutionary reconstruction",
            "genome rearrangements",
            "Computer science",
            "Computational biology",
            "Comparative genomics",
            "Algorithms"
          ],
          "primary_title": "Algorithms for Analyzing Human Genome Rearrangements",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11185/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11185/"
        },
        {
          "pid": "bdr:11205",
          "object_type": "pdf",
          "abstract": [
            "Vehicle routing is a class of optimization problems where the objective is to find low cost delivery routes from depots to customers using vehicles of limited capacity. Vehicle routing problems generalize the traveling salesman problem and have many real world applications to businesses with high transportation costs such as waste removal companies, newspaper deliverers, and food and beverage distributors. We study two basic vehicle routing problems: unit demand routing and unsplittable demand routing. In the unit demand problem, items must be delivered from depots to customers using a vehicle of limited capacity and each customer requires delivery of a single item. The unsplittable demand problem is a generalization, where customers can have different demands for the number of items, but each customer's entire demand must be delivered all together by one route. Both problems are NP-Hard and do not admit better than constant factor approximation algorithms in the metric setting. However in many practical settings the input to the problem has Euclidean structure. We show how to exploit this to design arbitrarily good approximation algorithms. We design a quasi-polynomial time approximation scheme for the Euclidean unit demand problem in constant dimensions, and asymptotic polynomial time approximation schemes for the unsplittable demand problem in one dimension."
          ],
          "keyword": [
            "Geometric Approximation Algorithms",
            "Vehicle Routing",
            "Vehicle routing problem"
          ],
          "primary_title": "Approximation Schemes for Euclidean Vehicle Routing Problems.",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11205/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11205/"
        },
        {
          "pid": "bdr:11069",
          "object_type": "pdf",
          "abstract": [
            "Automating the process of measuring human shape characteristics and estimating body postures from images is central to many practical applications. While the problem is difficult in general, it can be made tractable by employing simplifying assumptions and relying on domain specific knowledge, or by engineering the environment appropriately.In this thesis we demonstrate that using a data-driven model of the human body supports the recovery of both human shape and articulated pose from images, and has many benefits over previous body models. Specifically, we represent the body using SCAPE, a low-dimensional, but detailed, parametric model of body shape and pose deformations. We show that the SCAPE parameters can be estimated directly from image data in a variety of imaging conditions and present a series of techniques enabled by this model.We first consider the case of multiple calibrated and synchronized camera views and assume the subject wears tight-fitting clothing. We define a cost function between image silhouettes and a hypothesized mesh and formulate the problem as an optimization over the body shape and pose parameters. Second, we relax the tight-fitting clothing assumption and develop a robust method that accounts for the fact that observed silhouettes of clothed people provide only weak constraints on the true shape. Our approach is to accumulate many weak silhouette constraints while observing the subject in various poses and combine them with strong constraints from regions detected as skin and with a prior expectation of typical shapes to infer the most likely shape under clothing. Third, we consider scenes with strong lighting and show that a point light source and the shadow of the body cast on the ground provide an additional view equivalent to a silhouette from an actual camera. This approach effectively reduces the number of cameras needed for successful recovery of the body model by taking advantage of the lighting information in the scene. Results on a novel database of thousands of images of clothed and \"naked\" subjects, as well as sequences from the HumanEva dataset, suggest these methods may be accurate enough for biometric shape analysis in video."
          ],
          "keyword": [
            "human pose and shape estimation",
            "scape",
            "multi-view geometry",
            "shape representation",
            "image-based modeling"
          ],
          "primary_title": "Detailed Human Shape and Pose from Images",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11069/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11069/"
        },
        {
          "pid": "bdr:11105",
          "object_type": "pdf",
          "abstract": [
            "Photolithography allows millions of identical computer chips to be efficiently produced by repeatedly shining light through a set of masks. Since the 1960's, this process has resulted in ever-smaller, ever-faster processors comprised of millions of logic gates all operating at astronomically high levels of reliability. Now that feature sizes have dipped below 90nms, however, the inherent physical limitations of light-based circuit manufacture have become increasingly apparent.In order to produce truly nanoscale logic, chemists have began investigating ''bottom up'' manufacturing technologies that no longer rely on patterns of light. Instead of producing chips from pre-made blueprints (i.e. masks), nanoscale architectures can potentially be ''grown'' by directing the stochastic assembly of millions, or billions, of individual nanoscale components. Although a range of promising nanoscale technologies have been demonstrated, this radically different assembly process introduces a host of design challenges.In nanoscale architectures, chip-to-chip variation is expected to increase substantially in terms of device placement, reliability and interconnect. As a result, emerging computing technologies necessitate fundamental changes in the way architectures are designed and analyzed. The significant uncertainty associated with the assembly and operation of nanoscale devices must not only be modeled and accounted for, but actively embraced as part of the design process. In stark contrast with today's technology, probabilistic modeling and analysis are primary requirements for the successful realization of nanoscale architectures.This thesis demonstrates a range of techniques for probabilistically analyzing nanowire crossbars, the current frontrunner for near-term nanoscale architectures. We apply these techniques to address a number of specific challenges associated with interfacing nanoscale wires with lithographically produced mesoscale wires via stochastically assembled nanowire decoders. We also consider the more general problem of performing reliable computations using unreliable logic gates. In this context, we discuss how arbitrary circuits can be made to tolerate transient gate faults by employing error-correcting codes. This approach, referred to as ''coded computation'' has the potential to allow for more efficient fault-tolerance than traditional modular redundancy."
          ],
          "keyword": [
            "Stochastic Assembly",
            "Probabilistic Analysis",
            "Fault-Tolerance",
            "Nanowire Decoders",
            "Coded Computation",
            "Nanotechnology",
            "Integrated circuits--Fault tolerance"
          ],
          "primary_title": "Reliable Computing at the Nanoscale",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11105/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11105/"
        },
        {
          "pid": "bdr:11333",
          "object_type": "pdf",
          "abstract": [
            "Cloud computing has emerged as an important new computational andstorage medium and is increasingly being adopted both by companies and individuals as a means of reducingoperational and maintenance costs. However, remotely-stored sensitive datamay be lost or modified and third-party computations may not be performed correctly due toerrors, opportunistic behavior, or malicious attacks. Thus, while thecloud is an attractive alternative to local trusted computationalresources, users need integrity guarantees in order to fully adopt this new paradigm. Specifically, they need tobe assured that uploaded data has not been altered and outsourced computations have been performed correctly.Tackling the above problems requires the design of protocols that, on the one hand, are provably secureand at the same time remain highly efficient, otherwise the mainpurpose of adopting cloud computing, namely efficiency and scalability,is defeated. It is therefore essential that expertise in cryptography andefficient algorithmics be combined to achieve these goals.This thesis studies techniques allowing the efficient verification of data integrity and computations correctness in such adversarial environments. Towards this end, several new authenticated data structures for fundamental algorithmics and computation problems, e.g., hash table queries and set operations, are proposed. The main novelty of this work lies in employing advanced cryptography such as lattices and bilinear maps, towards achieving high efficiency, departing from traditional hash-based primitives. As such, the proposed techniques lead to efficient solutions that introduce minimal asymptotic overhead and at the same time enable highly-desirable features such as optimal verification mechanisms and parallel authenticated data structures algorithms. The small asymptotic overhead does translate into significant practical savings, yielding efficient protocols and system prototypes."
          ],
          "keyword": [
            "applied cryptography",
            "authenticated data structures",
            "Cloud computing"
          ],
          "primary_title": "Cryptography for Efficiency: New Directions in Authenticated Data Structures",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11333/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11333/"
        },
        {
          "pid": "bdr:11316",
          "object_type": "pdf",
          "abstract": [
            "Peer-to-peer systems have been proposed for a wide variety of applications, such as file-sharing, distributed storage, and distributed computation. These systems seek the benefits of a decentralized design--chiefly, the ability of a system to self-scale as new participants join, since participants are motivated to contribute resources that offset the added workload they generate. Decentralization also offers improved fault-tolerance and user privacy, because no central authority is responsible for orchestrating or recording peer interactions.However, these beneficial system properties are at risk from selfish participants. While many peer-to-peer systems provide incentives for encouraging participation, past work has shown that these mechanisms can be gamed by selfish peers that consume resources while providing little or none in return. For example, the pairwise reputation scheme used by BitTorrent applies only in the short term of a single download; new peers must be bootstrapped by altruistic service.A currency-based accounting system provides fungible, long-term incentives for participation that persist beyond the limited scale of a single download or pairwise interaction. However, currency raises a number of privacy and systems concerns arising from the infrastructure required to support it. For example, a \"bank\" must check and count currency, which presents a potential performance bottleneck and privacy concern. In the digital world, the privacy of peer interactions can be preserved through the use of anonymous, cryptographically secure electronic cash (e-cash).This thesis shows that e-cash is a practical technique for ensuring fairness, robustness, and better long-term incentives in decentralized systems. It investigates how to build systems that anonymously account for three different resource types--bandwidth, storage, and computation--through the application of protocols for cryptographic fair exchange and e-cash, and how to mitigate the overhead involved in doing so. As a proof of concept, this thesis introduces Cashlib, an open- source library for e-cash; ZKPDL, a programming language that speeds both the performance and development of cryptographic implementations; and FairTrader, a currency-based file sharing system that uses e-cash to provide long-term, reliable service for users."
          ],
          "keyword": [
            "computer systems",
            "networking",
            "decentralized systems",
            "peer-to-peer",
            "P2P",
            "e-cash",
            "electronic cash",
            "incentives",
            "Peer-to-peer architecture (Computer networks)",
            "Stored-value cards"
          ],
          "primary_title": "Anonymous Accounting for Decentralized Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11316/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11316/"
        },
        {
          "pid": "bdr:1129289",
          "object_type": "pdf",
          "abstract": [
            "Extracting and inferring valuable knowledge from raw data requires training in programming and statistics, and more importantly, domain expertise. Yet domain experts, the people who own and are most knowledgeable about their data, often lack the technical skills to accomplish common analysis and prediction tasks themselves. In an effort to democratize data science, researchers have introduced a variety of tools to empower domain experts to explore and mine their data, and to build and interpret predictive models on their own. As a result such users are becoming increasingly sophisticated in their ability to perform data science tasks, in particular on tabular data. However, despite the fact that time series analysis and forecasting are fundamental tasks in data science, the HCI and data visualization communities have paid little attention to the problem of designing interactive tools that lower the barrier for non-expert users to perform time-oriented data science tasks. In this work we investigate visual techniques aimed at addressing open challenges in building solutions for exploring and forecasting time series. We present solutions to four different but related problems with analyzing time series. First, we propose a sketch-based technique to search large collections of time series for best matches, and present a method to evaluate the subjective accuracy of pattern-matching algorithms against human matches. Second, we explore a method to aid the analysis of correlated events in time series and their associated meta data. Third, we present a system that facilitates black-box analysis of such time series event detection algorithms, such as anomaly detectors, to give domain experts a tool to reason about commonalities and differences among them. Fourth, we develop a system that supports non-expert users in interactively building and evaluating time series forecasting models. Finally, given the importance of fast response times when interacting with both time series as well as general tabular data, we present a benchmark for interactive data exploration, which, contrary to standard synthetic database benchmarks, features workloads that are based on the traces of real interaction logs and metrics that allow adjustment of the speed/accuracy trade-off."
          ],
          "keyword": [
            "Visual analytics"
          ],
          "primary_title": "Visual Methods for Exploring and Forecasting Time Series",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129289/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129289/"
        },
        {
          "pid": "bdr:1129312",
          "object_type": "pdf",
          "abstract": [
            "The analysis of the statistical properties of data is a core goal in computer science. The challenges encountered in this task have evolved through time due to the explosion of the size of the datasets being considered and to the development of new analysis tools. Rather than computing the exact quantities of interest, computing high-quality estimates allows for a considerable reduction of computational effort while retaining meaningful and reliable insight into the statistical properties of the data. In this thesis, we study the applications of probabilistic analysis and statistical learning theory to the analysis of large datasets. Using these concepts allows us to rigorously characterize the relationship between the amount of input training data, the quality, in terms of confidence and precision, of the approximations achievable for a certain task, and the complexity of the task itself. In the first part of this work, we study the application of statistical learning tools such as Rademacher Complexity and VC dimension, to the classical hypothesis testing problem. We intro-duceRadaBound, a rigorous, efficient, and practical procedure for controlling the generalization error when using a holdout sample for multiple adaptive testing. Further, in the standard (nonadaptive) multiple hypothesis setting, ourRadeFWER(resp., RadeFDR) procedure achieves control of the Family Wise Error Rate -FWER- (resp., False Discovery Rate -FDR-). The statistical power of our procedures decreases with respect to the actual complexity of the considered hypotheses, captured in a data-dependent way by estimation of the Rademacher Complexity. In the second part of this thesis, we study massive dynamic graphs being observed as an adversarial stream of edges insertions and deletions towards counting the number of occurrences of a given small sub-graph pattern or a “motif” of interest: we presentTrièst, a suite of one-pass stream-ing algorithms for triangle counting based on reservoir sampling and the random pairing sampling schemes, and Tiered Sampling, an extension ofTrièstwhich employs multiple reservoir sample tiers in order to accurately estimate the count of rare and complex patterns."
          ],
          "keyword": [
            "Big data",
            "Machine learning--Statistical methods"
          ],
          "primary_title": "Probabilistic approaches for rigorous and efficient analysis of statistical properties of large datasets",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129312/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129312/"
        },
        {
          "pid": "bdr:1129310",
          "object_type": "pdf",
          "abstract": [
            "The current paradigm in health tracking research, as performed in fields such as public health, social sciences, and research initiatives like mHealth, is to find generalizable effects that can be disseminated to the public. However, by definition, there only has to be a small effect on a subset of that population for those studies to claim a positive result. In order to avoid following general advice and to find out what works specifically for them as individuals, some people perform experiments on themselves (self-experiments). However, in reality the insights people draw are often flawed because not everyone is trained to conduct scientifically valid experiments. The aim of this work is to study how individuals perform self-experiments and to build novel systems that guide people through the steps of such experiments. This dissertation outlines tools and guidelines to make self-experimentation accessible by providing easy to understand interventions and results. The first system presented is SleepCoacher: an automated system for self-experiments in sleep that includes a sleep tracking smartphone application that collects data from sensors and user input to provide and evaluate the effect of actionable personalized recommendations for improving sleep. Next, based on findings from studies with the system, the SleepCoacher approach is modified to add more guidance, flexibility, and agency in the self-experimentation process to create a robust app called SleepBandits. In SleepBandits, Thompson Sampling, a heuristic from reinforcement learning, estimates how likely it is that the behavior change is helpful for the given user. Finally, extending this line of research, Self-E is developed as a system for broader self-experiments, which lets the user choose which behaviors to change and automatically breaks down the experiment into a series of steps, and communicates them to the user through actionable messages. Together, these systems are a step towards a vision of perpetual self-experiments, where users can continuously receive recommendations and change little snippets of their behavior to constantly improve their well-being."
          ],
          "keyword": [
            "human-computer interaction",
            "Computer science"
          ],
          "primary_title": "Personalized Systems for Guided and Flexible Self-Experiments",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129310/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129310/"
        },
        {
          "pid": "bdr:1129306",
          "object_type": "pdf",
          "abstract": [
            "The invent of modern RDMA-enabled networks in data centers calls for a fundamental re-evaluation of how distributed data stores should be designed. Conventionally, many data structures, algorithms, and on the higher level, the architecture of these systems were primarily centered around the assumption that network communication is the dominant bottleneck and thus should be avoided as much as possible. However, in the light of recent advances in modern interconnects, this assumption is no longer valid, as RDMA functionality, together with high bandwidth and low latency of these networks enables direct remote access with significantly more efficiency and lower overhead compared to conventional networks. In this work, we first present a novel distributed OLTP system and show that it can truly scale even when all transactions span multiple machines. Motivated by this insight, we then argue that the conventional data partitioning techniques are no longer optimal in this new environment. To that end, we propose a new data partitioning scheme that optimizes for contention, which is the real bottleneck in the next generation of transactional databases. Finally, we tackle the problem of high availability in OLTP systems, and propose a new replication scheme that efficiently leverages RDMA to completely eliminate the processing redundancy in existing techniques."
          ],
          "keyword": [
            "Distributed databases",
            "RDMA",
            "Distributed Transactions",
            "Modern Networks",
            "Transaction Processing"
          ],
          "primary_title": "Distributed Transaction Processing on Modern RDMA-enabled Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129306/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129306/"
        },
        {
          "pid": "bdr:1129438",
          "object_type": "pdf",
          "abstract": [
            "Cancer is an evolutionary process where cells acquire somatic mutations over time. As a result of this process, tumors are often highly heterogeneous — containing cell populations with different sets of mutations. While tumor evolution cannot be directly observed, high-throughput DNA sequencing provides a measure of tumor heterogeneity. In this thesis, we introduce three algorithms that leverage tumor heterogeneity to infer the phylogenetic history of cancer. These algorithms account for specific issues in the two technologies for cancer DNA sequencing — bulk sequencing, where thousands of cells from a single tumor are sequenced simultaneously, and single-cell sequencing. First, we introduce an algorithm, PASTRI, that simultaneously deconvolves bulk sequencing data and constructs a tumor phylogeny based on somatic single-nucleotide variants (SNVs); PASTRI uses a hybrid probabilistic-combinatorial approach to more accurately model sequencing noise compared to existing combinatorial approaches. In addition to SNVs, tumors often contain copy-number aberrations (CNAs), a structural variation where larger regions of the genome are duplicated or deleted. Most existing phylogenetic inference algorithms use either SNVs or CNAs alone; however CNAs often overlap SNVs and failure to model these interactions between the two types of mutations can lead to inaccurate phylogenies. We introduce two algorithms, SPRUCE and SCARLET, that integrate SNVs and CNAs using bulk sequencing data and single-cell sequencing data, respectively. Finally, we investigate technical biases in single-cell DNA sequencing data and demonstrate how these biases can be used to phase genomic variants."
          ],
          "keyword": [
            "Genomics",
            "DNA sequencing",
            "Cancer",
            "Algorithms",
            "Phylogeny",
            "Evolution"
          ],
          "primary_title": "Tumor Phylogeny Reconstruction from DNA Sequencing Data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129438/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129438/"
        },
        {
          "pid": "bdr:1129462",
          "object_type": "pdf",
          "abstract": [
            "In today's age of blockchain and cloud-computing, we are constantly in search of tools that offer accountability without compromising privacy. A class of predominantly used tools is proof systems that provide soundness and privacy guarantees. Over the years, proof systems with different privacy guarantees such as zero-knowledge (ZK), witness-indistinguishability (WI) or witness-hiding (WH) have been constructed. For a lot of applications, it is desirable that proof systems have enhanced properties such as homomorphism, or efficiency for special-purpose languages. In this thesis, we propose and develop the following enhancements to proof systems: - Fully Homomorphic NIZK and NIWI Proofs: Given non-interactive zero-knowledge (NIZK) or witness-indistinguishable (NIWI) proofs for different statements, we give a framework to form proofs for new inferred statements. The security guarantee along with soundness and zero-knowledge is that of unlinkability; an inferred proof should be indistinguishable from a fresh proof for a combined statement. - Privacy Preserving Verifiable Key Directories: The current implementations of online chat services place a lot of trust in the central server that stores all the usernames and their public keys. We initiate the study of the primitive of Verifiable Key Directories (VKD) to formalize the security and privacy of a key verification service. As a key building block, we develop append-only zero-knowledge sets and give highly efficient constructions for the same. - Proofs of Ignorance: We introduce the notion of proofs of ignorance; we formalize what it means to provably not know and explore the settings in which one could give a convincing proof of ignorance. We define the notion of proofs of ignorance for all of NP, construct such proofs for a subset of NP languages, and show applications to witness-hiding protocols."
          ],
          "keyword": [
            "Cryptography",
            "Computer science"
          ],
          "primary_title": "Enhancing Privacy-Preserving Proof Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129462/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129462/"
        },
        {
          "pid": "bdr:1129226",
          "object_type": "pdf",
          "abstract": [
            "The downside of the world becoming more digitally connected is that it is now easier for powerful adversaries to strongly discourage digital communications by conducting mass surveillance. This has a chilling effect on freedom of speech. While proper encryption and authentication can secure the confidentiality of messages' content, communication patterns (i.e., who is communicating with whom) can still leak from observed traffic over the Internet. Onion routing is the most promising method for enabling anonymous communications. In an onion routing protocol, messages travel through several intermediaries before arriving at their destinations; they are wrapped in layers of encryption (hence they are called ``onions''). Despite the widespread use of onion routing in the real world (e.g., Tor, Mixminion), the foundations of onion routing have not been thoroughly studied. In this dissertation, we present new results on onion routing protocols and onion encryption schemes that lend themselves to onion routing. Our results include: 1. An anonymous protocol with polylogarithmic (in the security parameter) onion cost (onions transmitted per party) in the presence of the passive adversary who can monitor a constant fraction of the nodes. 2. A differentially private protocol with polylogarithmic (in the security parameter) onion cost in the presence of the active adversary who can control a constant fraction of the nodes. 3. The first onion routing protocol that is simultaneously efficient, fault-tolerant and anonymous in the active adversary setting. The protocol requires an expected polylogarithmic (in the security parameter) number of onions to be transmitted per message. 4. A lower bound (matching our protocol) for achieving anonymity in the active adversary setting. We show that for an onion routing protocol to be anonymous and fault-tolerant, the onion cost is omega(log lambda), where lambda is the security parameter. 5. We also present the first provably secure ``repliable'' onion encryption scheme for enabling the recipient of an onion to reply to the anonymous sender. Our work resolves the previously open problem of formalizing onion encryption for two-way channels."
          ],
          "keyword": [
            "Privacy",
            "Onion routing",
            "Anonymity"
          ],
          "primary_title": "Onions, Shallots and Leaks: Anonymous Communications Through Public Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129226/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129226/"
        },
        {
          "pid": "bdr:1129493",
          "object_type": "pdf",
          "abstract": [
            "In order to intuitively and efficiently collaborate with humans, robots must learn to complete tasks specified using natural language. Natural language instructions can have many intentions: for example, they can specify a goal condition, or provide guidance to achieve a goal, or provide constraints that must be satisfied in achieving goals. Given a natural language command, a robot needs to ground the instruction to an action sequence executed in the environment, which satisfies the request along with all its constraints and guidances. This work addresses the problem of grounding natural language instructions to plans with various approaches, that all depend upon predicates from first order logic. We will first describe a hierarchical planner for planning in large state spaces. We will then present different procedures such as classification, sequence to sequence mapping and compositional parser learning, to ground natural language for hierarchical and constraint based tasks. These approaches allow for language commands specifying simple goal based specifications to richer temporal constraints with guidances. We finish with looking at an approach that learns symbols and their natural language groundings using demonstrations and instructions. This allows the agent to follow novel instructions in unseen environments without hand-specified symbols."
          ],
          "keyword": [
            "Artificial intelligence",
            "Robotics"
          ],
          "primary_title": "Learning to Ground Natural Language Instructions to Plans",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129493/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129493/"
        },
        {
          "pid": "bdr:11280",
          "object_type": "pdf",
          "abstract": [
            "The goal of an automated database designer is to produce auxiliary structures that speed up user queries within the constraints of the user-speci?ed resource budget (typically disk space). Most existing research on automating physical database design has been performed in the context of commercial row-store databases such as Microsoft SQL Server or IBM DB2. In fact, every commercial database o?ers some sort of a tool that can provide design recommendations for the consideration of the database administrator. An automated tool is necessary not only because a database administrator is not always available but because the complexity of the design problem is constantly increasing: new auxiliary structures and query processing methods continue to be introduced, and more users and queries need to be serviced.This dissertation extensively investigates the problem of automating physical database design in the context of a column-store that supports clustered indexing. In the experiments presented here, we primarily used Vertica, a commercial column-store database that is based on the C-Store research project jointly developed at Brandeis, Brown and MIT. Although on the surface it seems like only the underlying storage system has changed, while the problem of designing the physical structures remains essentially the same, we found several fundamental di?erences that make physical design in a clustered column-store database a unique problem. Many of the basic axioms that are used in a row-store design are invalid in a column-store design (and vice versa). In this dissertation, we demonstrate the construction of an e?ective design tool and an analytic cost model for use in a column-store. We show that certain techniques from machine learning such as clustering can reduce and simplify this design problem. To our knowledge there has been little previous work on the problem of physical design in the context of column-stores and none in the context of column-stores such as C-Store or Vertica."
          ],
          "keyword": [
            "Database Physical Design DBMS Column Store Column-store"
          ],
          "primary_title": "Design Tool for a Clustered Column-Store Database",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:11280/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:11280/"
        },
        {
          "pid": "bdr:297608",
          "object_type": "pdf",
          "abstract": [
            "Web programs are distinct from traditional programs in two key ways. First,<br/><br/> programs freely compose data and code from several sources. Second, the user<br/><br/> often cannot control which programs run; Web programs are visited, not<br/><br/> installed. These problems do not affect traditional software.<br/><br/> <br/><br/> Web programs are universally written in JavaScript, the \\emph{lingua<br/><br/> franca} of the Web. JavaScript has quirks and features that<br/><br/> make it difficult to read and reason about simple snippets of code. JavaScript<br/><br/> also lacks mechanisms necessary to control the complexity of large<br/><br/> programs. These deficiencies make Web programming harder<br/><br/> than necessary. <br/><br/> <br/><br/> This dissertation presents a type-checker for JavaScript <br/><br/> that uses two novel<br/><br/> techniques to verify JavaScript programs that are untypable by conventional<br/><br/> means. These types define invariants and interfaces for programming in the<br/><br/> large, and also catch bugs caused by JavaScript's quirks. We demonstrate its<br/><br/> efficacy by type-checking several thousand lines of JavaScript, written by<br/><br/> several third-party programmers.<br/><br/> <br/><br/> Our JavaScript type-checker is sophisticated enough to verify security<br/><br/> properties of JavaScript code. We use the type-checker to verify and find bugs<br/><br/> in ADsafe, a third-party language-based Web sandbox that makes program<br/><br/> composition safe. As a result, ADsafe is the first Web sandbox with a<br/><br/> precisely defined notion of safety and proof thereof.<br/><br/> <br/><br/> Finally, this dissertation presents a core calculus for JavaScript that is<br/><br/> tested against commercial Web browsers. The design of the calculus and our<br/><br/> testing give us confidence that our tools and proofs have some bearing on<br/><br/> reality. For the same reasons, our semantics are already in use by other<br/><br/> research groups."
          ],
          "keyword": [
            "programming languages",
            "type systems",
            "operational semantics"
          ],
          "primary_title": "Semantics and Types for Safe Web Programming",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297608/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297608/"
        },
        {
          "pid": "bdr:297607",
          "object_type": "pdf",
          "abstract": [
            "This dissertation investigates learning dependency grammars for statistical natural language parsing from corpora without parse tree annotations. Most successful work in unsupervised dependency grammar induction has assumed that the input consists of sequences of parts-of-speech, ignoring words and using extremely simple probabilistic models. However, supervised parsing has long shown the value of more sophisticated models which use lexical features. These more sophisticated models however require probability distributions with complex conditioning information, which must be smoothed to avoid sparsity issues.<br/> In this work we explore several dependency grammars that use smoothing, and lexical features. We explore a variety of different smoothing regimens, and find that smoothing is helpful for even unlexicalized models such as the Dependency Model with Valence. Furthermore, adding lexical features yields the highest accuracy dependency induction on the Penn Treebank WJS10 corpus to date. In sum, this dissertation extends unsupervised grammar induction by incorporating lexical conditional information, by investigating smoothing in an unsupervised framework."
          ],
          "keyword": [
            "Grammar Induction",
            "Unsupervised Learning",
            "Dependency Parsing"
          ],
          "primary_title": "Unsupervised Bayesian Lexicalized Dependency Grammar Induction",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297607/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297607/"
        },
        {
          "pid": "bdr:297605",
          "object_type": "pdf",
          "abstract": [
            "Model-based, data-driven prediction is emerging as an essential ingredient for both user-facing applications, such as predictive analytics, and system-facing applications such as autonomic computing. This thesis studies the two complementary research questions of how to effectively support and leverage predictive models for database applications. We explore the performance and usability advantages of integrated predictive functionality within database systems and argue that next generation database systems should natively support and manage predictive models, tightly integrating them in the process of data management and query processing. This is in contrast to the current practice of implementing such functionality within the application space. We study how various types of predictive models can be efficiently supported by utilizing and extending database system mechanisms. Specifically, we discuss (i) white-box support, in which the knowledge of model semantics facilitates a tight integration, thus introducing rich optimization opportunities and (ii) black-box support, in which no such knowledge is assumed, thus leading to a general but less optimizable system. We derive our results from two detailed case studies. In the first one, we describe white-box model support of Bayesian Networks to enable continuous predictive queries over streaming data. In the second, we describe black-box model support and its application on query performance prediction in database systems. We describe efficient implementations of both applications in open-source database systems. Our experimental studies provide quantitative evidence that predictive functionality can be achieved within databases with a level of performance and accuracy that is competitive with or favorable to that of specialized, custom solutions."
          ],
          "keyword": [
            "database systems",
            "predictive applications in databases",
            "prediction support in databases",
            "predictive modeling",
            "query performance prediction",
            "Databases"
          ],
          "primary_title": "Supporting and Leveraging Prediction Models for Database Applications",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297605/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297605/"
        },
        {
          "pid": "bdr:297606",
          "object_type": "pdf",
          "abstract": [
            "Visual sensor networks (VSN) are networks of smart cameras capable of local image processing and data communication. Unlike traditional camera-based surveillance network in which cameras stream all image data to a centralized server for processing, cameras in VSNs form a distributed system, performing information extraction and collaborating on application-specific tasks. This thesis studies how complex vision tasks can be integrated with system resource constraints such as computation capacity, battery power and bandwidth, in two different VSN contexts. The first context is large-scale ad-hoc wireless smart cameras working on battery power which resembles the architecture of general wireless sensor networks. In this context we build geographic hash table based network protocols that are adapted to the nature of image sensors. These protocols decouple the event sensing from the camera location. Simulation results show that these protocols allow efficient distributed camera calibration and event-based constraint processing. The second context is smaller-scale static wired smart cameras with constant power supplies which can be found in public spaces that need surveillance such as airports and casinos. We study the performance advantages of applying probabilistic fusion methods in cross-camera object tracking. Object tracking based on a single feature type can produce high error rates due to environmental and view changes. We present a probabilistic object matching framework which employs multiple object features and a decision mechanism to combine results from multiple features. The framework builds matching probability distributions for each feature algorithm based on empirical data and combines these historical results into an aggregated result. Our experimental studies on realistic data show that while there is no single feature algorithm works best all the time, our probabilistic integration method on multiple features can almost always achieve better object matching accuracy than the best individual feature algorithm."
          ],
          "keyword": [
            "Visual Sensor Networks",
            "Distributed Camera Calibration",
            "Distributed Constraint Processing",
            "Object Tracking"
          ],
          "primary_title": "Supporting Complex Tasks in Visual Sensor Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297606/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297606/"
        },
        {
          "pid": "bdr:297610",
          "object_type": "pdf",
          "abstract": [
            "In correlation clustering, given similarity or dissimilarity information for all pairs of data items, the goal is to find a clustering of the items into similarity classes, with the fewest inconsistencies with the input. This problem is hard to approximate in general but we give arbitrarily good approximation algorithms (PTASs) for two interesting special cases: when there are few clusters, and when the input is generated from a natural noisy model. In the feedback arc set problem in tournaments, given comparison information (a better than b) for all pairs of data items, the goal is to find a ranking of the items with the fewest inconsistencies with the input. We give the first PTAS for this problem. We then extend our techniques to a more general class of problems called fragile dense problems."
          ],
          "keyword": [
            "dense constraint satisfaction problems",
            "correlation clustering",
            "feedback arc set tournament",
            "Approximation algorithms"
          ],
          "primary_title": "Approximation Schemes for Inferring Rankings and Clusterings from Pairwise Data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297610/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297610/"
        },
        {
          "pid": "bdr:297612",
          "object_type": "pdf",
          "abstract": [
            "This dissertation introduces and evaluates novel visualization methods that enable researchers to derive and test hypotheses from available scientific data faster and more accurately than before. Following the traditional visualization approach, we introduce novel ways of visualizing and interacting with scientific data that support and accelerate researchers' data analysis workflows. Following the visual analytics path, which advocates for supporting the reasoning process itself, we quantify the degree to which interface design elements can be used to unobtrusively guide researchers towards applying verified and established analysis techniques in their research.<br/><br/> We first present novel visualization methods that were developed in response to analytic needs indentified through collaborative efforts in three concrete application areas. In neuroscience we enable faster interaction with diffusion tensor imaging (DTI) datasets by creating planar representations of the inherently 3D data. In proteomics we facilitate the visual collation of experimental data and existing protein interaction information and accelerate the discovery process by uncovering and supporting elements of the proteomic analysis workflow. In genomics we increase the accessibility of analyzable visualizations of microarray data and eliminate the overhead of creating visualizations and learning new systems by implementing and evaluating a novel data distribution method.<br/><br/> Finally, we use the concepts of persuasive technology and ``choice architecture'' which state that a user of a system can be unobtrusively guided towards behavioral patterns that are more efficient, in terms of self-assumed goals, by slight alterations in the system interface. We provide quantitative experimental support for the hypothesis that we can use subtle changes in the interfaces of visual analysis systems to influence users' analytic behavior and thus unobtrusively guide them towards improved analytic strategies. We posit that this approach may facilitate the use of visual analytics expertise to correct biases and heuristics documented in the cognitive science community."
          ],
          "keyword": [
            "User Studies",
            "Evaluation",
            "Networks",
            "Visualization",
            "Visual analytics"
          ],
          "primary_title": "Improved Scientific Analysis through Domain Driven Visualization and Support for Analytic Deliberation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297612/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297612/"
        },
        {
          "pid": "bdr:297611",
          "object_type": "pdf",
          "abstract": [
            "Recent natural disasters, such as Hurricane Katrina in 2005 and the Japan earthquake in 2011, have demonstrated that situational awareness, the focus of much research in disaster management, is not sufficient to provide effective mitigation, response, and recovery. The United States government has recognized the need for decision support to enhance the cognitive abilities of decision makers. Developing such tools is particularly challenging because they require modeling complex interdependent infrastructure systems and must operate under aggressive runtime constraints.<br/><br/> <br/><br/> This thesis demonstrates that hybrid optimization technology can produce decision support tools for delivery of relief supplies, restoration of the electrical power infrastructure alone, and joint restoration of interdependent natural gas and electrical power infrastructures. These hybrid methods use mixed integer programming for infrastructure modeling, large neighborhood search for routing of repair crews, and randomized adaptive decomposition for scaling the optimization algorithms to practical sizes. The proposed hybrid methods increase the restoration quality significantly over current field practices and outperform traditional technologies, which cannot find feasible solutions within the runtime constraints.<br/><br/> <br/><br/> The optimization algorithms must rely on linear approximations of the nonlinear AC power flow equations to meet the runtime constraints of these applications. Unfortunately, the accuracy of the traditional linearized DC model degrades significantly as network disruptions increase. This thesis also proposes novel linear approximations that are shown to provide significant improvements in accuracy under disruptions."
          ],
          "keyword": [
            "Disaster Management",
            "Power Systems",
            "Disasters--Management",
            "Artificial intelligence",
            "Combinatorial optimization"
          ],
          "primary_title": "Decision Support for Disaster Management Through Hybrid Optimization",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297611/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297611/"
        },
        {
          "pid": "bdr:297609",
          "object_type": "pdf",
          "abstract": [
            "When developing a new heuristic or complete algorithm for a constraint satisfaction or constrained optimization problem, we frequently face the problem of choice. There may be multiple branching heuristics that we can employ, various restart strategies, or different types of inference mechanisms. The way in which the choices we make affect one another is not readily perceptible. The task of making these choices is known as algorithm configuration.<br/> <br/> Developers often make many of these algorithmic choices during the prototyping stage. Based on a few preliminary manual tests, certain algorithmic components are discarded, even before all the remaining components have been implemented. Additionally, developers have limited knowledge about the instances a user will typically employ the solver for. That is the very reason why solvers have parameters: to enable users to fine-tune a solver for their specific needs.<br/> <br/> Alternatively, manually tuning a parameterized solver requires significant resources, effort, and expert knowledge. Before even trying the numerous possible parameter settings, the user must learn about the inner workings of the solver to understand what each parameter does. Furthermore, manual tuning has been shown to often lead to highly inferior performance.<br/> <br/> This dissertation shows how to train a multi-scale, multi-task approach for enhanced performance based on machine learning techniques automatically. In particular this work presents a new methodology for Instance-Specific Algorithm Configuration (ISAC). ISAC is a general configurator that focusses on tuning parameterized solvers according to the instances they will be applied to. Specifically, this dissertation shows that the instances of many problems can be decomposed into a representative vector of features and that instances with similar features often cause similar behavior in the applied algorithm. ISAC exploits this observation by automatically detecting the different sub-types of a problem and then training a solver for each variety. This technique is explored on a number of problem domains, including set covering, mixed integer, satisfiability, and set partitioning. ISAC is then further expanded to demonstrate its application to traditional algorithm portfolios and adaptive search methodologies. In all cases, marked improvements are shown over the existing state-of-the-art solvers. <br/>"
          ],
          "keyword": [
            "algorithm configuration",
            "optimization",
            "Machine learning"
          ],
          "primary_title": "Instance-Specific Algorithm Configuration",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297609/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297609/"
        },
        {
          "pid": "bdr:297615",
          "object_type": "pdf",
          "abstract": [
            "Despite the fact that the human genome was sequenced ten years ago, there exists no database of cis-regulatory architecture that is validated conclusively by rigorous experimental criteria; this has been a notoriously longstanding unresolved computational biology problem. This proposal describes the construction of the first such database, the cis-Lexicon, containing only causality-inferred DNA sequence structure information on the regulatory regions of transcription factor-encoding genes and other regulatory genes. The data in the Lexicon is purely causality-inferred, meaning that each annotation is backed by experimental techniques which prove causality; there is no information due to noisy experimental methods or computational prediction. This data, previously not available in databases (outside of the original papers) or mixed with lower-quality data, is necessary for understanding the cis-regulatory code, the relationship between sequence structure and regulatory function. Only through completeness of information will correct conclusions be drawn from the Lexicon, and for this purpose we built the cis-Lexicon Ontology Search Engine (CLOSE). CLOSE is an information retrieval system designed to find biology journal articles containing cis-regulatory sequence structure information and evaluate the completeness of the cis-Lexicon. Information must be entered into the Lexicon in a reliable manner to ensure accurate annotations; in addition, the data must be conveniently accessible to be useful for experimental work. For both of these purposes we have built the cis-Browser, a genome browser customized for cis-regulatory analysis."
          ],
          "keyword": [
            "Databases",
            "Computational biology",
            "Genomes",
            "Information retrieval"
          ],
          "primary_title": "A database of causality-inferred structure-function information for genomic cis-regulatory architecture",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297615/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297615/"
        },
        {
          "pid": "bdr:298",
          "object_type": "pdf",
          "abstract": [
            "Electronic cash is an important tool for preserving on-line privacy. It allows a user to make purchases without revealing his identity to the merchant and prevents banks from monitoring the transactions of all their users. In this thesis, we use secret sharing techniques to extend electronic cash. We examine the problem of fair exchange that lets a user atomically exchange an electronic coin for some digital good or service. While all fair exchange protocols require the help of a trusted third party, in an optimistic protocol, the trusted party becomes involved only if something goes wrong. We construct the first optimistic fair exchange protocol for exchanging a single electronic coin for a digital good or service. We use secret sharing to extend the protocol to allow a user to efficiently exchange multiple electronic coins for multiple goods and services in one atomic transaction. We also show how a user can pay a single electronic coin for multiple goods or services, which need not be delivered atomically. We apply these fair exchange protocols to create incentives in anonymous peer-to-peer filesharing systems. We show how to perform an efficient fair exchange for very large files (such as movies); the trusted third party can resolve problems without downloading the entire file. We also show how to escrow electronic coins to allow peers to barter for files. If one of the peers fails to deliver, he acquires the escrowed coin. Electronic cash can be used for anonymous authentication. Instead of purchasing a good or service, the user may use electronic coins to access restricted resources. For example, the user may purchase a license to download twenty songs a month from a provider. We show how to use secret sharing to create flexible policies. In the case where the electronic coins are stored on a small hardware device, the extra flexibility may be used to provide glitch protection in case the user accidentally spends a few extra coins. Finally, we end by developing a new secret sharing protocol for a disjunctive multi-level access structure."
          ],
          "keyword": [
            "secret sharing",
            "electronic cash",
            "e-cash",
            "anonymoush authentication",
            "peer-to-peer networks",
            "Stored-value cards",
            "Peer-to-peer architecture (Computer networks)"
          ],
          "primary_title": "Sharing Secrets for Fun and Profit",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:298/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:298/"
        },
        {
          "pid": "bdr:273",
          "object_type": "pdf",
          "abstract": [
            "Modern multicore processors, such as Intel's Core 2 Duo, bring urgency to parallel programming research, which, despite several decades of work, has not changed much since the advent of mutual exclusion in the 1960s. Software Transactional Memory (STM) is a promising alternative that borrows heavily from the database community to propose a transactional approach to program state. By optimistically assuming that most computations executed in parallel do not conflict and then detecting and terminating those few computations that do, STM provides the basis for new programming language abstractions that permit programmers to think linearly while the code they write is safely executed in parallel. The difficulty with today's STM libraries, however, is their generally poor performance and convoluted programming model. In order to make STMs more popular, accessible, and functional, this thesis argues that three things must happen: first, a streamlined programming model can hide all complex details of the STM library from the programmer; second, static analysis can optimize the client code's interaction with the underlying STM; third, the STM library itself should be decomposed and optimized for use by an optimizing compiler. Together, we show that these optimizations achieve an order of magnitude performance improvement. As part of this work, we develop four nonblocking transaction synchronization and validation algorithms designed for use in STM systems, all of which support a nonblocking progress condition called obstruction-freedom, provide always consistent reads, and integrate orthogonal contention management. Finally, we propose the first language extensions and compiler support for transactional boosting, a methodology for transforming highly concurrent linearizable objects into highly concurrent transactional objects. Based on these results, we conclude that appropriate language support and high quality compiler optimizations are necessary for the success of any STM system."
          ],
          "keyword": [
            "Compilers",
            "Programming Languages",
            "Software Transactional Memory"
          ],
          "primary_title": "Language Support and Compiler Optimizations for Object-based Software Transactional Memory",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:273/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:273/"
        },
        {
          "pid": "bdr:320482",
          "object_type": "pdf",
          "abstract": [
            "Large graphs are ubiquitous in many diverse fields ranging from sociology and economics to biology and engineering. Applications in numerous areas such as transportation, geographical routing, computer vision and VLSI design involve solving optimization problems on large planar graphs. The sheer growth in the number and size of available data sets drives a need to design optimization algorithms that are not merely efficient in the qualitative sense that their computational resource consumption is polynomial in the size of the input, but in the stronger quantitative sense that the amount of required resources (specifically, running time and space) is as close to linear as possible.<br/> In this thesis we study the combinatorial structural properties of directed planar graphs, and exploit these properties to devise efficient algorithms for shortest-path and maximum-flow problems in such graphs. Among the results we present are:<br/> - A linear-space algorithm for the shortest-path problem in directed planar graphs with real (positive and negative) arc lengths that runs in near-linear time. Our algorithm is more efficient and conceptually simpler than previously known algorithms for this problem.<br/> - Data structures for efficiently reporting exact distances in directed planar graphs (distance oracles) with significantly improved space-to-query-time ratio over previously known exact distance oracles.<br/> - A near-linear time algorithm for computing a maximum flow in directed planar graphs with multiple sources and sinks. This problem has been open for more than 20 years. No planarity exploiting algorithm for the problem was previously known. Our algorithm is significantly faster than previous algorithms for this problem.<br/><br/> These problems, and the tools and techniques used in solving them, are interrelated."
          ],
          "keyword": [
            "planar graphs",
            "shortest-paths",
            "maximum-flow",
            "Graph theory",
            "Algorithms"
          ],
          "primary_title": "Efficient Algorithms for Shortest-Path and Maximum-Flow Problems in Planar Graphs",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320482/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320482/"
        },
        {
          "pid": "bdr:320476",
          "object_type": "pdf",
          "abstract": [
            "This dissertation presents new computational approaches toward the virtual histology of white-matter microstructures and new visualization and interaction techniques for identifying and segmenting white-matter anatomy. Together, our new techniques, which are based on diffusion magnetic resonance imaging (diffusion MRI), enhance the microstructural and anatomical analysis of brain white matter to localize neurological changes resulting from disease and degeneration in two ways. First, clinically feasible acquisition is sufficient for accurate reconstruction of quantitative measurements of microstructural properties in brain tissue of unknown orientation. Second, qualitative visualization using 3D interaction combined with anatomical landmarks can enhance user performance in isolating tracts for pathological analysis.<br/> <br/> The first part of the dissertation presents new computational approaches toward the virtual histology of brain tissue that reveal quantitative local measures of their microstructure and attempt to provide reliable and sensitive biomarkers for neurological changes. We develop analytical models of water diffusion and incorporate them into a computational algorithm in order to extract the underlying microstructural properties that are otherwise unattainable in vivo. We go beyond current experimental limitations, which require high gradients (not achievable in clinical scanners) and known tissue orientation, by using double-pulsed field gradient (double-PFG) diffusion MRI. We demonstrate that clinically feasible acquisition is sufficient to reconstruct these microstructural properties accurately in brain tissue of unknown orientation. The feasibility and reliability of these computational approaches were first quantitatively validated using simulations, which let us go beyond previous work to provide a complete study that aggregates a set of characteristics not previously combined. To validate our methods further, we then apply our computational approaches to live human subjects using clinical 3T MRI scanners for microstructure quantification and cross-subject comparisons. The results demonstrate the sensitivity of our approach to human microstructural properties, and also verify microstructural variations known from histology along the corpus callosum across different subjects.<br/> <br/> The second part of the dissertation presents new qualitative visualization and interaction techniques that help neuroscientists isolate neural tracts of interest (TOI) for quantitative local pathological analysis. First, we develop two new visualization techniques and demonstrate their benefits in disentangling complex neural tracts and in helping neuroscientists identify the underlying neuroanatomy and its connectivity. Second, we study the design principles related to the TOI selection techniques to analyze their utility, usability, accuracy and reliability. We develop taxonomy and design guidelines for these TOI selection tasks as a framework for categorizing the design space of the techniques. Using these design guidelines, we implement two selection techniques to enhance user performance in TOI selection tasks that use two or more of the following features: (1) anatomical landmarks matching experts' training, (2) free-form lasso drawing for flexible selection, (3) a 3D stereo virtual reality environment to avoid visual flattening of brain structures, (4) haptics-assisted 3D lasso drawing for direct 3D interaction, and (5) a higher-input-bandwidth device to reduce navigation time."
          ],
          "keyword": [
            "brain white matter; axon; brain microstructure; tracts of interest; diffusion MRI"
          ],
          "primary_title": "In-vivo Microstructural and Anatomical Analysis of Brain White Matter from Diffusion MRI",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320476/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320476/"
        },
        {
          "pid": "bdr:320474",
          "object_type": "pdf",
          "abstract": [
            "Data processing frameworks provide application programmers an interface to manipulate and analyze data. This thesis studies a novel parallel stream processing model, designed for workflow-based data processing frameworks, that leverages application performance requirements to motivate the flexible scheduling and fine-grained allocation of data to computing nodes.<br/> <br/> We feature this processing model through the design and implementation of the Continuous-MapReduce (C-MR) data processing framework. C-MR abstracts away the complexities of parallel stream processing and workflow scheduling while providing the simple and familiar MapReduce programming interface with the addition of stream window semantics. Its novel processing model enables: 1) fine-grained, workflow-wide load balancing across computing nodes; 2) the evolving application of data and task parallelism models as guided by application performance requirements; and 3) a novel scheduling framework which supports gradual transitions between scheduling policies relative to application performance and/or resource availability.<br/> <br/> This work explores the potential of the C-MR processing model by studying our single-host implementation of C-MR that supports parallel execution on non-dedicated and heterogeneous computing nodes (both multi-core CPUs and GPUs). We then study this processing model through the implementation of a distributed version of C-MR that supports execution on multiple hosts. This endeavor involved the generalizable strategy of employing hierarchical instances of the C-MR processing model while requiring modifications to the data acquisition and load balancing strategies. Experimental results from these studies show that the C-MR processing model can effectively support the continuous execution of workflows of MapReduce jobs for stream processing while being resilient to stream and resource fluctuations due to the processing model's flexibility and diversification of processing responsibilities.<br/>"
          ],
          "keyword": [
            "Stream processing",
            "distributed computing",
            "Streamflow--Data processing",
            "Electronic data processing--Distributed processing"
          ],
          "primary_title": "C-MR: Continuous Execution of MapReduce Workflows for Stream Processing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320474/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320474/"
        },
        {
          "pid": "bdr:2jzzzvav",
          "object_type": "pdf",
          "abstract": [
            "Reusing knowledge allows intelligent systems to learn solutions to complex tasks more quicker by avoiding re-learning the components of the solution from scratch. Recent advances in reinforcement-learning research have demonstrated that deep learning algorithms can solve complex tasks. However, achieving knowledge reuse characteristic of human behaviour has been elusive. Humans are adept at flexibly transferring knowledge between different tasks, whereas established reinforcement-learning algorithms are much more limited. Reusing knowledge in reinforcement-learning algorithms is a central, yet not well understood challenge. This dissertation addresses the question of which models allow an intelligent system to reuse knowledge and provides a partial solution. Viewing knowledge representations through the lens of representation learning, we show that models that are predictive of future reward outcomes implicitly encode reusable knowledge. Through a sequence of theoretical and empirical results, this dissertation discusses different state representations and presents connections to model-based reinforcement learning, model-free reinforcement learning, and successor features. Furthermore, different transfer-learning experiments are presented, demonstrating that representations that are predictive of future reward outcomes generalize across different tasks. Lastly, we introduce a clustering algorithm to learn representations that are predictive of future reward sequences for tasks with continuous state spaces. We demonstrate under which assumptions this clustering algorithm converges to an accurate model. Furthermore, on a visual control task, we demonstrate that this learned model generalizes across different tasks and can be used to accelerate learning. These results suggest that learning a model detailed enough to predict future reward outcomes prevents overfitting to one task and allows an agent to accelerate learning across previously unseen tasks."
          ],
          "keyword": [
            "Reinforcement learning",
            "Transfer Learning",
            "Representation Learning",
            "Successor Representation",
            "Successor Features",
            "Model-Based Reinforcement Learning",
            "Generalization in Reinforcement Learning"
          ],
          "primary_title": "Encoding Reusable Knowledge in State Representations",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:2jzzzvav/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:2jzzzvav/"
        },
        {
          "pid": "bdr:320527",
          "object_type": "pdf",
          "abstract": [
            "Modeling the complex interactions that arise when query workloads share computing resources and data is challenging albeit critical for a number of tasks such as Quality of Service (QoS) management in the emerging cloud-based database platforms, effective resource allocation for time-sensitive processing tasks, and user-experience management for interactive systems. In our work, we develop practical models for query performance prediction (QPP) for heterogeneous, concurrent query workloads in analytical databases.<br/> <br/> Specifically, we propose and evaluate several learning-based solutions for QPP. We first address QPP for static workloads that originate from well-known query classes. Then, we propose a more general solution for dynamic, ad hoc workloads. Finally, we address the issue of generalizing QPP for different hardware platforms such as those available from cloud-service providers.<br/> <br/> Our solutions use a combination of isolated and concurrent query execution samples, as well as new query workload features and metrics that can capture how different query classes behave for various levels of resource availability and contention. We implemented our solutions on top of PostgreSQL and evaluated them experimentally by quantifying their effectiveness for analytical data and workloads, represented by the established benchmark suites TPC-H and TPC-DS. The results show that learning-based QPP can be both feasible and effective for many static and dynamic workload scenarios."
          ],
          "keyword": [
            "data management",
            "workload modeling",
            "workload prediction",
            "Big data"
          ],
          "primary_title": "Query Performance Prediction for Analytical Workloads",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320527/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320527/"
        },
        {
          "pid": "bdr:320650",
          "object_type": "pdf",
          "abstract": [
            "We propose a new paradigm for vision-based human motion capture. This paradigm extends the traditional capture of poses by providing guarantees of physical plausibility for the motion reconstructions and mechanisms for adaptation of the estimated motions to new environments. We achieve these benefits by estimating control programs for simulated physics-based characters from (potentially monocular) images. The control programs encode motions implicitly, based on their ``underlying physical principles'' and reconstruct the motions through simulation. Feedback within the control allows application of the principles in modified environments, providing an ability to adapt the motion to external events and perturbations. We explore two control models: trajectory control and state-space control. The trajectory control model encodes the desired behavior of the character as a sequence of per-frame target poses tracked by the controller. We can recover this sequence incrementally and produce pose estimates that do not suffer from common visual artifacts. However, the inference process is prone to overfitting. To address this limitation, we then explore a more compact model that is less sensitive to the quality of observations. State-space controllers allow concise representation of motion dynamics through a sparse set of target poses and control parameters, in essence allowing a key-frame-like representation of the original motion. We represent state-space controllers using state machines that characterize the character behavior in terms of motion phases (states) and physical events that cause the phases to switch (transitions, e.g., a foot contact). Parameters of the controller encode the control programs that reproduce the individual phases in simulation. Because this control representation is sparse, we are able to integrate information locally from multiple (tens of) image frames in inference, inducing smoothness in the resulting motion, resolving some of the ambiguities that arise in monocular video-based capture and enabling inference with weak likelihoods. We demonstrate our approach by capturing sequences of walking, jumping, and gymnastics. We evaluate our methods quantitatively and qualitatively and illustrate that we can produce motion interpretations that go beyond state-of-the-art in pose tracking and are physically plausible."
          ],
          "keyword": [
            "motion capture",
            "control",
            "physics-based characters",
            "physical simulation",
            "controller estimation",
            "optimal control",
            "Computer vision"
          ],
          "primary_title": "Physically Plausible Human Pose and Control Estimation from Video",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320650/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320650/"
        },
        {
          "pid": "bdr:320604",
          "object_type": "pdf",
          "abstract": [
            "One of the major goals in cryptography is to design protocols that withstand malicious behavior of an adversary. Traditionally, the focus was on a setting where honest users followed their protocol exactly, without fault. But what if an adversary can induce faults, for example a physical attack that changes the state of a user's computation, forcing a user to accept when he should be rejecting; or tries to use a modified secret key? Can any security guarantees still be given when such errors occur? My PhD work studies the implications of various types of errors and develops techniques that protect against them.<br/> <br/> I have delved into the following topics for different scenarios of errors: (1) cryptography with imperfect hardware, where the adversary can cause the cryptographic device to leak some secret information and tamper with the device's memory; (2) secure delegation protocols, where a user can delegate some computation to an untrusted server that causes errors. <br/> <br/> To highlight some of my results: <br/> <br/> I gave a generic construction to secure *any* cryptographic functionality against continual memory tampering and leakage errors in the *split-state model*. My main tool is to construct a non-malleable code that is also leakage resilient in this model, which resolves one central open problem in the previous work (due to Dziembowski et al. -- ICS 10). <br/> <br/> I developed new delegation protocols that allow a user, who only stores a short certificate of his data (potentially very large), to delegate the computation on the data to the cloud, and then verify the outcome in time *sub-linear* in the data size. <br/> <br/> In the thesis, I elaborate my work in these two lines, and some potential future directions."
          ],
          "keyword": [
            "protocol",
            "cryptographic device",
            "delegation",
            "tampering attacks",
            "leakage attacks"
          ],
          "primary_title": "Error Tolerant Cryptography",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:320604/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:320604/"
        },
        {
          "pid": "bdr:297613",
          "object_type": "pdf",
          "abstract": [
            "This thesis studies various techniques that exploit correlations between<br/><br/> attributes to significantly improve the performance and maintainability of<br/><br/> analytic databases.<br/><br/> We first show how a correlation-based secondary index developed here<br/><br/> achieves index sizes smaller by orders of magnitude than a<br/><br/> conventional secondary index. We then illustrate how, in the context of<br/><br/> analytic databases, using a secondary index that is strongly correlated with a clustered<br/><br/> index performs orders of magnitude faster than the uncorrelated case. Our goal<br/><br/> was, then, to exploit these two observations in real database system settings.<br/><br/> To meet the above goal, we developed (1) a data structure to<br/><br/> store correlations as a secondary index, and (2) a database design tool to produce<br/><br/> correlated indexes and materialized views.<br/><br/> We further extended its applicability in a few directions, namely:<br/><br/> (3) a formulation and optimization of<br/><br/> index deployment to achieve faster completion as well as earlier query<br/><br/> speed-up,<br/><br/> (4) flexible partitioning and sorting techniques<br/><br/> to apply the idea of correlations in distributed systems such as MapReduce, and<br/><br/> (5) a clustered index structure for uncertain attributes to apply<br/><br/> the benefits of correlations to uncertain databases."
          ],
          "keyword": [
            "Physical Database Design",
            "Correlation",
            "CP",
            "Databases",
            "Indexes"
          ],
          "primary_title": "Correlation-Aware Optimizations for Analytic Databases",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297613/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297613/"
        },
        {
          "pid": "bdr:297614",
          "object_type": "pdf",
          "abstract": [
            "Solving combinatorial problems is an interplay between search and inference. In this thesis, we focus on search and investigate its important aspects. We start with complete search procedures and consider binary search, which is frequently used to augment a feasibility solver to handle optimization problems. In this setting, we often observe that negative trials (i.e., showing that a certain solution quality cannot be achieved) are significantly harder than positive trials. We consider a simple cost model where negative trials cost a constant factor more than positive trials and show how binary search can be biased optimally to achieve optimal worst-case and average-case performance.<br/> <br/> Next, as a complementary approach, we turn to incomplete search procedures. We propose Hegel and Fichte's dialectic as a local search meta-heuristic. Dialectic is an appealing mental concept for local search as it allows developing functions for search space exploration and exploitation independently. We illustrate dialectic search, its simplicity and great efficiency on problems from highly different problem domains.<br/> <br/> We then study variable and value selection heuristics, and propose a simple modification to impact-based search strategy. We present computational results on constraint satisfaction problems that show improvements in the search performance.<br/> <br/> Finally, we look at the interaction between search and inference. In particular, we investigate incrementality during tree search interleaved with constraint propagation. We first consider constraints based on context-free grammars for which we devise a time-and space-efficient filtering algorithm. We then look at constraints that enforce the same-relation for every pair of variables in binary constraint satisfaction problems. We show that achieving generalized arc-consistency in special graphs such as cliques, complete bipartite, and directed acyclic graphs is NP-hard. However, we can leverage the knowledge that sets of pairs of variables all share the same relation for both theoretical and practical gains."
          ],
          "keyword": [
            "Constraint Programming",
            "Constraint Satisfaction",
            "Search Procedures",
            "Meta-Heuristics",
            "Artificial intelligence",
            "Constraint programming (Computer science)",
            "CONSAT",
            "Constrained optimization"
          ],
          "primary_title": "Efficient Search Procedures for Solving Combinatorial Problems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:297614/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:297614/"
        },
        {
          "pid": "bdr:105",
          "object_type": "pdf",
          "abstract": [
            "In this thesis we investigate distinct techniques to avoid the impact of miscellaneous failures derived from hostile attacks in a message-passing distributed computing environment. Our failure palette assumes more benign faults as an appetizer and then moves on to more malign ones. To start with, we revisit the classical k-set agreement problem, where different distributed processors must all agree on up to k values previously proposed by them. We develop an optimal crash-resilient k-set agreement protocol, which tolerates the best possible number of crashes given (exact or close to) minimal synchrony features provided by limited-scope failure detectors in asynchronous systems. Tight bounds on the maximum number of crashes are achieved through combinatorial topology, and relation of failure detectors to timing assumptions, in settings such as stabilizing ones, is unfolded. In the sequence, we broaden our failure repertoire to include message omissions performed by coprocessor hosts with high incentives to cheating. Here, coprocessors are tamper-proof and receive and send encrypted messages. This prevents arbitrary behavior of hosts, which may just omit incoming or outgoing messages or crash their own coprocessor. In this context, making use of secret shared coins, we derive randomized consensus (that is, 1-set agreement) protocols, optimal both in terms of time and resilience, and of very practical use for e-business. Deterministic versions and automatic transformations for failure detectors are also discussed. Finally, we show how to boost threshold protocols in adversarial structure models where failures may be dependent and processors may behave badly in an arbitrary, byzantine way. In particular, we look at the problem of running any protocol that has an upper bound of n > c t, for any positive integer constant c, where n is the total number of processors and t is the maximum number of faults. We introduce an optimal byzantine-resilient protocol that enables simulation with less than n processors of any such threshold protocol in adversarial structure models. We also define equivalence classes using a particular set of key hierarchy properties."
          ],
          "keyword": [
            "IT-Security",
            "Distributed Computing",
            "Fault-Tolerance",
            "Optimization",
            "Electronic data processing--Distributed processing",
            "Fault tolerance (Engineering)"
          ],
          "primary_title": "Distributed Protocals Robust Against Malicious Attacks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:105/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:105/"
        },
        {
          "pid": "bdr:697876",
          "object_type": "pdf",
          "abstract": [
            "Typically, gene interaction networks are expressed as graphs, storing pairwise interactions between genes. Because of the vast amount of literature on statistical graph inference, this is a useful representation in practice. However, such a pairwise representation ignores more complex features of gene interactions, such as gene regulation and assembly. In this thesis, we propose a hypergraph model for gene interaction networks. Since many network-based algorithms rely on random walks, we analyze our model through the viewpoint of random walks. We outline a general framework for random walks on a hypergraph, and we show in a precise sense that random walks on hypergraphs are not special cases of random walks on graphs. We also define a mapping between hypergraphs and graphs, and use that mapping to build a hypergraph from an already-existing graph representation of a gene interaction network. We then use this hypergraph network to perform disease-gene prioritization via the PageRank algorithm. For monogenic diseases, we find that the hypergraph noticeably outperforms the graph, demonstrating the value of using hypergraphs as gene interaction networks."
          ],
          "dateCreated": "2017-01-01T00:00:00Z",
          "keyword": [
            "Computational biology",
            "Graph theory",
            "Hypergraphs",
            "Random walks (Mathematics)"
          ],
          "primary_title": "Random walks on hypergraphs with applications to disease-gene prioritization",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:697876/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:697876/"
        },
        {
          "pid": "bdr:733345",
          "object_type": "pdf",
          "abstract": [
            "This thesis advances visualization design research by developing and evaluating new theoretical knowledge and computational techniques, which target the rising complexity of data and growing diversity of visualization users. To ground our research, we focus our study on common design limitations that are found in cancer genomics, which is an exemplar of how research at-large is affected by the rising ubiquity and democratization of visualization in analysis. We first identify four cancer genomics task requirements for visual analysis through interviews and evaluate whether the multiple visualizations in MAGI – a cancer genomics visualization tool – can support such diversity. Second, we evaluate how simple classifiers trained on annotated mouse interaction logs can help designers understand how domain experts use visualizations. Third, we explore the ways in which the size and perceptual grouping of data in visualization can affect visual search performance and visual analysis tasks. Last, we discuss a novel tool for creating categorical color palettes based on user-defined importances of discriminability and aesthetic preference, which can be a common and difficult task in visualization design independent of application area. These contributions may help mitigate visualization design barriers by providing guidelines and techniques to help visualization creators avoid common pitfalls. For example, our evaluation of Colorgorical demonstrates that the tool can automatically generate color palettes based on user defined balances of discriminability and preference, which are comparably discriminable and typically more preferable compared to industry standards. Colorgorical thus provides an effective alternative to making categorical palettes by hand, which can be time consuming and require design expertise. While the contributions in this thesis have immediate applications in cancer genomics, our contributions are not limited in application. Many may generalize to other domains, such as using domain expert interaction log analysis to better understand how visualization is used by different kinds of researchers in the brain sciences. Given the visualization design research similarities between cancer genomics and other domain expert centric applications like brain science, we conclude by hypothesizing how our findings could be used to investigate open research areas."
          ],
          "keyword": [
            "Information visualization",
            "human-computer interaction",
            "Visual analytics"
          ],
          "primary_title": "A computational approach to mitigate visualization design barriers",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733345/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733345/"
        },
        {
          "pid": "bdr:733298",
          "object_type": "pdf",
          "abstract": [
            "A natural language parser recovers the latent grammatical structures of sentences. In many natural language processing (NLP) applications, parsing is applied to sentences first and the parses along with their sentences are fed to following NLP systems. For example, Google parses the entire web and applies a series of NLP programs to index the web and the quality of search results depends on the quality of parses. Parsing is difficult because sentences are ambiguous: a sentence has different syntactic structures depending on its meaning. For example, a sentence \"Eugene wears a bow tie with polka dots\" can have very different meanings depending on what \"with polka dots\" modifies. It is natural for us humans to infer that \"with polka dots\" modifies \"a bow tie\" because we have common sense that \"with polka dots\" rarely (if not never) describes an action \"wears.\" Computers, however, lack common sense and learn such a relationship from large amounts of texts by just looking for statistical patterns. We explore four ways of improving parsing in this thesis: creating a training data of high quality parses using paraphrases; a model combination technique applied to n-best parsing; a generative reranker based on a language model; a discriminative parser inspired by neural machine translation. Our parse-reranker achieves human-level performance on the standard Penn Treebank dataset."
          ],
          "keyword": [
            "Natural language processing (Computer science)"
          ],
          "primary_title": "Toward Solving Penn Treebank Parsing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733298/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733298/"
        },
        {
          "pid": "bdr:792601",
          "object_type": "pdf",
          "abstract": [
            "Eye tracking, the process of capturing the gaze location within a display, is extensively used in usability studies, psychology, human-computer interaction, and marketing. The setup and operation of modern eye trackers is time-consuming and a specialist is needed to calibrate them and be present throughout the experiment, leading to highly-controlled user studies with artificial tasks and only a small number of participants. In addition, their steep price, which rises to tens of thousands of dollars, restricts their use to only a small number of labs that can afford them. This thesis aims to democratize eye tracking by using common webcams already present in laptops and desktops. We introduce WebGazer, a webcam eye tracker that infers the gaze of web visitors in real time. WebGazer is developed as an open-source JavaScript library that can be incorporated into any website. Its eye tracking model self-calibrates by mapping eye features to positions on the display that correspond to user interactions. We investigate whether webcam eye tracking can lead to similar conclusions to in-lab eye tracking studies. We explore this question in the context of web search, by extending WebGazer so that it can predict the examined search element within a search engine result page. We use SearchGazer to replicate three seminal studies in the area of information retrieval and demonstrate that scalable and remote eye tracking studies on user behavior are possible at a fraction of cost and time. Finally, we create a benchmark for webcam eye tracking with data collected from a lab study with more than 60 participants. This dataset allows us to investigate the relationship between user interactions and gaze, confirming past findings on the alignment of gaze with clicks and cursor movement, and introducing novel insights into the differences in gaze behavior across users based on their ability to touch type. Taking advantage of the temporal alignment of gaze and user interactions, we perform improvements in WebGazer's accuracy and functionality. These contributions make eye tracking accessible to everyday users, researchers, and developers. Traditional eye tracking studies that are confined to labs can now be performed remotely and at scale. Subjects can participate in studies in their everyday environments which can yield a more naturalistic behavior and lead to more powerful insights."
          ],
          "keyword": [
            "human-computer interaction",
            "Eye tracking"
          ],
          "primary_title": "Democratizing Eye Tracking",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792601/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792601/"
        },
        {
          "pid": "bdr:792615",
          "object_type": "pdf",
          "abstract": [
            "We propose a general-purpose probabilistic framework for scene understanding tasks. We show that several classical scene understanding tasks can be modeled and addressed under a common representation, approximate inference scheme, and learning algorithm. We refer to this approach as the Probabilistic Scene Grammar (PSG) framework. The PSG framework models scenes using probabilistic grammars which capture relationships between objects in terms of compositional rules that provide important contextual cues for inference with ambiguous data. We show how to represent the distribution defined by a probabilistic grammar using a factor graph. We also show how to estimate the parameters of a grammar using an approximate version of Expectation-Maximization, and describe an approximate inference scheme using Loopy Belief Propagation with an efficient message-passing scheme. Inference with Loopy Belief Propagation naturally combines bottom-up and top-down contextual information and leads to a robust algorithm for aggregating evidence. To demonstrate the generality of the approach, we evaluate the PSG framework on the scene understanding tasks of contour detection, face localization, and binary image segmentation. The results of the PSG framework are competitive with algorithms specialized for these scene understanding tasks."
          ],
          "keyword": [
            "Machine Learning",
            "Computer Vision"
          ],
          "primary_title": "Probabilistic Scene Grammars: A General-Purpose Framework For Scene Understanding",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792615/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792615/"
        },
        {
          "pid": "bdr:792586",
          "object_type": "pdf",
          "abstract": [
            "This dissertation contributes to visualization research by introducing findings and methods that help advance the design of visual analytics tools for scientific reasoning. We have designed and implemented visualization tools that facilitate analysis of brain connectivity networks and the development of research proposals. Needs identified during development of the tools motivated controlled experiments on how the design of the tools can help users make better decisions by facilitating the perception of uncertainty and mitigating certain cognitive biases that may arise during information processing. We also present an evaluation approach that makes use of user interaction logs to better understand how scientists work with a visual interface to arrive at insights in addition to quantifying the utility of the tools as in conventional insight-based evaluation. The intellectual merit of the dissertation work is two-fold. First, findings presented here can lead to increased understanding of the interaction between the human mind and the visual analysis tools during scientific reasoning. Such understanding can be translated into design guidelines and will enable visualization researchers and practitioners to design more effective visual analysis tools for other scientific reasoning tasks. Second, the evaluation method can be used to gain more insights into how users accomplish analysis tasks using visual interfaces, and speed the pace at which such interfaces can be evaluated and improved. While most of the work is inspired by brain science as the application domain, we expect most of the findings and methods to be applicable to any visual analysis tool developed to support open-ended analytical tasks."
          ],
          "keyword": [
            "Information visualization",
            "human-computer interaction"
          ],
          "primary_title": "Shaping the Scientific Hypothesis Generation Process through the Design of Visual Analysis Tools",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792586/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792586/"
        },
        {
          "pid": "bdr:74",
          "object_type": "pdf",
          "abstract": [
            "This thesis investigates the practicality and utility of mathematical models to represent continuous and occasionally unavailable data stream attributes, and processing relational-style queries in a stream processing engine directly on these models. We present Pulse, a framework for processing continuous queries over stream attributes modeled as piecewise polynomial functions. We use piece-wise polynomials to provide a compact, approximate representation of the input dataset and provide query language extensions for users to specify precision bounds to control this approximation. Pulse represents queries as simultaneous equation systems for a variety of relational operators including filters, joins and standard aggregates. In the stream context, we continually solve these equation systems as new data arrives into the system. We have implemented Pulse on top of the Borealis stream processing engine and evaluated it on two real-world datasets from financial and moving object applications. Pulse is able to achieve significant performance improvements by processing queries directly on the mathematical representation of these polynomials, in comparison to standard tuple-based stream processing, thereby demonstrating the viability of our system in the face of having to meet precision requirements. In addition to our primary contribution of describing the core design and architecture of Pulse, this thesis presents a selectivity estimator and a multi-query optimizer to scale query processing capabilities. Our selectivity estimator uses histograms defined on a parameter space of polynomial coefficients for estimation, passing selectivities to our multi-query optimizer which may then determine how to construct a global query plan that shares work across individual queries. We evaluate these components on both a synthetic dataset and a financial dataset. Our experiments show that our optimization mechanisms provide significant reductions in processing overhead, and that our estimation algorithm provides an accurate and low overhead estimator for selective operators, that can be enhanced by sampling, while also being a general technique that can handle operators such as min and max aggregates, where sampling is known to be inaccurate."
          ],
          "keyword": [
            "Data stream processing",
            "model-based databases",
            "continuous query processing",
            "continuous functions",
            "selectivity estimation",
            "multi-query optimization",
            "Functions, Continuous",
            "Databases"
          ],
          "primary_title": "Pulse: Database Support for Efficient Query Processing Of Temporal Polynomial Models",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:74/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:74/"
        },
        {
          "pid": "bdr:674302",
          "object_type": "pdf",
          "abstract": [
            "We present research developing and evaluating computational tools for brain mapping with diffusion magnetic resonance imaging (MRI). Diffusion MRI enables the in-vivo reconstruction of the geometric structure of brain white matter, and it has potential to improve our understanding the brain in health and disease through numerous scientific and clinical applications. The complexity of both the imaging data and the underlying anatomy make computational tools an essential part of these applications, and improvements to this toolset can allow more intricate anatomical structures to be found and population-wide variation to be mapped. Beyond these applications, such computational tools also represent a research area with many open problems of their own, as every advance made in tool development also creates a need to expand our understanding of the strengths and limitations of the tools themselves. This thesis work represents advances made on both fronts to develop and evaluate computational tools for modeling, visualizing, and analyzing brain white matter with diffusion MRI. The contributions include voxel-based analysis, fiber bundle modeling, and model-based image processing. First, we develop an algorithm for supervoxel segmentation of brain white matter and show how this is useful for automated region-based analysis in population imaging studies. Second, we comparatively evaluate eight methods for spatial mapping in diffusion tensor studies and quantify their relationship to scan-rescan reliability and predictive models of age-related decline in an adult population of 80 subjects. Third, we develop and evaluate methods for tractography-based modeling of fiber bundles using a sparse closest point transform and its use in combination with statistical and machine learning algorithms in bundle clustering, simplification, and population selection. Fourth, we develop and evaluate a kernel regression framework for tractography and atlas construction with multi-compartment model imaging data. We show how this improves the accuracy of tractography reconstructions in crossing fiber regions and enables complex anatomy to be reliably reconstructed in both individual subjects and population averages. Finally, we apply this multi-fiber tractography method to the reconstruction of the corticospinal tract in tumor imaging applications."
          ],
          "keyword": [
            "diffusion mri",
            "neuroimaging",
            "white matter",
            "segmentation",
            "computational modeling",
            "Image processing",
            "Visualization",
            "Image segmentation"
          ],
          "primary_title": "Developing and Evaluating Computational Tools for the Modeling, Visualization, and Analysis of Brain White Matter with Diffusion MRI",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674302/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674302/"
        },
        {
          "pid": "bdr:674219",
          "object_type": "pdf",
          "abstract": [
            "We explore the role of software assistance in native language targeted second language education. Beginning with the classification task of determining an author’s native language from second language text, we demonstrate the use of Tree Substitution Grammar fragments as an effectively discriminative class of features. We contrast the use of several syntactic analyses of second language text as well as different methods for feature selection through grammar induction. We move on to investigate the data driven formulation of hypotheses for syntactic language transfer, which refers to the preferential use of second language syntax that mirrors an author’s native language. Our methodology produces a ranked and filtered list of hypotheses that provides compelling evidence for several examples of language transfer, and is easily augmented as more data becomes available. We conclude with a novel system for language generation in educational applications that incorporates the inherent vocabulary based constraints of the domain. While these constraints are easily handled with rejection sampling, this becomes inefficient as the amount of training data increases. To combat this, we show sampling algorithms for context free languages that avoid rejection, sampling directly from the acceptable outputs with tight approximation. Our work facilitates the construction of systems to both enhance the quality of second language education through awareness of students’ native languages and reduce the effort required to create language education exercises."
          ],
          "keyword": [
            "Contrastive Analysis",
            "Interlanguage",
            "Natural language processing",
            "Interlanguage (Language learning)",
            "Second language acquisition"
          ],
          "primary_title": "Computationally Connecting Language Transfer and Second Language Education",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674219/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674219/"
        },
        {
          "pid": "bdr:674297",
          "object_type": "pdf",
          "abstract": [
            "Recent improvements in DNA sequencing technology have opened new paths towards understanding cancer biology and designing personalized cancer treatments. Cancer is caused in part by somatic mutations to the DNA of healthy cells that can be identified with DNA sequencing technology. A major challenge in analyzing the mutations in cancer is distinguishing the handful of driver mutations that cause cancer from the multitude of passenger mutations that play no role in cancer. In part to address this challenge, consortia such as The Cancer Genome Atlas (TCGA) have generated massive catalogues of somatic mutations in thousands of tumors. The new wealth of mutation data has shown that identifying driver mutations is a difficult computational problem. Many driver mutations are rare, even in these large tumor cohorts, because different combinations of mutations cause cancer in different patients, even those of the same cancer (sub)type. Part of the reason for this phenomenon is that driver mutations target key genetic pathways that perform vital cell functions. Each pathway consists of a set of interacting genes, and can be perturbed in numerous ways. Therefore, we have developed computational methods to search for combinations of driver mutations targeting pathways. We describe several methods for identifying combinations of putative driver mutations. We first present two methods for identifying combinations of mutations that are mutually exclusive in a cohort of tumors, a pattern expected for genetic pathways. We then present an algorithm to identify significant clusters of mutations in an interaction network. We show that these algorithms outperform previous methods on simulated and real mutation data from TCGA, and identify potentially novel combinations of mutations. We also describe the Mutation Annotation and Genome Interpretation (MAGI) web application, which displays interactive visualizations as well as crowd-sourced and text-mined mutation annotations in order to help prioritize likely driver mutations. These methods contribute towards overcoming the computational challenge of identifying driver mutations in cancer sequencing data."
          ],
          "keyword": [
            "cancer genomics",
            "pathways",
            "driver mutation",
            "exact test"
          ],
          "primary_title": "Methods for Identifying Combinations of Driver Mutations in Cancer",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674297/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674297/"
        },
        {
          "pid": "bdr:674299",
          "object_type": "pdf",
          "abstract": [
            "Test-first development and peer review have been studied independently in computing courses, but their combination has not. In in-flow peer review, students provide feedback to one another on intermediate artifacts on their way to a final submission. We report on multiple studies of courses in which students conducted in-flow peer review of tests while assignments were in progress. We discuss student engagement, the correlation between review ratings and staff-assessed work quality, and evidence that test suites improved during the review process. Further, we present a useful distinction between full test suites and sweeps, which are small illustrative examples suitable for early submission and review. We show that these small sets of examples have reasonable quality, and are also a good target for peer review; for example, students suggest new tests to one another in a review over half the time."
          ],
          "keyword": [
            "examples",
            "Peer review",
            "Testing"
          ],
          "primary_title": "In-flow Peer Review of Examples in Example-First Programming",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674299/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674299/"
        },
        {
          "pid": "bdr:674327",
          "object_type": "pdf",
          "abstract": [
            "We develop new algorithms for training nonparametric clustering models based on the Dirichlet Process (DP), including DP mixture models, hierarchical Dirichlet process (HDP) topic models, and HDP hidden Markov models. These Bayesian nonparametric models allow coherent comparisons of different clusterings of a given dataset. The nonparametric approach is particularly promising for large-scale applications, where other model selection techniques like cross-validation are too expensive. However, existing training algorithms fail to live up to this promise. Both Monte Carlo samplers and variational optimization methods are vulnerable to local optima and sensitive to initialization, especially the initial number of clusters. Our new algorithms can reliably escape poor initializations to discover interpretable clusters from millions of training examples. For the DP mixture model, we pose a variational optimization problem in which the number of instantiated clusters assigned to data can be adapted during training. The focus of this optimization is an objective function which tightly lower bounds the marginal likelihood and thus can be used for Bayesian model selection. Our algorithm maximizes this objective score via block coordinate ascent interleaved with proposal moves that can add useful clusters to escape local optima while removing redundant or irrelevant clusters. We further introduce an incremental algorithm that can exactly optimize our objective function on large datasets while processing only small batches at each step. Our approach uses cached or memoized sufficient statistics to make exact decisions for proposal acceptance or rejection. This memoized approach has the same runtime cost as previous stochastic methods but allows exact acceptance decisions for cluster proposals and avoids learning rates entirely. We later extend these algorithms to HDP topic models and HDP hidden Markov models. Previous methods for the HDP have used zero-variance point estimates with problematic model selection properties. Instead, we find sophisticated solutions to the non-conjugacy inherent in the HDP that still yield an optimization objective function usable for Bayesian model selection. We demonstrate promising proposal moves for adapting the number of clusters during memoized training on millions of news articles, hundreds of motion capture sequences, and the human genome."
          ],
          "keyword": [
            "Bayesian nonparametrics",
            "Machine learning"
          ],
          "primary_title": "Reliable and scalable variational inference for nonparametric mixtures, topics, and sequences",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674327/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674327/"
        },
        {
          "pid": "bdr:674262",
          "object_type": "pdf",
          "abstract": [
            "We develop a family of algorithms for statistical inference in models of high dimensional continuous random variables. Our approach builds on existing variational methods, which provide computationally efficient alternatives to Markov chain Monte Carlo (MCMC) sampling. While efficient, existing variational approximations are not applicable to many continuous models of practical interest, or they are altogether unstable and produce degenerate solutions. We construct a more powerful class of algorithms for posterior marginal and maximum a posteriori (MAP) inference which avoids these limitations. Throughout this thesis we present a series of vignettes demonstrating the generality of our approach on disparate applications such as: human pose estimation in images and video, protein structure prediction, and target tracking. We begin by considering MAP inference problems for continuous Markov random fields (MRFs) where the well-known max-product (MP) variant of belief propagation (BP) cannot be applied due to non-Gaussian statistics. Motivated by similar ideas in sum-product BP we develop a particle-based approximation of the continuous MP messages. Unique to the MAP setting, however, is a need for diversity among the hypotheses, to avoid classic particle degeneracies. Using an integer programming formulation we enforce particle diversity, from which we can recover a set of distinct local maxima. Our nonparametric approximation applies to any model for which the probability density can be evaluated in a black-box manner, even for models with no analytic form. We validate our approach using a model for estimating human pose from single images and videos. To further motivate and validate our approach we consider the challenging problem of estimating three-dimensional protein structures. Using our particle-based approximations we optimize the continuous energy function encoding protein stability, and thereby avoid discrete approximations employed by most existing methods. In this way we are able to recover fine details of protein structure which standard methods fail to capture, and by preserving diverse hypotheses our approach maintains the conformational diversity proteins are known to exhibit."
          ],
          "keyword": [
            "graphical models",
            "Bayesian",
            "variational inference",
            "max-product",
            "belief propagation"
          ],
          "primary_title": "Variational Approximations with Diverse Applications",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674262/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674262/"
        },
        {
          "pid": "bdr:733381",
          "object_type": "pdf",
          "abstract": [
            "We develop Bayesian nonparametric statistical models of document collections and social networks. Extending classic parametric topic models of documents, and stochastic block models of networks, we first formulate flexible Bayesian nonparametric models based on the logistic stick-breaking process. This prior allows our model to automatically learn the dimension of the latent structure, use observed metadata to influence this structure, and discover correlations that exist between them. We call this model the Doubly Correlated Nonparametric Model (DCNM), and derive efficient MCMC learning algorithms. We then focus on the problem of scaling inference to large networks. We propose a hierarchical Dirichlet Process (HDP) relational model and derive a structured variational inference algorithm. For the practically important case of communities with assortative structure, we derive new updates where inference scales linearly in time and memory with the number of active clusters. From this, we develop a stochastic variational approach that allows us to scale inference to networks that contain tens of thousands of nodes. Finally, we develop pruning techniques that allow us to dynamically shrink the number of communities, and effective strategies for specifying learning rate parameters. After developing scalable inference models for relational data, we develop a memoized variational inference algorithm for the HDP topic model. This approach provides a more scalable framework for comparing models of varying complexity, by caching sufficient statistics of small batches of a very large dataset. Elegant delete-merge moves are then derived to optimize rigorous lower bounds on the marginal likelihood of the data, avoiding approximations required by previous stochastic inference algorithms. We use our memoized variational inference algorithms to develop Refinery, an open-source web platform for topic modeling that allows non-technical experts to leverage the power of topic models."
          ],
          "keyword": [
            "Bayesian Nonparametrics",
            "Variational Inference",
            "Machine Learning"
          ],
          "primary_title": "Scalable Bayesian Nonparametric Models for Networks and Documents",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733381/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733381/"
        },
        {
          "pid": "bdr:733422",
          "object_type": "pdf",
          "abstract": [
            "Futures are an attractive way to structure parallel computations. When a thread creates an expression with a keyword future, a new thread is spawned to compute that expression in parallel. When a thread later applies a touch operation to that future, it gets the result of the expression if the result has been computed, and otherwise blocks until the result becomes ready. In this thesis, we explore different aspects of parallel programs with futures, including their theoretical bounds, scheduling, and applications. Researchers have shown futures can have a deleterious effect on cache locality. We will show, however, that if futures are used in a simple, disciplined way, then their negative impact can be much alleviated. This structured use of futures is characteristic of many parallel applications. Futures lend themselves well to dynamic scheduling algorithms, such as work stealing, that ensure high processor utilization. Implementing work stealing on hierarchical platforms, such as NUMA systems and distributed clusters, have recently drawn lots of attention. However, little has been explored on its theoretical bounds. We present lower and upper bounds of work stealing for fork-join programs, a well-studied subclass of parallel-future programs, on hierarchical systems. As originally conceived, a future encapsulates a functional computation without side-effects. Recently, however, futures have been proposed as a way to encapsulate method calls to shared data structures. We propose a new program model, that supports both normal futures without side-effects and linearizable futures that exist for their side-effects. Using this model, we propose the lazy work stealing scheduler that facilitates certain optimizations for linearizable futures and guarantees good time bounds. The processing-in-memory (PIM) model has reemerged recently as a solution to alleviating the growing speed discrepancy between CPU's computation and memory access, commonly known as the memory wall. In this model, some lightweight computing units are directly attached to the main memory, providing fast memory access. We study applications of linearizable futures in the PIM model: operation requests to concurrent data structures are sent as linearizable futures to those computing units to execute. These PIM-managed data structures can outperform state-of-the-art concurrent data structures in the literature."
          ],
          "keyword": [
            "Parallel programming (Computer science)"
          ],
          "primary_title": "Theory and Applications of Parallelism with Futures",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733422/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733422/"
        },
        {
          "pid": "bdr:733374",
          "object_type": "pdf",
          "abstract": [
            "The need for rich, ad-hoc data analysis is key for pervasive discovery. However, generic and reusable systems tools for interactive search, exploration and mining over large data sets are lacking. Exploring large data sets interactively requires advanced data-driven search techniques that go well beyond the conventional database querying capabilities, whereas state-of-the-art search technologies are not designed and optimized to work for large out-of-core data sets. These requirements force users to roll their own custom solutions, typically by gluing together existing libraries, databases and custom scripts, only to end up with a solution that is difficult to develop, scale, optimize, maintain and reuse. To address these limitations, we propose a tight integration of data management and search technologies. This combination would not only allow users to perform search efficiently, but also offer a single, expressive framework that can support a wide variety of data-intensive search and exploration tasks. As the first step in this direction, we describe a custom search framework called Semantic Windows, which allows users to conveniently perform structured search via shape and content constraints over a large multidimensional data space. As the second step, we describe a general-purpose exploration framework called Searchlight, which allows Constraint Programming (CP) machinery to run efficiently inside a Database Management System (DBMS) without the need to extract, transform and move the data. This marriage concurrently offers the rich expressiveness and efficiency of constraint-based search and optimization provided by modern CP solvers, and the ability of DBMSs to store and query data at scale, resulting in an enriched functionality that can effectively support data- and search-intensive applications. As such, Searchlight is the first system to support generic search, exploration and mining over large multidimensional data collections, going beyond point algorithms designed for point search and mining tasks. Fast, interactive query evaluation is only one of the requirements of effective data-exploration support. Finding the right questions to ask is another notoriously challenging problem, given the users’ lack of familiarity with the structure and contents of the underlying data sets, as well as the inherently fuzzy goals in many exploration-oriented tasks. In the third part of this work, we study the modification of initial query parameters at run-time: we describe how Searchlight can dynamically relax or constrain the parameters of a query, based on its progress, to offer more or fewer results to the user. This feature allows users to iterate over the data sets faster and without having to make accurate guesses on what parameters to use."
          ],
          "keyword": [
            "Databases",
            "Data mining",
            "Constraint programming (Computer science)",
            "Information storage and retrieval systems"
          ],
          "primary_title": "Integrated Search and Exploration Over Large Multidimensional Data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733374/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733374/"
        },
        {
          "pid": "bdr:733434",
          "object_type": "pdf",
          "abstract": [
            "Sampling based and randomized algorithms are powerful tools in studying big data problems. In this thesis we provide sampling based algorithms for mining graphs and social network in three different contexts: Detecting Valuable Information. Detecting new information and events in a dynamic system by probing individual nodes has many practical applications: discovering new webpages, analyzing influence properties in network, and detecting failure propagation in electronic circuits or infections in public drinkable water systems. In practice, it is infeasible for anyone but the owner of the network (if existent) to monitor all nodes at all times. We study the constrained setting when the observer can only probe a small set of nodes at each time step to check whether new pieces of information (items) have reached those nodes. Centrality Maximization. Betweenness centrality (BWC) is a fundamental centrality measure in social network analysis. Given a large-scale network, how can we find the most central nodes? This question is of great importance to many key applications that rely on BWC, including community detection and understanding graph vulnerability. Despite the large amount of work on scalable approximation algorithm design for BWC, estimating BWC on large-scale networks remains a computational challenge. In this paper, we study the Centrality Maximization problem (CMP): given a graph G = (V,E) and a positive integer k, find a subset S of nodes V that maximizes BWC subject to the cardinality constraint |S| ≤ k. We present an efficient randomized algorithm that provides a (1 − 1/e − eps)-approximation with high probability, where eps > 0. Our results improve the current state-of-the-art result [127]. Influence Estimation. Social networks are important communication and information media. Individuals in a social network share information and influence each other through their social connections. Understanding social influence and information diffusion is a fundamental research endeavor and it has important applications in online social advertising, viral marketing, and trending topic prediction. We first study the Targeted-Influence problem (TIP): Given a network G = (V, E) and a model of influence, we want to be able to estimate in real-time (say in a few seconds per query) the influence of an arbitrary subset of users S over a arbitrary subset of users T, for any possible query (S; T). To do so, we allow an efficient preprocessing. Finally, we conclude the thesis by studying the problem of Conditional Influence Estimation (CIE) whose goal is to estimate the influence of a node given that a cascade started from that node has already reached a group of nodes (target nodes)."
          ],
          "keyword": [
            "Data mining",
            "Social networks",
            "Sampling (Statistics)",
            "Algorithms"
          ],
          "primary_title": "Scalable Algorithms for Mining Graphs and Social Networks via Sampling",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733434/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733434/"
        },
        {
          "pid": "bdr:733483",
          "object_type": "pdf",
          "abstract": [
            "Apps provide valuable utility and customizability to a range of user devices, but installation of third-party apps also presents significant security risks. Many app systems use permissions to mitigate this risk. It then falls to users to decide which apps to install and how to manage their permissions, but unfortunately, many users lack the expertise to do this in a meaningful way. In this thesis, I determine that users face two distinct privacy decisions when using apps: which apps to install, and how to manage apps' permissions once they are installed. In both cases, users are not given meaningful guidance to help them make these choices. For decisions about which apps to install, users would benefit from privacy information in the app marketplace, since that is how most users choose apps. Once users install an app, they are confronted with the second type of decision: how to manage the app's permissions. In this case, users would benefit from an assistant that helps them see which permissions might present privacy concerns. I therefore present two tools: a privacy-conscious app marketplace and a permission management assistant. Both of these tools rely on privacy information, in the form of ratings of apps' permissions. I discuss gathering this rating information from both human and automated sources and how it is used in the two tools. I also explore how the brand of an app could affect how users rate its permissions. Additionally, because my goal is to convey privacy information to users, I design and evaluate several interfaces for displaying permission ratings. I discuss surprising misconceptions generated by some of these interfaces, and present an interface that effectively communicates permission ratings."
          ],
          "keyword": [
            "human-computer interaction",
            "Privacy",
            "Smartphones"
          ],
          "primary_title": "On a (Per)Mission: Leveraging User Ratings of App Permissions to Help Users Manage Privacy",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733483/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733483/"
        },
        {
          "pid": "bdr:792684",
          "object_type": "pdf",
          "abstract": [
            "In today's world data is ubiquitous. Increasingly large and complex datasets are gathered across many domains. Data analysis - making sense of all this data - is exploratory by nature, demanding rapid iterations, and all but the simplest analysis tasks require humans in the loop to effectively steer the process. Current tools that support this process are built for an elite set of individuals: highly trained analysts or data scientists who have strong mathematics and computer science skills. This however presents a bottleneck. Qualified data scientists are scarce and expensive which makes it often unfeasible to inform decisions with data. How do we empower data enthusiasts, stakeholders or subject matter experts, who are not statisticians or programmers, to directly tease out insights from data? This thesis presents work towards making data analysis more accessible. We invent a set of user experiences with approachable visual metaphors where building blocks are directly manipulatable and incrementally composable to support common data analysis tasks at the pace that matches the thought process of a humans. First, we develop a system for back-of-the-envelope calculations that revolves around handwriting recognition - all data is represented as digital ink - and gestural commands. Second, we introduce a novel pen & touch system for data exploration and analysis which is based on four core interaction concepts. The combination and interplay between those concepts supports a wide range of common analytical tasks. The interface allows for incremental and piecewise query specification where intermediate visualizations serve as feedback as well as interactive handles to adjust query parameters. Third, we present a visual query interface for event sequence data. This touch-based interface exposes the full expressive power of regular expressions in an approachable way and interleaves query specification with result visualizations. Fourth, we present the results of an experiment where we analyze how progressive visualizations affect exploratory analysis. Based on these results, which suggest that progressive visualizations are a viable solution to achieve scalability in data exploration systems, we develop a system entirely based on progressive computation that allows users to interactively build complex analytics workflows. And finally, we discuss and experimentally show that using visual analysis tools might inflate false discovery rates among user-extracted insights and suggest ways of ameliorating this problem."
          ],
          "keyword": [
            "Big data",
            "human-computer interaction",
            "Quantitative research"
          ],
          "primary_title": "Towards Accessible Data Analysis",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792684/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792684/"
        },
        {
          "pid": "bdr:674385",
          "object_type": "pdf",
          "abstract": [
            "Cancer is caused largely by the accumulation of somatic mutations throughout an individual's life. Recent advances in next generation sequencing (NGS) enable measurement of somatic mutations in a cohort of samples. Cancer sequencing projects like The Cancer Genome Atlas (TCGA) have generated numerous somatic mutations in thousands of tumors. This thesis addresses two challenges. The first challenge is to distinguish the handful of driver mutations that are responsible for cancer development from the multitude of passenger mutations that play no role in cancer in a cohort of samples. The second challenge is to accurately identify larger genomic variants, also known as structural variants (SV). Although several computational methods have addressed this challenge using NGS technologies, they are limited by the underlying NGS data, resulting in a significant amount of SVs remaining undetectable, particularly in highly repetitive regions of the genome. To address the first challenge, we present two computational methods. First, we introduce a combinatorial approach for the problem of identifying independent and recurrent copy number aberrations, which are gains and losses of large genomic regions ranging in size from a few kilo-bases to whole chromosomes. We show that our method outperforms other methods on simulated data and performs well on TCGA cancer datasets. Second, we introduce a statistical model to identify combinations of driver mutations that are mutual exclusivity, a pattern expected for mutations in cancer pathways. Our model is more sensitive in detecting combinations of lower frequency mutations and outperforms other methods on simulated and real data. To address the second challenge, we present a method to identify SVs in a tumor more accurately by utilizing a new sequencing technology from 10X Genomics. 10X Genomics linked-reads technology uses barcodes to label reads originating from a longer DNA molecule, which enables the construction of synthetic long reads. Our method uses both signals from paired-end reads and synthetic long reads to identify SVs. We demonstrate that our method shows high sensitivity and specificity on simulated and real data. Together, these novel algorithms help address the challenge of characterizing and identifying driver mutations in cancer genomes."
          ],
          "keyword": [
            "cancer genomics",
            "statistical model",
            "driver mutation",
            "driver pathway",
            "structural variant",
            "Algorithms"
          ],
          "primary_title": "Computational detection of driver mutations in cancer genomes",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674385/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674385/"
        },
        {
          "pid": "bdr:674340",
          "object_type": "pdf",
          "abstract": [
            "Crowdsourced training data has become a mainstay in computer vision. Some of the most significant discoveries of the last few years were made possible by crowd annotated datasets. How can researchers best exploit the vast untapped human intelligence available in the crowd? Beyond naive annotation, we explore several distinct types of crowd interaction. We poll the crowd to discover a taxonomy of visual attributes, leverage intelligent annotation protocols to label a massive dataset, and use the crowd to build detectors with minimal supervision. This dissertation comprises three sections. First, we present a pipeline for automatically generating a large set of discriminative visual attributes. We demonstrate that the crowd can generate attributes useful for scene classification and create the SUN Attribute Dataset, the largest set of attributes for scenes. The next section introduces the largest dataset of attributes for objects, the MS COCO Attributes Dataset. Using our MS COCO Attributes Dataset, a fine-tuned classification system can do more than recognize object categories -- for example, rendering multi-label classifications such as ``sleeping spotted curled-up cat\" instead of simply ``cat\". To overcome the expense of annotating thousands of MS COCO object instances with hundreds of attributes, we present an Economic Labeling Algorithm (ELA) which intelligently generates crowd labeling tasks based on correlations between attributes. The final part of this dissertation describes the Tropel system, an active learning pipeline that takes one user-provided example and bootstraps a detector using active query responses from the crowd. We use Tropel to create hundreds of detectors on-demand from unlabeled images from three domains -- ornithological images, fashion images, and street-level images of Paris."
          ],
          "keyword": [
            "visual attributes",
            "scene understanding",
            "Computer vision",
            "Active learning"
          ],
          "primary_title": "Collective Insight: Crowd-driven Image Understanding",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674340/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674340/"
        },
        {
          "pid": "bdr:674410",
          "object_type": "pdf",
          "abstract": [
            "In our modern digital age, camera-equipped gadgets such as smart-phones and tablet devices let people capture and share increasing amounts of digital image data. However, we often find our daily snapshots plagued by various forms undesirable features such as blur, low resolution and sensor noise. Undoing such artifacts is typically ill-posed and involves filling in missing image details. But how can we get more from less? How can we go beyond the limits of what can be unambiguously restored? Fortunately, the vast digital imagery available on the Internet provides a dense sampling of our visual world through time and scale, and ushers in fundamental changes in the way we approach these problems. In this dissertation, we develop data-driven example-based image restoration methods in the context of super-resolution and deblurring. Recent advancements in modern methods have been very successful at restoring salient image structures and edges, but fall short in terms of recovering details in the highest spatial frequency bands. Furthermore, much of these details are beyond the recoverable limits given a single degraded image. Motivated by this observation, this dissertation places special emphasis on exploring methods and representations that are capable of hallucinating and synthesizing novel image details in an image restoration framework. First, we investigate how to remove motion blur from photos due to camera shake. We present patch-based image priors specifically tailored towards modeling image edges and show its superior performance compared to current leading methods in blind deblurring. We further compare generic vs specific image priors for non-blind deblurring, and investigate their ability to insert image details given example images of varying levels of similarity. Second, we examine how big-data can benefit super-resolution systems and show improved ability to hallucinate image details. By moving the image representation from intensity domain to a highly expressive feature space within a Convolutional Neural Network, combined with a sparse image correspondence to better guide synthesis of local image content, we show how convincing image details can be hallucinated far beyond capabilities of existing methods from super-resolution."
          ],
          "keyword": [
            "image restoration",
            "super-resolution",
            "deblurring",
            "image synthesis",
            "image prior",
            "Image reconstruction"
          ],
          "primary_title": "The Devil is in the Details: Example-based Image Restoration and Detail Synthesis",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:674410/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:674410/"
        },
        {
          "pid": "bdr:792891",
          "object_type": "pdf",
          "abstract": [
            "We develop new representations and algorithms for three-dimensional (3D) scene understanding from images and videos. To model cluttered indoor scenes, we introduce object descriptors that account for camera viewpoint, and use structured learning to perform 3D object detection and room layout prediction. We further boost accuracy by using latent support surfaces to capture style variations of objects and help detect small objects. In outdoor environments, we incorporate semantic segmentation in a cascaded prediction framework to more accurately model the 3D scene flow of moving objects. We first propose a cloud of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image boundaries. We also propose a Manhattan voxel representation which better captures room layout geometry. Effective classification rules are learned via a structured prediction framework. Contextual relationships among categories and layout are captured via a cascade of classifiers. Furthermore, we design algorithms that use latent support surfaces to better represent the 3D appearance of large objects, and provide contextual cues to improve the detection of small objects. Our model is learned solely from annotated RGB-D images, but nevertheless its performance substantially exceeds the state-of-the-art on the SUN RGB-D database. We then focus on outdoor scene flow prediction. Many existing approaches use superpixels for regularization. We instead assume that scenes consist of foreground objects rigidly moving in front of a static background, and use semantic cues to produce pixel-accurate scene flow estimates. Our cascaded classification framework accurately models scenes by iteratively refining semantic segmentation masks, stereo correspondences, 3D rigid motion estimates, and optical flow fields. Our method has state-of-the-art performance on the challenging KITTI autonomous driving benchmark."
          ],
          "keyword": [
            "Machine Learning",
            "Computer Vision",
            "Object Detection",
            "Scene Understanding",
            "Motion Estimation"
          ],
          "primary_title": "Semantic Three-Dimensional Understanding of Dynamic Scenes",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792891/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792891/"
        },
        {
          "pid": "bdr:792889",
          "object_type": "pdf",
          "abstract": [
            "Automated program repair (APR) aims to save people time and effort by repairing a faulty program automatically. A significant branch of current APR techniques are search-based: they define a set of modification rules to create a search space of patches first and then search in the space for a correct patch. Current search-based APR techniques face two main problems: (1) the search space problem and (2) the patch overfitting problem. For (1), they define a huge search space of patches to support repairing a large set of bugs, but they are not effective in finding a correct patch within such a huge space. For (2), they are prone to producing a patch that is overfitting. An overfitting, patched program can pass the test suite but does not actually repair the bug. To address (1), we developed our APR techniques ssFix and sharpFix. ssFix finds and reuses existing code fragments (from a code database) that are syntactically related to the context of a fault to produce patches for its repair. By leveraging such syntax-related code, ssFix potentially creates a search space of patches whose size is reduced but is still likely to contain a correct patch. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results showed that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java. sharpFix is an improved version of ssFix. Compared to ssFix, it uses different code search and code reuse methods. For the 357 Defects4J bugs, sharpFix repaired in total 36 bugs with correct patches generated. To address (2), we developed our patch testing technique DiffTGen which identifies a patched program to be overfitting by first generating new test inputs that uncover semantic differences between the original faulty program and the patched program, then testing the patched program based on the semantic differences, and finally generating test cases. We evaluated DiffTGen and found that it identified 49.4% overfitting patches in our patch dataset and that it can help an APR technique in producing less overfitting patches and more correct ones."
          ],
          "keyword": [
            "Software engineering",
            "Automated Program Repair",
            "Code Search",
            "Patch Testing"
          ],
          "primary_title": "Towards Improving the Effectiveness of Automated Program Repair",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792889/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792889/"
        },
        {
          "pid": "bdr:792914",
          "object_type": "pdf",
          "abstract": [
            "Recent research has proposed a variety of cross-cutting tools to help monitor and troubleshoot end-to-end behaviors in distributed systems. However, most prior tools focus on data collection and aggregation, and treat analysis as a distinct step to be performed later, offline. This restricts the applicability of such tools to only doing post-facto analysis. However, this is not a fundamental limitation. Recent research has proposed tools that integrate analysis and decision-making at runtime, to directly enforce end-to-end behaviors and adapt to events. In this thesis I present two new applications of cross-cutting tools to previously unexplored domains: resource management, and dynamic monitoring. Retro, a cross-cutting tool for resource management, provides end-to-end performance guarantees by propagating tenant identifiers with executions, and using them to attribute resource consumption and enforce throttling decisions. Pivot Tracing, a cross-cutting tool for dynamic monitoring, dynamically monitors metrics and contextualizes them based on properties deriving from arbitrary points in an end-to-end execution. Retro and Pivot Tracing illustrate the potential breadth of cross-cutting tools in providing visibility and control over distributed system behaviors. From this, I identify and characterize the common challenges associated with developing and deploying cross-cutting tools. This motivates the design of baggage contexts, a general-purpose context that can be shared and reused by different cross-cutting tools. Baggage contexts abstract and encapsulate components that are otherwise duplicated by most cross-cutting tools, and decouples the design of tools into separate layers that can be addressed independently by different teams of developers. The potential impact of a common architecture for cross-cutting tools is significant. It would enable more pervasive, more useful, and more diverse cross-cutting tools, and make it easier for developers to defer development-time decisions about which tools to deploy and support."
          ],
          "keyword": [
            "Cloud computing",
            "Debugging in computer science",
            "Computer networks"
          ],
          "primary_title": "A Universal Architecture for Cross-Cutting Tools in Distributed Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792914/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792914/"
        },
        {
          "pid": "bdr:792916",
          "object_type": "pdf",
          "abstract": [
            "Syntactic sugar is pervasive in language technology. Programmers use it to shrink the size of a core language; to define domain-specific languages; and even to extend their language. Unfortunately, when syntactic sugar is eliminated by transformation, it obscures the relationship between the user’s source program and the transformed program. First, it obscures the evaluation steps the program takes when it runs, since these evaluation steps happen in the core (desugared) language rather than the surface (pre-desugaring) language the program was written in. Second, it obscures the scoping rules for the surface language, making it difficult for ides and other tools to obtain binding information. And finally, it obscures the types of surface programs, which can result in type errors that reference terms the programmer did not write. I address these problems by showing how evaluation steps, scoping rules, and type rules can all be lifted—or resugared—from core to surface languages, thus restoring the abstraction provided by syntactic sugar."
          ],
          "keyword": [
            "Programming languages (Electronic computers)",
            "Syntactic Sugar"
          ],
          "primary_title": "Resugaring: Lifting Languages through Syntactic Sugar",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792916/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792916/"
        },
        {
          "pid": "bdr:792909",
          "object_type": "pdf",
          "abstract": [
            "An integral component of the modern computing era is the ability to outsource data and computation to remote Cloud Service Providers or CSPs. The advent of cloud services, however, raises important challenges of in terms of integrity and privacy of data and computation. As soon as users delegate computation to cloud platforms (such as Microsoft Azure or Amazon AWS), concerns related to integrity of the results arise. For example, have all correct inputs been used in the computation? Were all the computation steps applied in the correct order? Have the results been delivered untampered? Moreover, in the face of an alarming number of data breaches and massive surveillance programs around the globe, the privacy of outsourced data is becoming more important than ever. This thesis focuses on designing efficient privacy-preserving and verifiable data processing queries for a rich class of data structures along with prototype implementation and experimental validation. In particular, we focus on the following setting: how can a trusted data owner outsource her data to an untrusted server, such that the server will not be able to cheat while answering queries on the stored data. In other words, we require the server to produce a cryptographic proof for each answer it produces. Moreover, we require the proofs to be privacy-preserving, i.e., they should not leak any information about the data structure or the updates on it besides what can be inferred from the answers. We also consider another dimension of privacy for verifiable outsourced data-processing, namely, encrypting the outsourced data. More concretely, we consider the setting where the data structure is encrypted before outsourcing using a customized encryption scheme that allows the server to compute queries on the encrypted data. Furthermore, the client can efficiently check if the server has correctly computed the answer. In this thesis, we focus on range queries, closest point queries, dictionary queries, set algebraic queries and reachability and shortest path queries on general graphs."
          ],
          "keyword": [
            "Computer security",
            "Cryptography"
          ],
          "primary_title": "Integrity and Privacy in the Cloud: Efficient algorithms for secure and privacy-preserving processing of outsourced data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792909/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792909/"
        },
        {
          "pid": "bdr:792905",
          "object_type": "pdf",
          "abstract": [
            "Just as an interconnected-computerized world has produced large amounts of data resulting in exciting challenges for machine learning, connected households with robots and smart devices will provide developers with an opportunity to build technologies that learn from personalized household data. However, there exists a dilemma. When limited data is available for a user, for example when they initially procure a new smart device or robot, there will be a substantial burden placed on that user to personalize it to their household by the learner. At the outset, applying predictions learned from a general population to a user will provide better predictive success. But as the amount of data provided by the user increases, intelligent methods should choose predictions more heavily weighted by the individuals examples. This work investigated three problems to find algorithms that learn from both the general population and specialize to the human individual. We developed a solution to reduce the interactive burden when telling a robot how to organize a kitchen by applying a context-aware recommender system. Also, using the paradigm of trigger-action programming made popular by IFTTT, we sought to improve the programming experience by learning to predict the creation of programs from the user's history. Finally we developed several methods to personalize grounding natural language to these trigger-action programs. In a smart home where a user can describe to an intelligent home automated system rules or programs they desire to be created, their utterances are highly context dependent. Multiple users may use similar utterances to mean different things. We present several methods that personalize the machine translation of these utterances to smart home programs. This work presents several problems that show that learning algorithms that learn from both a general population and from personalized interactions will perform better than either learning approach alone."
          ],
          "keyword": [
            "Machine Learning",
            "Artificial intelligence",
            "Robotics"
          ],
          "primary_title": "Algorithms for the Personalization of AI for Robots and the Smart Home",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:792905/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:792905/"
        },
        {
          "pid": "bdr:733536",
          "object_type": "pdf",
          "abstract": [
            "Smartphone and tablet users are sensitive to the battery drain exerted by mobile applications. In this dissertation, we focus on extending the battery lifetime of mobile devices. Using the perspective of a software engineer, we present a series of techniques that modify the behavior of mobile applications and increase their energy efficiency. Our first contribution is Application Modes, a development aid that abstracts ancillary energy-management blocks (e.g., resource monitoring), thus allowing developers to focus on energy-related changes in software. Modes encapsulate code changes that modify the behavior of applications and their energy consumption. We show via examples that careful attention to application functionality substantially improves battery life. An alternative to changing functionality at compile time is to do so at runtime. Our second contribution is Tamer, an execution controller that monitors software events, rewires program binaries, and changes runtime behavior. Tamer interposes the execution of user-invisible energy-hungry software tasks, thus preventing or postponing their continuation. Developers can use Tamer as a tool to perform what-if analyses on the impact of battery life arising from potential code changes. Through a selective application of rate-limiting policies to demanding apps of the Android OS, we show that Tamer can effectively mitigate the excessive battery drain due to frequent wakeups of tasks running in the background. A Tamer policy specifies which events to interpose and when actuation should occur. To write effective policies, developers must first understand which events are worth interposing. Our third contribution is Meerkat, an analysis tool that correlates software-execution logs with power traces. Meerkat leverages data-mining techniques to discover the interestingness of sequences of events and associates them with energy consumption. Meerkat helps developers uncover sequences of function calls that draw the most power, which will serve as input to effective policies. We demonstrate Meerkat’s effectiveness in the domain of networked and I/O-intensive applications. The insights delivered by our proposed techniques accelerate the investigation on the energy efficiency of mobile software. Given appropriate support, developers can better understand how to improve battery life and adopt combinations of software-adaptation strategies."
          ],
          "keyword": [
            "causal inference",
            "Li-ion Batteries"
          ],
          "primary_title": "Software Analysis and Development for Energy Efficiency in Mobile Devices",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:733536/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:733536/"
        },
        {
          "pid": "bdr:ynauju9x",
          "object_type": "pdf",
          "abstract": [
            "Machine learning models, or algorithms trained on historical data, can be harmful when they are used to make decisions about people. From underfitting to overfitting, from bugs in code to biased training data, machine learning engineers and other stakeholders alike are concerned with the capacity of these algorithms to make problematic decisions while going unnoticed by their human supervisors, escaping scrutiny due to their increasing complexity and incomprehensibility. In recent years, this has led to a flurry of technical research on methods to anticipate, detect, or prevent undesirable behavior, under the umbrella terms of “fair machine learning” and “explainable artificial intelligence.” But these efforts have not gone without criticism, especially from social science disciplines quick to point out problems with a techno-solutionist approach. In this thesis, we concern ourselves with trying to articulate what exactly we can learn from quantitative methods proposed to address this problem and what we should do with that information. In the first Part, which centers on a family of “explainability” techniques known as feature importance, we quantitatively and qualitatively evaluate specific algorithms based on game theoretic principles, and interrogate the philosophy behind the overall methodology. The next Part articulates and draws attention to specific machine learning harms and failure modes that must be treated as public policy issues and offer suggestions on how to do so. For the third Part, we derive novel, data-dependent, group-specific generalization bounds on the performance of machine learning models trained on heterogeneous data, which offers a framework to understand the data and modeling conditions which cause performance disparities across those groups."
          ],
          "keyword": [
            "Fair Machine Learning",
            "Explainable AI",
            "Technology and law"
          ],
          "primary_title": "Explainability, fairness, and evaluation in machine learning: From theory to policy and back",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ynauju9x/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ynauju9x/"
        },
        {
          "pid": "bdr:wb8tus4x",
          "object_type": "pdf",
          "abstract": [
            "Direct-to-consumer genetic tests (DTC-GT), opposed to their clinical counterparts, are marketed directly to customers, and can be bought online or in stores. Use-cases for DTC genetic tests vary widely, from health and paternity testing to romantic matchmaking and wine taste profiling. Several hundred DTC-GT companies have emerged since the early 2000's, pointing to a large scale of genetic data usage in the DTC environment. This space contains high volumes of partnerships, acquisitions, and absorptions, which introduces questions about where consumers’ data is stored, who it is shared with, the types of data these companies can access, and how it is used both while the involved companies are operational, and after they cease. This thesis studies the direct-to-consumer genetic testing space to inform our understanding of existing and potential harms associated with misuse of genetic data. To accomplish this, we research and characterize the data flows, taxonomy of use-cases, laws, guidelines, and regulations governing the DTC-GT environment. This analysis strives to not only heighten awareness of blindspots in this space, but also make recommendations for improvements in areas such as regulation and transparency between companies and consumers. Our recommendations, detailed in a dedicated chapter, include public awareness campaigns to protect consumers, revisions to clauses in privacy acts to more actionably define accountability, regular incident and compliance reporting, and more. With recent events from 2023 such as 23andMe’s data breach that affected approximately 7 million, roughly half, of its users [3], and the FTC settlement addressing DTC-GT company 1Health’s failure to secure its users’ DNA and health data [4], there has been an uptick in concerning incidents in this space that warrants attention and action."
          ],
          "keyword": [
            "Genetic testing",
            "Harm analysis",
            "Direct-to-consumer"
          ],
          "primary_title": "Direct-to-Consumer Genetic Testing: Data Flow, Governance, and Recommendations to Mitigate Harm",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:wb8tus4x/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:wb8tus4x/"
        },
        {
          "pid": "bdr:xcvb947w",
          "object_type": "pdf",
          "abstract": [
            "With the rise of machine learning and growing attention to issues of racial injustice in the USA, there is renewed energetic discussion about how to teach students about ethics and the social impacts of computing. Talks and papers on these projects largely focus on case studies and examples that can be included in assignments. My thesis instead takes a pedagogic perspective. Drawing on papers from various disciplines including math education and cognitive science, we explored various pedagogic approaches to teach socially responsible computing. One recurring challenge in our exploration of pedagogical approaches has been guiding students to move beyond surface-level responses when engaging with case studies. An intriguing strategy we have identified is contrasting cases, where students are presented with multiple distinct instances of a concept, each differing in deep features. We conducted a study to evaluate the effectiveness of employing contrasting cases in teaching socially responsible computing (SRC), aiming to explore how this approach helps students understand and analyze the nuances of SRC concepts."
          ],
          "keyword": [
            "Education",
            "Computer science"
          ],
          "primary_title": "Using Contrasting Cases to Teach Socially Responsible Computing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:xcvb947w/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:xcvb947w/"
        },
        {
          "pid": "bdr:d68mp7u2",
          "object_type": "pdf",
          "abstract": [
            "Neural network models have grown in popularity to be the dominant artificial intelligence paradigm of our time, succeeding across a variety of challenging tasks despite largely receiving unstructured sequential input. Despite this broad success, it is a classically open question whether they can represent and generalize \\textit{symbolic structure}; in which there are entities which are represented as atomic symbols (CAT, MUFFIN), and there are abstract content-independent functions that operate over those symbols (e.g. NOT, AND, OR). Classically, whether neural network models can learn and generalize symbolic structure has been the subject of much debate. However, modern neural models (e.g. large language models) succeed on some tasks that appear to need symbolic structure, such as language processing and code generation. We hypothesize that prototype neural networks can learn to perform well at a set of tasks previously argued to involved symbolic structure. We evaluate LSTM- and Transformer-based models at small scale with low inductive biases and without extensive architectural engineering on tasks thought to have symbolic structure. We focus on carefully controlled experimental paradigms and small models in order to yield interpretable results. In the first chapter, we evaluate whether language models can differentiate logical operators in a symbolic reasoning task within a propositional logic setting, and find that models' performance relies on the degree to which the operators are separable given their distribution in the data. In the second chapter, we evaluate object-tracking vision models at a task from the developmental psychology literature which tests reasoning by exclusion within a visual setting, and find that models do not generalize to the logical inference when it is not explicitly featured in their training data. Finally, within a computational cognitive neuroscience setting, we find that Transformer models trained on a working memory task mimic biological functionality for storing and recalling items within working memory, despite not having an explicit ``memory'' component themselves. Overall, our results provide insight into the conditions within the training data and architecture of neural models which are necessary for representations of symbolic structure to arise, which can inform evaluation of neural models across the field of artificial intelligence."
          ],
          "keyword": [
            "Machine Learning",
            "Cognitive science",
            "Artificial intelligence",
            "Neural networks (Computer science)"
          ],
          "primary_title": "The Emergence of Symbolic Structure from Data in Prototype Neural Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:d68mp7u2/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:d68mp7u2/"
        },
        {
          "pid": "bdr:b5y72amx",
          "object_type": "pdf",
          "abstract": [
            "Grassroots organizing is a process by which people work from within marginalized communities to effect social, political, economic, and environmental change. People who engage in grassroots organizing have a complex relationship with digital technologies, which both facilitate their organizing work and render them more vulnerable to digital surveillance, violence, incarceration, and harassment. Digital privacy tools based on cryptographic protocols mitigate some of the risks of digitally-facilitated organizing, though they are seldom designed with activists' needs in mind, and can present additional accessibility, usability, and efficiency challenges. In this work, I introduce cryptographic design principles that aim to center activists' lived needs and experiences, as documented in numerous case studies of grassroots technology practice. I consider protocols for specific organizing tasks such as vetting and sharing information on a need-to-know basis, as well as relevant threat models such as security in the event of device compromise, also known as adaptive corruption. Finally, I propose a theory and practice of abolition cryptography—cryptography that is explicitly designed to help grassroots movements dismantle harmful systems and replace them with systems that sustain human lives and livelihoods."
          ],
          "keyword": [
            "Cryptography",
            "Grassroots Organizing"
          ],
          "primary_title": "Cryptography for Grassroots Organizing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:b5y72amx/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:b5y72amx/"
        },
        {
          "pid": "bdr:1129365",
          "object_type": "pdf",
          "abstract": [
            "Reinforcement learning defines the problem facing agents that learn to make good decisions through action and observation alone. To be effective problem solvers, such agents must efficiently explore vast worlds, assign credit from delayed feedback, and generalize to new experiences, all while making use of limited data, computational resources, and perceptual bandwidth. Abstraction is essential to all of these endeavors. Through abstraction, agents can form concise models of their environment that support the many practices required of a rational, adaptive decision maker. In this dissertation, I present a theory of abstraction in reinforcement learning. I first offer three desiderata for functions that carry out the process of abstraction: they should 1) preserve representation of near-optimal behavior, 2) be learned and constructed efficiently, and 3) lower planning or learning time. I then present a suite of new algorithms and analysis that clarify how agents can learn to abstract according to these desiderata. Collectively, these results provide a partial path toward the discovery and use of abstraction that minimizes the complexity of effective reinforcement learning."
          ],
          "keyword": [
            "Machine Learning",
            "Reinforcement learning",
            "Artificial intelligence",
            "Abstraction",
            "Sequential Decision Making"
          ],
          "primary_title": "A Theory of Abstraction in Reinforcement Learning",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:1129365/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:1129365/"
        },
        {
          "pid": "bdr:3uh99kat",
          "object_type": "pdf",
          "abstract": [
            "In the era of big data, distributed data stores have become an indispensable necessity. Almost all these data stores store sensitive data such as our voicemails, messages, emails, shopping data, health records, etc. Unfortunately, with an increasing number of data breaches on a regular basis, privacy has become a major concern. Encryption is often proposed as a solution to privacy issues, however, naively applied encryption leads to brittle systems that fail to provide satisfactory privacy guarantees; it is only through careful system-wide analysis that complicated systems can achieve provable privacy. This thesis formalizes the use of end-to-end encryption (where data is kept encrypted at all times) in distributed hash tables (DHTs) and key-value stores (KVSs). Both are fundamental in the design of storage systems we use today. For example, Amazon’s Dynamo KVS underlies its shopping cart, Facebook’s Cassandra KVS supports Messenger and Google’s Bigtable KVS manages data in Gmail. Integrating end-to-end encryption into these basic building blocks would therefore allow us to support privacy-preserving systems which would greatly increase the confidentiality of our data. In particular, we introduce the notion of encrypted DHTs and encrypted KVSs and provide security definitions that capture the security properties one would desire from such encrypted systems. We then isolate the key properties (such as load balancing, equivocation) needed from the plaintext DHTs and KVSs to have secure constructions. Finally, we give constructions of encrypted DHTs and encrypted KVSs and formally analyze their security under our security definitions. Also, to show that the required properties are indeed achievable in practice, we study common DHTs and KVSs and show that they satisfy all the requirements."
          ],
          "keyword": [
            "Secure DHTs"
          ],
          "primary_title": "Encrypted Distributed Hash Tables",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:3uh99kat/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:3uh99kat/"
        },
        {
          "pid": "bdr:918797",
          "object_type": "pdf",
          "abstract": [
            "Facility-placement and route-planning optimization problems aim to minimize the cost of providing a service to a set of clients. These problems arise frequently in a variety of contexts across computer science and operations research. Some of the most classic and well-studied of these including k-Center and the Traveling Salesperson Problem (TSP), though versatile, fail to adequately model the fact that actual service-providers have limited capacities. Facilities and vehicles have limited size (e.g. hospitals and buses can only hold so many people) as well as limited resources (e.g. staff, supplies, fuel, fleet size etc.), so are often unable to serve all clients at once. Most placement and route-planning algorithms cannot easily be extended to accommodate these considerations without drastic sacrifices to performance. In this thesis, we present approximation algorithms that provide guaranteed near-optimal solutions to these types of planning problems. Moreover, our algorithms are specifically designed for metrics that model transportation networks: planar and bounded-genus graphs, graphs with bounded treewidth, and graphs with bounded highway dimension. We capitalize on the structure of these metrics to improve the performance of our algorithms. We first consider Capacitated Domination problems, which aim to minimize the number of facilities required to serve all clients without exceeding facility capacities. For fixed capacities, we present an approximation scheme for planar graphs. Next, we address Capacitated Vehicle Routing, in which vehicles can only visit a limited number of clients in each trip. For arbitrary capacities, we improve the best-known approximation ratio for vehicle routing in trees. For fixed capacities, we design the first approximation schemes for vehicle routing in planar graphs and graphs of bounded highway dimension. Finally, we present a framework for designing vehicle-routing approximation schemes in tree metrics that can be customized to address many different vehicle-routing problems. In particular, the framework accommodates such constraints as vehicle distance (e.g. those imposed by fuel limits or driver fatigue), vehicle capacity, fleet size, and client regret."
          ],
          "keyword": [
            "Approximation Algorithms",
            "Vehicle routing problem",
            "Transportation Network",
            "Graph algorithms"
          ],
          "primary_title": "Approximation Algorithms for Capacitated Facility-Placement and Vehicle-Routing Problems in Transportation Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918797/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918797/"
        },
        {
          "pid": "bdr:918766",
          "object_type": "pdf",
          "abstract": [
            "While humans have physically and cognitively evolved to work alongside and communicate with each other, humans and robots cannot intuit each others behavior. We can neither accurately understand the other's state nor anticipate their future actions. This is for two reasons: one, robots lack a theory of mind; two, the physical modalities (i.e. ways of communicating) of human-human interaction do not translate well to human-robot interaction. The aim of this dissertation is to improve human-robot interaction by combining partially observable Markov decision process (POMDP)-based interaction managers with virtual and augmented reality interfaces. POMDP based interaction systems allow the robot to reason about what it does and does not know, and what the human may or may not know. Virtual and augmented reality addresses the problem of bi-directional communication by creating new interaction channels that can replace hard to replicate human-human modalities. Subtle body language cues or facial expressions that a robot cannot do are replaced with 3D visualizations, either in a virtual scene or are superimposed on the real world. Combining these technologies enables untrained humans to effectively direct, and communicate with, robots."
          ],
          "keyword": [
            "human-robot interaction",
            "Mixed reality",
            "Bayesian statistical decision theory"
          ],
          "primary_title": "Multimodal Human-Robot Interaction With Decision Theory and Mixed Reality",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918766/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918766/"
        },
        {
          "pid": "bdr:918868",
          "object_type": "pdf",
          "abstract": [
            "This work unifies insights from the systems and functional programming communities, in order to enable compositional reasoning about software which is nonetheless efficiently realizable in hardware. It exploits a correspondence between design goals for efficient concurrent data structures and efficient immutable persistent data structures, to produce novel implementations of mutable concurrent trees with low contention and an efficient snapshot operation to support speculative execution models. It also exploits commutativity to characterize a design space for integrating traditional high-performance concurrent data structures into Software Transactional Memory (STM) runtimes, and extends this technique to yield a novel algorithm for concurrent execution of so-called \"smart contracts\" (specialized programs which manipulate the state of blockchain ledgers)."
          ],
          "keyword": [
            "Computer multitasking",
            "Data structures (Computer science)",
            "Functional programming (Computer science)",
            "Blockchains (Databases)",
            "Smart Contracts",
            "Software Transactional Memory"
          ],
          "primary_title": "Adapting Persistent Data Structures for Concurrency and Speculation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918868/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918868/"
        },
        {
          "pid": "bdr:918774",
          "object_type": "pdf",
          "abstract": [
            "Today’s tools to improve the reliability and manageability of networks can be generally classified into two different classes: before-the-fact network verification and after-the-fact network troubleshooting. Unfortunately, neither of the two classes can individually make the network fully reliable and predictable due to fundamental limitations. Recently, there have been proposals to make the verification and troubleshooting constantly work together in a continuous cycle. The cycle involves verifying network-wide properties with latest network configurations, then monitoring the runtime network state and localizing the root cause once some network issue happens, and changing configurations in response. However, state-of-the-art tools in this cycle cannot be used in large real-world production networks. This is because these tools fail to take some important realistic challenges into account. For example, modeling an enterprise-scale network’s behavior is non-trivial. A wrong model impacts the verification accuracy. For another example, handling complex header transformations due to multiple encapsulations is difficult, thus making existing troubleshooting efforts impractical to locate deep root causes. This dissertation introduces two practical tools fit into this cycle and address important limitations of previous works. Meanwhile, we propose a general and flexible network behavior model. The first tool, Titan, focuses on verification. It consumes configurations to be deployed and answers questions the operators have regarding the reachability of routes and packets under failure cases. We show that Titan performs orders of magnitude faster than the state-of-the-art tools and achieves near-100% verification accuracy on a production global WAN. Our second work, dShark, mainly targets troubleshooting. dShark is a general and scalable framework for analyzing in-network packet traces collected from distributed devices. With dShark, operators can quickly express their intended analysis logic without worrying about scaling and some practical challenges including header transformations and packet capturing noise. Our third work, Simon, focuses on offering network operators a general and flexible programming model on top of a stream of network events. With this model, operators can probe the network behavior with queries interactively or compose scripts for repetitive tasks, monitoring of invariants. We present the design of Simon and discuss its implementation and use."
          ],
          "keyword": [
            "Computer networks"
          ],
          "primary_title": "Towards Reliable and Predictable Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918774/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918774/"
        },
        {
          "pid": "bdr:841078",
          "object_type": "pdf",
          "abstract": [
            "This thesis examines measures of sample complexity as a means to evaluate the effectiveness and reliability of machine learning algorithms. We give an overview of VC-dimension, Rademacher complexity, and variations of those measures. We introduce the Cartesian Empirical Maximum Discrepancy (CEMD) framework, which applies Rademacher-like bounds to learning functions over combinatorial data. We use this method to evaluate algorithms for removing noise from audio files. We also introduce a progressive sampling algorithm that incorporates Rademacher complexity to the task of function selection, and we test that method on the problem of selecting between audio compression formats."
          ],
          "keyword": [
            "machine learning",
            "sample complexity",
            "confidence bounds"
          ],
          "primary_title": "Applying Rademacher-Like Bounds to Combinatorial Samples and Function Selection",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:841078/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:841078/"
        },
        {
          "pid": "bdr:qkg6zvsg",
          "object_type": "pdf",
          "abstract": [
            "Sorting is one of the most fundamental algorithms in Computer Science and a common operation in databases. DB systems need to sort large amounts of data all the time, and they need to do that in the shortest amount of time possible. The most commonly used sorting algorithms in databases are optimized variations of Quicksort that run in O(N log N ) time on average. There exist O(N) sorting algorithms, like Radix Sort, which is very fast on numerical data. However, these algorithms usually are sensitive to the key length and cannot be easily generalized for data types. Nevertheless, there is more potential to be explored in that space since specialized and data-centric optimizations are, arguably, much more efficient than general-purpose solutions. Even though most real-world datasets contain complex distributions, the Recursive Model Index learning framework provides a robust method for approximating the Empirical Distribution Function. Therefore, it is compelling to import this technique onto the design of sorting, one of the most fundamental operations in DBMS systems whose efficiency affects a large portion of data processing pipelines. This dissertation explores that idea and introduces a novel, high-performing, learning-enhanced sorting algorithm called LearnedSort that effectively leverages distribution modeling techniques and outperforms traditional sorting algorithms. We will describe the engineering process and examine the algorithmic design through the lens of practical, implementation-specific trade-offs. Then, we evaluate LearnedSort's efficiency with respect to state-of-the-art sorting algorithms and observe that it demonstrates clear performance advantages in extensive benchmarks. Its high sorting throughput is evident in synthetic and real inputs of varying sizes, skew, and duplicates. We also used the learned model technique with datasets too large to reside in memory, and we observed equally strong performance metrics for both uniform and skewed distributions. Furthermore, our external sorting algorithm has beaten the current world record of the most energy-efficient string sorting algorithm by 41%."
          ],
          "keyword": [
            "Machine Learning",
            "Databases",
            "Algorithms"
          ],
          "primary_title": "Engineering a high-performing, learning-enhanced sorting algorithm",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:qkg6zvsg/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:qkg6zvsg/"
        },
        {
          "pid": "bdr:atm2tvyv",
          "object_type": "pdf",
          "abstract": [
            "In recent years, there has been a renewed interest in near-memory processing (NMP) architectures as a workaround for the performance and energy issues of frequent and irregular memory access, which are prevalent in today's data-intensive applications. With NMP architectures, simple yet data-intensive computations can be offloaded to near-memory processing units in order to reduce data movement across the memory hierarchy. While the idea is intuitive, programmability remains a major concern for the widespread dispersion of NMP. The lack of consensus among computer architects on the design details of NMP hardware have impeded the development of a common programming interface; moreover, even with a well-defined programming interface, a thorough understanding of the architecture's features and limitations will be crucial in order to adapt application software to fully exploit NMP. As this is the case, we determined that it is most appropriate to begin with redesigning fundamental, general-purpose data structures for new NMP architectures. Data structures are basic building blocks of software, yet because their implementations are encapsulated as packaged software libraries, developers can easily take advantage of highly optimized data structures without having to understand their complex implementation details. Therefore, if commonly used data structures are redesigned to optimally exploit NMP architecture, the benefits can easily extend to many applications. This dissertation focuses particularly on the NMP-aware redesign of concurrent data structures. Much of the difficulties in translating applications and algorithms to execute on NMP architecture arise from reinforcing parallelism and ensuring correct, concurrent access to data. Nonetheless, existing high-performance data structures have been optimized for high multithreaded concurrency, and NMP-based data structures must retain the concurrency and correctness guarantees in order to be useful in large-scale applications. To this end, this dissertation conducts a holistic software-hardware co-design with conservative assumptions on the baseline NMP hardware. The first part of the dissertation work provides a thorough empirical evaluation of NMP-based data structure designs that had been verified by theoretical assumptions only. The empirical evaluation using full-system architecture simulations uncovers bottlenecks that had been overlooked by prior theoretical analysis and proposes simple hardware adjustments that reduces their impact. The second part of the dissertation work introduces and evaluates new NMP-aware algorithms for data structures that have been optimized for on-chip cache locality in conventional non-NMP systems."
          ],
          "keyword": [
            "Computer architecture",
            "Data structures (Computer science)",
            "concurrent computing"
          ],
          "primary_title": "Concurrent Data Structures with Near-Memory Processing: Software-Hardware Co-Design",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:atm2tvyv/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:atm2tvyv/"
        },
        {
          "pid": "bdr:yydvkm3m",
          "object_type": "pdf",
          "abstract": [
            "Robotic perception is a key step in any autonomous robotic task including manipulation, localization and planning. The more precise the perception system is, the more complicated and detailed tasks the robot can carry out. Since robots are operating in a dynamic environment, robot perception algorithms need to be robust to different interference. Development in perception algorithms requires increasingly complex algorithms, making real-time perception challenging on the robot computing platforms. Domain-specific hardware accelerators offer the opportunity for creating the optimal hardware design with low-power execution. Although such accelerators have been widely studied for neural networks inference, their applications in the field of robotics are limited. In this dissertation work, we create both software and hardware solutions for energy efficient robust robot perception systems. We focus on Monte-Carlo based generative algorithms for 6 DoF rigid and articulated object pose estimation. We show that by combining generative inference algorithm with neural network output as a prior distribution, we can perform efficient inference with robust performance and explainable results. In this work, we focus on algorithms of particle-filtering and belief propagation and accelerate the two algorithms on FPGA through optimized dataflow design, deep pipelined processing units, and concurrent memory access. We are able to achieve significant runtime, power and energy improvement compared to both high-performance and low-power embedded GPU implementation."
          ],
          "keyword": [
            "Field programmable gate arrays",
            "Computers",
            "Robotics"
          ],
          "primary_title": "Towards Robust and Energy-efficient Robot Perception Compute System",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:yydvkm3m/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:yydvkm3m/"
        },
        {
          "pid": "bdr:wgtu3pq6",
          "object_type": "pdf",
          "abstract": [
            "Students often tackle programming problems with a flawed understanding of what the problem is asking. Some pedagogies attempt to address this by encouraging students to develop examples in the form of input–output assertions (henceforth “functional examples”), independent of (and typically prior to) developing and testing their implementations. However, without an implementation to run examples against, examples are impotent and do not provide feedback. Consequently, students may be inclined to begin their implementations prematurely—a process whose comparatively ample feedback may mask underlying misunderstandings and instill a false sense of progress. In this dissertation, I demonstrate that providing students with timely feedback on their functional examples incentivizes them to develop functional examples, improves the quality of their test cases, and may improve the correctness of their implementations."
          ],
          "keyword": [
            "Education",
            "Computer science"
          ],
          "primary_title": "Executable Examples: Empowering Students to Hone Their Problem Comprehension",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:wgtu3pq6/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:wgtu3pq6/"
        },
        {
          "pid": "bdr:gugu6tkn",
          "object_type": "pdf",
          "abstract": [
            "Personal data is under constant threat in the modern world --- from corporations looking to profit from over-collection and sale of personal data, to criminal interests who steal data for ransom, identity theft, and personal and corporate secrets. In response to the alarming rise in the exploitation of data, governments worldwide have begun enacting privacy legislation to give users more control over their personal data. However, there are technological constraints to making current systems compliant. Legacy systems are unlikely to have been designed with privacy considerations in mind. As such, it is difficult to instrument them in order to support the degree of transparency and access that are mandated by privacy legislation. Even if systems are instrumented to support user control over their data, they are still vulnerable to insider attacks and large-scale data breaches. The only fool-proof method to protect against such breaches is to use encryption for private data. However, plain encryption reduces the utility of outsourcing data, in that it does not allow the user to operate on their own data without downloading all of it. Then we turn to cryptographic primitives such as fully-homomorphic encryption (FHE), structured encryption (STE), property-preserving encryption (PPE) and oblivious RAM (ORAM). These primitives have all been widely studied and used to build systems that support various degrees of operation over encrypted data. Each of them also offers different trade-offs in efficiency and security. The security of these primitives can be quantified in terms of the leakage i.e., meaningful information that is visible to an adversarial server. In this thesis, we describe work that advances the state-of-the-art in compliant and secure databases. We present: (1) a tool that will largely automate GDPR access requests on legacy databases, thereby reducing the manual work required to deal with custom schema and application logic; (2) a general leakage suppression framework for structured encryption schemes that support updates to the data structure, and, (3) efficient leakage suppression techniques for dictionary encryption schemes that do not support updates to the underlying dictionary structure."
          ],
          "keyword": [
            "Databases",
            "Structured Encryption",
            "Leakage Suppression",
            "GDPR Compliance"
          ],
          "primary_title": "Compliant and Secure Databases",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:gugu6tkn/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:gugu6tkn/"
        },
        {
          "pid": "bdr:ewxb2c65",
          "object_type": "pdf",
          "abstract": [
            "Mechanism design is a branch of game theory and economics that is used to create methods for allocating scarce resources based on user reports, such as the value for a good being auctioned off. One of the desiderata of mechanism design is to design truthful mechanisms, in which it is optimal for participants to submit truthful reports. In his seminal work on mechanism design, Myerson describes how to design a truthful mechanism that can optimize expected revenue. This mechanism is applicable when participants are fully characterized by a single parameter, called their type, which is assumed to be private to each participant. While Myerson's mechanism relies on full distributional information about the participants' private types, this thesis is concerned with the design of single-parameter mechanisms that relax this assumption. Myerson's optimal mechanism concerns agents with quasi-linear utilities, in which utility is linear in money. We study settings in which agent behavior is affected by the Pareto principle, also known as the 80:20 rule, which makes utilities non-linear. For example, in a contest for artwork, one can easily submit a stick figure; submitting anything at the level of the Mona Lisa is exponentially harder. Our mechanisms, which operate despite missing details about the distribution of types, such as abilities in contest settings, are truthful and guarantee near-optimal performance. Next, we study mechanisms for cloud computing, in which users' types reflect their demand for compute resources, based on their intended workload. We built a classifier to label a public dataset of virtual machine traces according to workload categories, from which we extracted per-workload resource demand distributions. Then, we designed a truthful, revenue-maximizing mechanism to allocate and price cloud resources, assuming users' demands are drawn from these empirical distributions. Our mechanism is a natural one for the cloud, as users report their needed quantity given a known value, rather than their needed value given a known quantity."
          ],
          "keyword": [
            "Computer science",
            "Computer Science and Economics",
            "Computer Science and Game Theory",
            "Computer Science and Mechanism Design"
          ],
          "primary_title": "Case Studies in Single-Parameter Mechanisms",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ewxb2c65/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ewxb2c65/"
        },
        {
          "pid": "bdr:q7xwk48c",
          "object_type": "pdf",
          "abstract": [
            "Free-hand interaction allows users to pick up, move, manipulate, and interact with virtual content using their hands. In the smartphone augmented reality (AR) environment, free-hand interaction has the potential to bring a more immersive experience to everyday AR. However, this type of interaction is not yet intuitive and efficient enough for robust or precise AR tasks on handheld devices. This thesis explores system iteration, behavioral analysis, and personalized interaction models with the goal of enabling more intuitive and efficient free-hand experiences with a specific focus on smartphones. During free-hand interaction, visual, haptic, and auditory feedback improves users’ interaction performance and lowers the overall cognitive load that arises due to a lack of depth information and object affordances in AR. Personalizing thresholds for grabbing gestures increases successful interaction attempts, and adaptive bimanual free-hand coordination helps to further increase selection accuracy with objects as small as 3 mm in diameter. Interaction distance also affects performance, with distant free-hand interaction yielding faster performance that requires less effort, while direct manipulation creates a more immersive experience. Free-hand AR experiences can be improved on the smartphone’s small screens by sharing AR content with wearables during AR sketching or using automatic document layout extraction to author and view virtual annotations on printed documents. These findings provide a basis for designing more intuitive and efficient free-hand interaction systems, bridging the gap between advancements in 3D hand tracking and everyday smartphone applications."
          ],
          "keyword": [
            "human-computer interaction",
            "Gesture",
            "Augmented reality"
          ],
          "primary_title": "Personalizing Free-Hand Interaction for Smartphone Augmented Reality",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:q7xwk48c/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:q7xwk48c/"
        },
        {
          "pid": "bdr:kuvehb7p",
          "object_type": "pdf",
          "abstract": [
            "Reinforcement learning (RL) techniques have led to remarkable results in challenging domains such as Atari games, Go, and Starcraft, suggesting that practical applications lie just over the horizon. Before we can trust decisions made by RL policies, however, we need more visibility into how they work. To explain a reinforcement-learning agent, I propose extending the power of counterfactual reasoning to sequential domains by comparing its policy to a baseline policy at a set of automatically identified decision points. My novel method for selecting important decision points considers a large pool of candidate states and decomposes the agent's value into the reward obtained before vs. after visiting that state. A state is considered important if the accumulated reward obtained after switching to the baseline policy is most different from that obtained after continuing its policy. The engine of this computation is a decomposition of occupancy frequencies of an agent’s policy that characterize the whereabouts of an agent before and after the policy change. Structuring the policy evaluation in this way provides a causal account for its outcome. I have demonstrated the approach on a set of standard RL benchmark domains, providing explanations using the decomposed occupancy frequencies."
          ],
          "keyword": [
            "Reinforcement learning",
            "Artificial intelligence",
            "Explainable AI"
          ],
          "primary_title": "Explaining Reinforcement Learning Agents by Policy Comparison",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:kuvehb7p/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:kuvehb7p/"
        },
        {
          "pid": "bdr:mpf48b26",
          "object_type": "pdf",
          "abstract": [
            "Considerable effort and enthusiasm has been directed towards building deep generative models of 3D shapes. Existing techniques can be divided into two categories: unstructured and structured models. Unstructured models capture high-level global shape attributes well, but lack user-driven inputs and control. By contrast, structured part-based generative model synthesis enables local editing, yet the coarse proxies are a poor approximation for surface geometry. In this work, we present an ongoing generative modeling project. This modeling project began with CageSDF, a model that achieves the best of both worlds by conditioning an implicit surface generator on an arrangement of bounding cuboid proxies. CageSDF enables user-driven control at the high-level shape structure by manipulating cuboid proxies, and at the surface detail level by modifying the generator’s latent code. Key to achieving this disentanglement is a carefully-designed, self-supervised training procedure, which guarantees the model’s latent code governs only the low-level surface geometry. We demonstrate that CageSDF is effective at generating novel shapes and provides both high and low level control. We lastly explore extensions designed to improve the visual quality and quantitative efficacy of this model for upcoming conference submissions."
          ],
          "keyword": [
            "Deep learning (Machine learning)",
            "Geometry--Data processing"
          ],
          "primary_title": "Disentangling Levels of Detail in Implicit Shape Generation with Explicit Bounding Proxies",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:mpf48b26/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:mpf48b26/"
        },
        {
          "pid": "bdr:424dqpa4",
          "object_type": "pdf",
          "abstract": [
            "End-to-end encrypted relational database management systems are the “holy grail” of database security and have been studied by the research community for the last 20 years. During this time, several systems that handle some subset of SQL over encrypted data have been proposed, including CryptDB, Monomi, ESPADA, Blind Seer and Stealth. CryptDB and Monomi are based on property-preserving encryption (PPE) and have been shown to leak a considerable amount of information even in the snapshot model, which is the weakest adversarial model in this setting. While ESPADA, Blind Seer and Stealth achieve much better leakage profiles, they suffer from two main limitations: (1) they cannot handle SQL queries that include join or project operations; and (2) they are not legacy-friendly which means that, unlike CryptDB and Monomi, they require a custom database system. We design and build an encrypted database management system called KafeDB that addresses all these limitations. At the core of KafeDB are two new cryptographic schemes based on structured encryption (STE) called OPX and PKFK. In these schemes we propose STE-based techniques for optimal join, query optimization, leakage reduction and locality improvement over encrypted relational data. Overall KafeDB achieves a leakage profile comparable to the ESPADA, Blind Seer and Stealth systems. However KafeDB handles a non-trivial subset of SQL which includes queries with joins, selections and projections. In addition, KafeDB is legacy-friendly, meaning that it can be deployed on top of any relational database system. The TPC-H benchmark showed that KafeDB had 4.2× query overhead and 3.6× storage overhead over plaintext PostgreSQL."
          ],
          "keyword": [
            "Databases",
            "Computer security",
            "Cryptography",
            "Cloud computing"
          ],
          "primary_title": "Building a Structurally-Encrypted Relational Database System",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:424dqpa4/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:424dqpa4/"
        },
        {
          "pid": "bdr:8xyurz8a",
          "object_type": "pdf",
          "abstract": [
            "Software is becoming increasingly complex. To keep up with evolving applications, modern operating systems (OSes) provide a rich and continually-growing set of features through OS interfaces, system libraries, and APIs. However, applications typically have indiscriminate access to this entire range of features, regardless of how much functionality is actually required. As a result, programs are often overprivileged, allowing a potential attacker to (ab)use functionality otherwise irrelevant to the program. Existing methods to reduce application privileges are often fragile or have a prohibitive deployment cost, such as source code, which is not always available, or collection of execution traces, which inherently have limited accuracy due to incomplete code coverage. In this thesis, we present a set of techniques for reducing overprivilege without requiring access to source code or dynamic tracing, thus enabling more widespread deployment. We characterize overprivilege in applications written in type- and memory-unsafe code (C, C++, assembly) and develop low-cost tools to safely and automatically enforce the principle of least privilege by restricting program code to necessary features only. Specifically, we present sysfilter, a framework to reduce the attack surface of the OS kernel by reducing overprivilege with respect to the system call API. sysfilter builds on contemporary binary analysis to statically identify a program's system call usage by reconstructing its function call graph (FCG) so that unused system calls can be restricted. We use sysfilter to analyze ~30K binaries in a major Linux distribution to 1) characterize system call overprivilege in situ, 2) demonstrate how our techniques can analyze and improve the security posture of a wide range of programs, 3) investigate challenges to improving the precision of our analysis in real-world programs involving dynamically-loaded code and varied privileges across execution phases. Finally, we extend our FCG extraction techniques into a generic framework, libfcg, to facilitate new tools to identify overprivilege in other security domains. Our tools and distribution-wide studies demonstrate the security benefits of reducing overprivilege, as well as the feasibility of using static analysis to improve system security in a precise, effective, and scalable manner."
          ],
          "keyword": [
            "Computer security",
            "Computer software--Security measures"
          ],
          "primary_title": "Improving Application Security at Scale by Reducing System Call and Library Overprivilege",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:8xyurz8a/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:8xyurz8a/"
        },
        {
          "pid": "bdr:919224",
          "object_type": "pdf",
          "abstract": [
            "This paper presents results from an evaluation of using virtual reality techniques to measure improvement of mental rotation ability. We compared the performance improvements of university students in Mental Rotation Tasks (MRTs) in two different settings: a control setting in which participants practice mental rotation in a desktop environment, and an experimental setting in which participants practice mental rotation tasks in a virtual reality environment. Pre- and post-learning session MRT performance measures show that practicing mental rotation tasks improves the ability to do those tasks (p < .001) and that a virtual reality environment can more effectively aid learning (p = .017). We also find that low spatial ability participants benefit significantly more when learning in a virtual reality environment compared to learning in a desktop computer environment (p = .015). Survey feedback from participants suggests that VR learning sessions were helpful because the MRTs were displayed in 3D, possibly alleviating extraneous cognitive load. These results support the proposition that VR can be utilized as an aid for spatial task learning, particularly among those with lower spatial abilities."
          ],
          "keyword": [
            "Virtual reality in education",
            "Spatial ability",
            "Visualization",
            "Virtual reality",
            "Mental rotation"
          ],
          "primary_title": "Practicing in Virtual Reality Improves Mental Rotation Ability: Lower Scorers Benefit More",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919224/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919224/"
        },
        {
          "pid": "bdr:919221",
          "object_type": "pdf",
          "abstract": [
            "In this thesis we examine the AI-hard problem of natural language understanding. We use the Quora Questions dataset as motivation - strong performance on this dataset requires a classifier that can perform reading comprehension and compare meanings accurately. We explore improvements upon existing approaches to this dataset, such as the SiameseLSTM model. This is primarily accomplished by modifying the LSTM unit to its Bidirectional GRU variant, allowing the model to make better use of question context, as well as utilizing new optimization techniques to train the model. The GRU is often easier and faster to train than the LSTM and gives better results on less training data. We use the Quasi-Hyperbolic Adam optimizer from Facebook AI Research to train our model, which is a variant of the classical Adam algorithm by Kingma & Ba (2014). Our result of 88.4% testing accuracy over arbitrary train/validation/test splits is competitive with existing results and shows the importance of considering context when determining question similarity as well as the choice of optimization technique."
          ],
          "keyword": [
            "nlu",
            "Natural language processing (Computer science)",
            "question",
            "Semantics",
            "Mathematical optimization"
          ],
          "primary_title": "Natural Language Understanding within the context of Question Matching",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919221/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919221/"
        },
        {
          "pid": "bdr:919203",
          "object_type": "pdf",
          "abstract": [
            "This thesis addresses the problem of memory safety in the Linux kernel. Despite many memory isolation, control-flow integrity, and code diversification techniques existing both in user space and kernel space, many of these techniques have been proven to be less effective in the kernel and can often be bypassed using memory disclosure vulnerabilities. This thesis describes a way in which critical data, such as keyrings that hold authentication keys, encryption keys, etc., can be moved to an isolated memory location so that, in conjunction with the kernel hardening scheme kR^X, they are protected from vulnerabilities like memory leaks. The techniques described in this paper can be generalized and used with other sensitive data in the kernel like user credentials."
          ],
          "keyword": [
            "Linux",
            "Computer science",
            "Computer security",
            "Operating systems (Computers)"
          ],
          "primary_title": "Hardening the Linux Kernel Key Retention Service against Information Disclosure Vulnerabilities",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919203/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919203/"
        },
        {
          "pid": "bdr:919204",
          "object_type": "pdf",
          "abstract": [
            "In this paper, we introduce a novel framework for improving intent classification performance in domains with limited labeled data. We call our framework RIPPED: Recursive Intent Propagation using Pretrained Embedding Distances. Unlike most graph-based semi- supervised approaches, RIPPED uses dense pretrained embeddings to construct the representation graph. In combining this representation scheme with a recursive variant of the label propagation algorithm, RIPPED is able to accurately propagate labels throughout the unlabeled dataset in domains with a large number of unbalanced classes and complex, noisy decision boundaries. In a given data-poor domain, RIPPED acts as an augmentation system, adding to the labeled dataset by classifying unlabeled examples, thus allowing a more effective inductive classifier to be trained. As a result, RIPPED can be easily incorporated into any classification pipeline. RIPPED is simple to apply to new domains, and our results indicate its empirical effectiveness. On four intent classification datasets, given access to only a few labeled examples per class, RIPPED achieved performance comparable to state-of-the-art classifiers given access to the entire training dataset. In some cases (including the one-shot setting), RIPPED outperformed the next-best semi-supervised methods by more than 70%. We propose that RIPPED can be used as an out-of-the-box tool for bootstrapping natural language understanding systems in data-poor domains."
          ],
          "keyword": [
            "Machine learning",
            "Artificial intelligence",
            "Natural language processing (Computer science)"
          ],
          "primary_title": "RIPPED: Recursive Intent Propagation using Pretrained Embedding Distances",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919204/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919204/"
        },
        {
          "pid": "bdr:919219",
          "object_type": "pdf",
          "abstract": [
            "WebMesh is a browser-based computational framework for serverless applications. Unlike existing serverless frameworks which run in managed datacenters, WebMesh nodes run in browser tabs, using WebAssembly to allow C++ code to run portably in web browsers and WebRTC to allow nodes direct peer-to-peer communication. We evaluate the scalability and feasibility of such a system by implementing the MapReduce distributed computing model on the framework. We find that, in tests of a word count job running on MapReduce, WebMesh is able to handle 200,000+ concurrent jobs, execute jobs 3.6 times faster than running locally, and support a variety of tasks. We suggest it is possible to create value from otherwise unused computing power, allowing more cost-efficient distributed computing than existing serverless providers."
          ],
          "keyword": [
            "Distributed operating systems (Computers)",
            "JavaScript (Computer program language)",
            "Assembly languages (Electronic computers)",
            "MapReduce (Computer file)",
            "C++",
            "Client/server computing",
            "Web servers",
            "Web services",
            "Electronic data processing--Distributed processing",
            "Peer-to-peer architecture (Computer networks)"
          ],
          "primary_title": "WebMesh: A Browser-Based Computational Framework for Serverless Applications",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919219/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919219/"
        },
        {
          "pid": "bdr:919202",
          "object_type": "pdf",
          "abstract": [
            "Proofs of work are used to secure blockchain applications such as Bitcoin. One drawback of proofs of work is that they can be wasteful, and this drawback has prompted research into alternatives such as proofs of sequential work. This thesis investigates the open problem of whether there is a black-box construction of proofs of sequential work with unique proofs. We do not manage to solve this problem, but we do make progress on a related problem by presenting an original black-box construction of proofs of work with unique proofs."
          ],
          "keyword": [
            "Cryptography"
          ],
          "primary_title": "Proofs of sequential work with unique proofs",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919202/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919202/"
        },
        {
          "pid": "bdr:92",
          "object_type": "pdf",
          "abstract": [
            "Recently, some researchers have attempted to exploit state-aggregation techniques to compute stable distributions of high-dimensional Markov matrices. While these researchers have devised an efficient, recursive algorithm, their results are only approximate. We improve upon past results by presenting a novel state aggregation technique, which we use to give the first (to our knowledge) scalable, exact algorithm for computing the stochastically stable distribution of a perturbed Markov matrix. Since it is not combinatorial in nature, our algorithm is computationally feasible even for high-dimensional models."
          ],
          "keyword": [
            "Markov chains",
            "stochastic stability",
            "state aggregation",
            "Markov processes"
          ],
          "primary_title": "An Algorithm to Compute the Stochastically Stable Distribution of a Perturbed Markov Matrix",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:92/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:92/"
        },
        {
          "pid": "bdr:918761",
          "object_type": "pdf",
          "abstract": [
            "Recent advancements in hardware have caused a shift toward purely in-memory data processing, forcing a complete redesign of the high-overhead abstractions (e.g., Volcano-style iterators) at the core of traditional, disk-based systems. One popular replacement for these outdated query processing models is code generation, which refers to the process of generating query-specific, machine-executable code to evaluate a query. In general, code generation is highly efficient and enables a variety of low-level optimizations, yet it comes with its own set of drawbacks. This dissertation provides an in-depth exploration of code generation for in-memory data analytics. First, we present two novel code generation strategies that jointly consider properties of the operators, data, and underlying hardware to significantly improve performance compared to existing code generation approaches. Then, in order to mitigate the main disadvantages associated with code generation, we propose a new, alternative query processing model that achieves comparable performance while avoiding these downsides."
          ],
          "keyword": [
            "Databases"
          ],
          "primary_title": "Code Generation for In-Memory Data Analytics",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918761/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918761/"
        },
        {
          "pid": "bdr:xf8qz4fz",
          "object_type": "pdf",
          "abstract": [
            "I introduce novel concentration-of-measure bounds for the supremum deviation, several variance concepts, and a family of game-theoretic welfare functions. I then apply these bounds to analyze machine learning and data science problems, with deep theoretical consequences and impactful practical ramifications. Parts I and II assume an independently and identically distributed (i.i.d.) setting, where we observe many statistically independent occurrences, and part III extends some of my methods and themes to study non-i.i.d. mean-estimation problems. Naturally, some conclusions are weaker in these relaxed settings, but I find that many of the same data-dependent and variance-sensitivity themes apply, and give efficient algorithms for real-world problems where the i.i.d. assumption is prohibitive. This thesis is split into three parts, each representing a significant advancement in its respective field; the first in statistical learning theory, with an emphasis on algorithms and generalization guarantees making efficient use of training data; the second in the axiomatic underpinnings, practice, and statistical elements of fair machine learning; and the third in showing that themes and bounds from sampling problems in standard i.i.d. settings translate well into non-i.i.d. settings. Special attention is paid to keep each part independently readable, however the reader will better appreciate thematic connections, applications, and technical synergy when they are considered as a whole. Concretely, I note that the methods of part I and part III are not mutually incompatible, opening the door to strong uniform convergence bounds in non-i.i.d. settings, and furthermore, the fair learning setting of part II benefits from the statistical bounds of part I, and similar analysis is possible in non-i.i.d. settings. Indeed, in a connected and interdependent world, it may be the case that practical fair systems need to consider the intricacies of non-i.i.d. learning. Ultimately, the importance of philosophically grounded and statistically rigorous fair-learning systems, operating on real-world data with all the messy dependence structures that may entail, seems to be not only of immense academic interest, but also a necessary step in joining the theory and practice of machine learning and its societal impact."
          ],
          "keyword": [
            "Machine Learning",
            "Markov chain Monte Carlo",
            "Mean estimation",
            "Fair Machine Learning",
            "Statistical Learning Theory",
            "Computational learning theory",
            "Convex Optimization"
          ],
          "primary_title": "Bounds and Applications of Concentration of Measure in Fair Machine Learning and Data Science",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:xf8qz4fz/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:xf8qz4fz/"
        },
        {
          "pid": "bdr:tm42hr2z",
          "object_type": "pdf",
          "abstract": [
            "The abundance of data and ease of availability of third-party storage solutions, has led people to store their private and sensitive data on the cloud. These cloud services also allow us to do searches and updates over our data without having to download all of it to our devices. Hence, these services not only see our data in the clear but also all our searches and updates. Even if we choose to trust the cloud, data breaches are an ever-present threat. Over the last twenty years, many government institutions as well as private companies, big and small, have been targeted for people’s data including but not limited to financial and identity-related information, medical records, etc. Motivated by this privacy loss and the constant occurrence of data breaches, we study the problem of encrypted search, where a client outsources data to an untrusted server while maintaining the ability to privately query and dynamically update the stored data. From a privacy perspective, the goal is to protect the data and the operations on it from the server as well as from other unwanted parties such as hackers who could breach the server. We model real world threats, provide new formal security definitions and design new practical solutions with reasonable leakage profiles that are provably-secure under our security definitions. Specifically, we deal with persistent, snapshot and tenacious adversaries, to name a few, that capture different real-world adversarial threats. We design schemes that are secure against these different adversaries. The constructions we propose guarantee several standard security properties for dynamic searchable encryption schemes including volume-hiding, forward and backward privacy. We also introduce formal definitions for security properties such as dynamic volume-hiding and for novel properties such as past-hiding and correlation security that thwart new kinds of attacks that have not been considered in the past. We also implemented our constructions and evaluated their concrete efficiency empirically. Our experiments show that our constructions are highly efficient, where the search protocol in some schemes takes less than 1 microsecond per query/result pair."
          ],
          "keyword": [
            "Encrypted Search",
            "Searchable Encryption",
            "Structured Encryption"
          ],
          "primary_title": "Theoretical and Practical Advances in Structured Encryption",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:tm42hr2z/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:tm42hr2z/"
        },
        {
          "pid": "bdr:yfrs372n",
          "object_type": "pdf",
          "abstract": [
            "In this work, I present an implemented model that can learn interactively from natural language, enabling non-expert human trainers to convey complex tasks to machines using task decomposition and a combination of evaluative feedback and natural language. Over a series of experiments and associated algorithms, I show: - Contrary to the assumptions of methods that learn behavior interactively, feedback from people is policy dependent. - The method of COnvergent Actor-Critic by Humans (COACH) can interpret and learn from this kind of feedback. - Complex tasks can be hard to learn directly from feedback, but non-expert human trainers can decompose complex tasks into simpler units. - Geometric Linear Temporal Logic (GLTL) can be used as a logical form that can capture decomposed task descriptions and serve as the basis of an effective learning algorithm for building up complex tasks. - People can use natural language feedback to convey evaluative feedback effectively. - A deep sequence-to-sequence (Seq2Seq) approach can be used to interpret natural language feedback while discovering how to convert spontaneous natural language input to GLTL for machine execution. These elements are demonstrated in an implemented system that learns online from interaction with an end user to interpret and execute the user's tasks."
          ],
          "keyword": [
            "Machine Learning",
            "Natural language processing (Computer science)",
            "Reinforcement learning",
            "human-robot interaction",
            "Artificial intelligence",
            "Robotics",
            "Computer science"
          ],
          "primary_title": "Interactive Reinforcement Learning from Human Language and Evaluative Feedback Through Task Decomposition",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:yfrs372n/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:yfrs372n/"
        },
        {
          "pid": "bdr:95",
          "object_type": "pdf",
          "abstract": [
            "Online stochastic combinatorial optimization problems are problems in which a decision maker is trying to minimize or maximize an objective by making a sequence of discrete decisions, while acquiring information over time, and in face of an uncertain future. Two examples are: * the scheduling of drug development tasks in a pharmaceutical company, where a decision maker has to manage costly human and material resources in face of uncertainty regarding the efficiency of drugs; * the scheduling of polymerizations in a polystyrene factory, where reactors can be used to produce different varieties of polystyrene, and future demands are uncertain. We study decision making in limited time for such online stochastic combinatorial optimization problems, and focus on anticipatory algorithms, a class of algorithms that relaxes constraints due to the timing of information acquisition by the decision maker. A first question we investigate is why a specific heuristic based on this simplification perform sometimes very well. We then turn on to problems such as the two aforementioned ones on which this heuristic produces largely suboptimal decisions. To address these problems, we then propose Amsaa, the anytime multistep anticipatory algorithm, that in addition to attractive characteristics of previous anticipatory algorithms is guaranteed to produce optimal decisions with enough computation time. Empirical results show that Amsaa outperforms existing approaches on the stochastic project scheduling problem and on the polystyrene production planning problem. In particular, Amsaa scales thousands of time better than a stochastic programming approach in the project scheduling problem."
          ],
          "keyword": [
            "stochastic optimization",
            "combinatorics",
            "Algorithms",
            "Combinatorial analysis"
          ],
          "primary_title": "Amsaa: An Anticipatory Algorithm for Online Stochastic Combinatorial Optimization",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:95/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:95/"
        },
        {
          "pid": "bdr:zhchjga9",
          "object_type": "pdf",
          "abstract": [
            "In this thesis, we first contribute to the empirical-game theoretic analysis (EGTA) literature both from a theoretical and a computational perspective. Theoretically, we present a mathematical framework to precisely describe simulation-based games and analyze their properties. In a simulation-based game, one only gets to observe samples of utility functions but never a complete analytical description. We provide results that complement and strengthen previous results on guarantees of the approximate Nash equilibria learned from samples. Computationally, we find and thoroughly evaluate Probably Approximate Correct (PAC) learning algorithms, which we show make frugal use of data to provably solve simulation-based games, up to a user's given error tolerance. Next, we turn our attention to mechanism design. When mechanism design depends on EGTA, it is called empirical mechanism design (EMD). Equipped with our EGTA framework, we further present contributions to EMD, in particular to parametric EMD. In parametric EMD, there is an overall (parameterized) mechanism (e.g., a second price auction with reserve prices as parameters). The choice of parameters then determines a mechanism (e.g., the reserve price being $10 instead of $100). Our EMD contributions are again two-fold. From a theoretical point of view, we formulate the problem of finding the optimal parameters of a mechanism as a black-box optimization problem. For the special case where the parameter space is finite, we present an algorithm that, with high probability, provably finds an approximate global optimal. For more general cases, we present a Bayesian optimization algorithm and empirically show its effectiveness. EMD is only as effective as the set of heuristic strategies used to optimize a mechanism's parameters. To demonstrate our methodology's effectiveness, we developed rich bidding heuristics in one specific domain: electronic advertisement auctions. These auctions are an instance of combinatorial auctions, a vastly important auction format used in practice to allocate many goods of interest (e.g., electromagnetic spectra). Our work on designing heuristics for electronic advertisement led us to contribute heuristics for the computation of approximate competitive (or Walrasian) equilibrium, work of interest in its own right."
          ],
          "keyword": [
            "Computer Science and Economics",
            "Computer Science and Game Theory",
            "Computer Science and Mechanism Design",
            "Combinatorial Markets",
            "Machine Learning for Game Theory",
            "Machine Learning for Mechanism Design",
            "Electronic Advertisement Auctions",
            "Heuristic Bidding for Advertisement Auctions",
            "Machine Learning for Competitive Equilibria"
          ],
          "primary_title": "Learning Equilibria of Simulation-Based Games: Applications to Empirical Mechanism Design",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:zhchjga9/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:zhchjga9/"
        },
        {
          "pid": "bdr:tngkk2fu",
          "object_type": "pdf",
          "abstract": [
            "The rise of the Internet has generated and has enabled the collection of massive amounts of data. However, this modern ubiquity and abundance of data is worth little unless we can efficiently extract from it useful information and insights. In this thesis, we focus on the question of data efficiency, that is, optimizing the amount of data needed to accomplish a statistical task to some given accuracy and confidence guarantees. Drawing on tools from across probability, statistics and theoretical computer science, we propose optimally data-efficient algorithms for two basic estimation problems. The problems and their solutions, defined in distinct data-access models, highlight and give techniques to overcome important data-collection and data-utilization challenges faced by algorithm designers in the modern era. The first problem is a classic and fundamental problem in statistics: what is the best way to estimate the mean of a real-valued distribution from independent samples? Under the minimal and essentially necessary assumption that the distribution has finite (but unknown) variance, we settle the problem by presenting an estimator with convergence tight to within a 1+o(1) multiplicative factor. This contrasts previous works that are either only tight up to multiplicative constants, or require strong additional assumptions such as knowledge of the variance, or a bounded 4th moment (kurtosis) assumption. Our estimator construction and analysis gives a generalizable framework, tightly analyzing a sum of dependent random variables by viewing the sum implicitly as a 2-parameter psi-estimator, and constructing bounds using mathematical programming techniques. The second problem is a coin-flipping problem motivated by crowdsourcing applications. Given a mixture between two populations of coins, \"positive\" coins that each have---unknown and potentially different---bias >= 1/2+Delta and \"negative\" coins with bias <= 1/2-Delta, we consider the task of estimating the fraction of positive coins to within a given accuracy through drawing coins from the mixture and flipping them. We give an adaptive algorithm and a fully-adaptive lower bound with matching sample complexity, simultaneously tight in all relevant problem parameters, up to a multiplicative constant. The fine-grained adaptive flavor of both our algorithm and lower bound contrasts with much previous work in distributional testing and learning."
          ],
          "keyword": [
            "Theoretical computer science",
            "Mathematical statistics",
            "Mean estimation",
            "Crowdsourcing"
          ],
          "primary_title": "Classic and Modern Challenges in Statistical Estimation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:tngkk2fu/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:tngkk2fu/"
        },
        {
          "pid": "bdr:zmz42prz",
          "object_type": "pdf",
          "abstract": [
            "In both supervised and reinforcement settings, there exist learning problems that are hard due to having high computational or sample complexity. Researchers have shown, using standard models, that this issue causes certain classes of problems to be provably unlearnable. We propose new learning models that incorporate both iterative data augmentation techniques and human intervention to overcome these issues. In the supervised learning setting, we extend and enhance the standard learning model to include an iterative, round-based learning structure. Additionally, we introduce a benevolent, knowledgeable teaching agent to the model. With these adjustments, we are able to show theoretically that certain unlearnable classes are efficiently teachable. We achieve improved results for previously studied classes and present new results for another class in our model. Additionally, we provide evidence that non-expert human users can play the role of the teaching agent effectively. Inspired by our work in the supervised setting, we leverage the same intuitions to create a reinforcement-learning model that takes advantage of an iterative round-based learning structure. We are able to train agents to learn arbitrarily complex behaviors induced by temporally defined goals in a simple grid world. We are able to show that reinforcement given by non-expert humans is sufficient to get the learning agent to exhibit the desired behavior."
          ],
          "keyword": [
            "Machine Learning",
            "Reinforcement learning",
            "Supervised learning (Machine learning)"
          ],
          "primary_title": "Methods for Teaching Hard Learning Problems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:zmz42prz/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:zmz42prz/"
        },
        {
          "pid": "bdr:ev5agd9j",
          "object_type": "pdf",
          "abstract": [
            "This dissertation primarily contributes to the visualization and visual analytics community. It offers findings and methods to expand the visualization design space and to deepen the current understanding of visualization users. Specifically, we examine the recent advances in display devices (i.e., virtual reality head-mounted displays) and computational methods (i.e., hierarchical modeling and deep learning approaches) to assist users in understanding a statistical concept or the output of a quantitative model. The dissertation starts with a study of new visualization designs that aimed to engender users’ trust in a machine learning recommendation. This is followed by an exploratory study that used virtual reality and visualizations to facilitate the retrieval of scientific knowledge from scholarly articles. The dissertation then presents two studies that investigated users’ perception and intention when comparing the summary statistics in visualizations, aiming to understand scientifically thinking users. This dissertation also contributes to the fields of machine learning and perceptual science. It provides alternatives and answers to the shared problems from a different perspective. This dissertation supports that visualizing a machine learning result can help researchers and practitioners understand, analyze, and debug a model. The findings and methods used in the dissertation may inform researchers of perceptual science to tackle similar problems in their domains. These contributions together directly provide new designs and empirical knowledge about how to appropriately design and use visualizations; they also indirectly advance the fields via the insights and models of users’ underlying perceptual and cognitive processes. While the series of works presented in this dissertation often focuses on a specialized domain problem—the visualization studied here was often an example of a specific dataset—many results ought to generalize to a different one. We also hope that the methods and lessons learned from this dissertation can inform and inspire future studies, particularly with the extensive supplementary materials provided via the persistent interoperable identifiers. More broadly, this dissertation hopes to share examples of bringing researchers and designers from different disciplines to advance science."
          ],
          "keyword": [
            "Machine Learning",
            "Information visualization",
            "Cognition",
            "Perception",
            "Virtual reality",
            "Trust"
          ],
          "primary_title": "Fusing visualization, virtual reality, and vision science for scientific thinking",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ev5agd9j/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ev5agd9j/"
        },
        {
          "pid": "bdr:wk7zgbgx",
          "object_type": "pdf",
          "abstract": [
            "Depth reconstruction tries to obtain 3D scene geometry from incomplete or low-dimensional data — and it is usually a vital first step in many computational photography tasks. Most image-space geometric representations, however, fail to be general-purpose as they focus on a narrow set of metrics, and do not preserve all information of potential relevance. This dissertation shows that multi-view edges encode all relevant information for supporting higher level computational photography tasks that rely on depth reconstruction. We do this by presenting a novel encoding of multi-view scene geometry from structured light fields, and a reconstruction method for inverting this code. Our model is based on edges in the Epipolar Plane Images (EPIs) of a light field. These edges provide a small number of high-gradient key points and depth labels that can be used to accurately identify occlusion boundaries, and also to anchor the reconstruction in the angular domain for view-consistency. We present a differentiable representation of our model which allows the reconstruction to be optimized via gradient descent on a multi-view reconstruction loss. We evaluate our reconstruction for accuracy, view consistency, and occlusion handling to show that it retains all the geometric information required for higher level computational photography tasks."
          ],
          "keyword": [
            "multiview stereo",
            "light fields",
            "depth reconstruction",
            "scene representations",
            "Three-dimensional imaging"
          ],
          "primary_title": "Are Multi-view Edges Incomplete?",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:wk7zgbgx/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:wk7zgbgx/"
        },
        {
          "pid": "bdr:919002",
          "object_type": "pdf",
          "abstract": [
            "The adoption of cloud computing has pushed many big-data analytics frameworks to run as multi-tenant services where users submit jobs and a cluster resource manager is responsible for provisioning resources and executing jobs. In order to support a wide-range of data processing models, designers of modern cluster resource management frameworks advocate for a strict decoupling of resource management infrastructure (e.g., Apache YARN) from specific data processing models (e.g., Hadoop, TensorFlow, Spark). This dissertation introduces application-aware resource management, which advocates for judiciously pushing semantics from the application into the cluster manager in order for it to make more informed scheduling decisions. We show that application-aware cluster management can lead to significant gains in terms of decreased job completion times, increased cluster utilization, and improved job utility. We present prototypes that exemplify the benefits of our approach in systems we call Yaq and HyperDrive. We identify inefficiencies in existing cluster management framework designs that lead to poor utilization and degraded user-experience in the presence of heterogeneous workloads. We present Yaq as a cluster manager which is able to prioritize task execution based on application-level metrics such as task durations or total amount of work remaining in a job. We show that Yaq can significantly improve both cluster utilization and workload completion times. We identify several difficulties and inefficiencies in the building and training of machine-learning models in the context of shared clusters. We present HyperDrive as a cluster manager which is able to classify and prioritize jobs based on user-defined utility (e.g., model accuracy). We implement a scheduling algorithm in HyperDrive that uses probabilistic model-based classification with dynamic scheduling and early termination to improve model performance and decrease workload completion times."
          ],
          "keyword": [
            "Distributed systems",
            "Cluster scheduling"
          ],
          "primary_title": "Application-Aware Cluster Resource Management",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919002/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919002/"
        },
        {
          "pid": "bdr:918912",
          "object_type": "pdf",
          "abstract": [
            "In the age of big data, uncertainty in data constantly grows with its volume, variety and velocity. Data is noisy, biased and error-prone. Compounding the problem of uncertain data is uncertainty in data analysis. A typical end-to-end data analysis pipeline involves cleaning and processing the data, summarizing different characteristics and running more complex machine learning algorithms to extract interesting patterns. The problem is that all the steps are error-prone and imperfect. From the input data to the output results, uncertainty propagates and compounds. This thesis addresses a subset of the uncertainty problems in data exploration. First, it is shown how uncertainty in the form of missing unknown data items can affect aggre- gate query results, which are very common in exploratory data analysis. It is challenging to make sure that we have collected all the important data items to derive correct data analysis, especially when we deal with real-world big data; there is always a chance that some items of unknown impacts are missing from the collected data set. To this end, sound techniques to derive aggregate queries with the open-world assumption (the data set may or may not be complete) are proposed. Next, uncertainty in the form of data errors is examined. It is almost guaranteed that any real- world data sets contain some forms of data error. This is an important source of uncertainty in data analysis because those errors would almost surely corrupt the final analysis results. Unfortunately, there has not been much work to measure the quality of data in terms of the number of remaining data errors in the data set. The data cleaning best practices basically employ a number of (orthog- onal) data cleaning techniques, algorithms or human/crowd workers in a hope that the increased cleaning efforts would result in a perfectly clean data set. To guide such cleaning efforts, techniques to estimate the number of undetected remaining errors in a data set are proposed. Lastly, an uncertainty problem related to machine learning (ML) model quality is addressed. ML is one of the most popular tools for learning and making predictions on data. For its use, ensuring good ML model quality leads to more accurate and reliable data analysis results. The most com- mon practice for model quality control is to consider various test performance metrics on separate validation data sets; however, the problem is that the overall performance metrics can fail to reflect the performance on smaller subsets of the data. At the same time, evaluating the model on all possible subsets of the data is prohibitively expensive, which is one of the key challenges in solving this uncertainty problem."
          ],
          "keyword": [
            "Machine Learning",
            "Data mining",
            "Big data",
            "Uncertainty Quantification",
            "Data cleaning"
          ],
          "primary_title": "Quantifying Uncertainty in Data Exploration",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918912/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918912/"
        },
        {
          "pid": "bdr:918891",
          "object_type": "pdf",
          "abstract": [
            "As the volume and complexity of generated data grow, users would like to maintain the ability to issue expressive queries on their data without sacrificing privacy. Encrypted databases are one of the most promising approaches towards this direction. However, this efficiency comes with the price of leaking information about the plaintext data. In this thesis, we use an algorithmic approach to develop rigorous attacks on encrypted databases and secure protocols. In the first part of this thesis, we consider secure protocols that allow two parties to jointly compute the similarity between their respective high-dimensional data without revealing the data to each other. Typical secure similarity approximation protocols first execute a sketching algorithm offline where the necessary randomness for the computation is “leaked” by design to all the participants. We show how a participant can generate an adversarial input so as to heavily mis-approximate the similarity and harm the correctness of the computation. In the second part of the thesis, we study a scenario where a client outsources to a server an encrypted database indexed by a one-dimensional attribute and issues a sequence of k-nearest neighbor queries. An adversary (e.g., the server or man-in-the-middle) observes which encrypted records are retrieved in response to each query and attempts to reconstruct the plaintext values of the indexed attribute. This so-called access pattern is “leaked” by design so as to achieve efficient performance. We prove that for ordered k-NN responses, the adversary can approximate plaintext attribute values with considerable accuracy. For unordered k-NN responses, we characterize the set of all valid reconstructions and present an attack that reconstructs the plaintext attribute values with tight approximation guarantees. State-of-the-art attacks on access pattern leakage operate under the assumption that the queries are generated uniformly at random. The last part of this thesis shows how to overcome this unrealistic assumption by using tools from learning theory, statistics, and optimization. Namely, we present the first leakage-based attacks for both range and k-NN queries that make no assumptions about the data or the query distribution. We introduce a new set of distribution-agnostic techniques which, as we demonstrate, achieve accurate reconstruction under a broad class of query distributions."
          ],
          "keyword": [
            "Computer security"
          ],
          "primary_title": "Information Leakage in Encrypted Systems Through an Algorithmic Lens",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:918891/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:918891/"
        },
        {
          "pid": "bdr:919063",
          "object_type": "pdf",
          "abstract": [
            "In the past, streaming data management systems (SDMSs) have eschewed transactional management of shared mutable state in favor of low-latency processing of high-velocity data. Streaming workloads involving ACID state management have required custom solutions involving a combination of database systems, complicating scalability by adding blocking communication, which severely impedes performance. This dissertation presents a solution: a transactional streaming system, S-Store. Built on a main-memory OLTP system, S-Store implements a novel transaction model and gives best-in-class performance on hybrid streaming and OLTP workloads while maintaining the correctness guarantees of both worlds. To achieve optimal performance, S-Store distributes processing across a cluster of machines. This requires an implementation which solves the unique challenges to system design and partitioning that arise from dataflow state management. This dissertation describes heuristics for optimizing two overarching workload categories: those that use data parallelism and those that use pipeline parallelism. Transactional streaming has opened opportunities for new solutions to real-world problems, particularly in streaming data ingestion. Together, these contributions provide a foundation for a new class of streaming research in which ACID state management is an essential feature rather than an afterthought."
          ],
          "keyword": [
            "Databases",
            "Transaction systems (Computer systems)",
            "Database design--Management",
            "Computer science"
          ],
          "primary_title": "Transactional Streaming: Managing Push-Based Workloads with Shared Mutable State in a Distributed Setting",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919063/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919063/"
        },
        {
          "pid": "bdr:919075",
          "object_type": "pdf",
          "abstract": [
            "The widespread popularity of visual data exploration tools has empowered domain experts in a broad range of fields to make data-driven decisions. However, a key requirement of these tools is the ability to provide query results at “human speed,” even over large datasets. To meet these strict requirements, we propose a complete redesign of the interactive data exploration stack. Instead of simply replacing or extending existing systems, however, we present an Interactive Data Exploration Accelerator (IDEA) that connects to existing data management infrastructures in order to speed up query processing for visual data exploration tools. In this redesigned interactive data exploration stack, where an IDEA sits between a visual data exploration tool and a backend data source, several interesting opportunities emerge to improve interactivity for end users. First, we introduce a novel approximate query processing formulation that better models the conversational interaction paradigm promoted by visual data exploration tools. Since the time dimension is often a critical component of many datasets, we then present a detailed survey of existing backend data sources specifically designed for time-dependent data in the context of interactive data exploration. Finally, based on the results from our study, we propose a new approximate index structure for an interactive data exploration stack that leverages the trends that exist in the underlying data to improve interactivity while significantly reducing the storage footprint."
          ],
          "keyword": [
            "Databases",
            "Big data"
          ],
          "primary_title": "Accelerating Interactive Data Exploration",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919075/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919075/"
        },
        {
          "pid": "bdr:919191",
          "object_type": "pdf",
          "abstract": [
            "Obtaining accurate markerless 3D pose estimations of subjects is an important problem in neuroscience and other fields. Recent work has employed CNNs very successfully to make pose estimations in 2D using limited data. We explore several models that attempt to leverage the accuracy of 2D models while extrapolating them to 3D, operating on RGB images taken from several viewpoints. These include: methods to refine 2D predictions, a model that learns the appropriate confidence that should be placed each on each of a set of 2D predictions when merging them to 3D, a model that explicitly aligns relevant features between camera views, and a model that incorporates error from 3D predictions directly into its objective function. Our results show that of these techniques, the most accurate 3D predictions are produced by learning appropriate confidence levels to attribute to the 2D predictions on which they are based."
          ],
          "keyword": [
            "Neural networks (Computer science)",
            "Computer vision",
            "Three-dimensional imaging",
            "pose"
          ],
          "primary_title": "Markerless 3D Pose Estimation from RGB Data",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:919191/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:919191/"
        },
        {
          "pid": "bdr:j9ma8pcq",
          "object_type": "pdf",
          "abstract": [
            "Reinforcement learning (RL) is the study of the interaction between an environment and an artificial agent that learns to maximize reward through trial-and-error. Owing to its generality, RL is used to formulate numerous applications, including those with high-dimensional state or action spaces. RL agents that are equipped with function approximation can tackle high-dimensional problems, but they are also notoriously difficult to study. Thus, a fundamental question in RL is how to design algorithms that are compatible with function approximation, but are also guaranteed to converge, can explore effectively, and can perform long-horizon planning. In this thesis I study RL through the lens of smoothness formally defined using Lipschitz continuity. I present theoretical results showing an essential role for smoothness in stability and convergence of RL, effective model learning and planning, and in value-function approximation in continuous control. Through many examples and experiments, I also demonstrate how to adjust the smoothness of key RL ingredients to improve performance in presence of function approximation."
          ],
          "keyword": [
            "Reinforcement learning"
          ],
          "primary_title": "Smoothness in Reinforcement Learning with Large State and Action Spaces",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:j9ma8pcq/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:j9ma8pcq/"
        },
        {
          "pid": "bdr:a58prvyj",
          "object_type": "pdf",
          "abstract": [
            "In recent years, the rising popularity and availability of Virtual Reality (VR) systems has enabled the use of immersive display systems for scientific data exploration. This opened up a large design space for novel visualization techniques that needs to be thoroughly investigated in order to build practical software. This dissertation investigates aspects of creating effective VR visualizations from multiple angles: First, by analyzing how display fidelity affects user performance immersive data exploration situations. Here, results from a large scale study evaluating the factors of display resolution, field of view and display artifacts show their varying impact on user effectiveness depending on the exploration tasks presented. A second line of studies analyzed the limits of human perception within high-resolution VR environments by evaluating the ability of users to identify minute details in immersive medical scan visualizations and their ability to read text within 3D environments containing visual obstructions. In an effort to obtain generalizable results, the experiments above were performed across multiple VR hardware systems, leading to recommendations about their effective utilization. Finally, this dissertation provides insights into best practices for VR application development through a large-scale case study of the \"Scientific Sketching\" design methodology. A managed collaboration of biologists, art students and computer science students over the course of three years resulted in a novel fluid-dynamics visualization application tailored to the specific needs of our biology collaborators. The information gathered alongside the development process built a strong basis for future VR application development. Together, the contributions of this work form a set of guidelines for the effective use of immersive visualizations in scientific settings."
          ],
          "keyword": [
            "Visual perception",
            "human-computer interaction",
            "Virtual reality",
            "Scientific Visualization"
          ],
          "primary_title": "Using Virtual Reality Effectively in Scientific Data Exploration - Perception, Usability and Design in Immersive Displays",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:a58prvyj/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:a58prvyj/"
        },
        {
          "pid": "bdr:kgyft3b4",
          "object_type": "pdf",
          "abstract": [
            "Educational assessments are crucial for both instructors and education researchers to measure learning, troubleshoot student problems, evaluate pedagogy, and improve education. Unfortunately, creating and administering reliable assessments is a labor-intensive process. This dissertation frames assessment creation from a pool of assessment items as a machine learning problem and tackles this problem by learning directed graphical models of topic prerequisite relationships. It is shown on a variety of real datasets that these models can be learned in a computationally- and data- efficient manner from records of student responses, can be queried efficiently, and produce accurate predictions about student knowledge. This technique is used to develop and administer novel computer science assessments of instructor-defined specificity using student-authored questions."
          ],
          "keyword": [
            "Machine Learning",
            "Algorithms",
            "Education",
            "Educational tests and measurements"
          ],
          "primary_title": "Query Strategies for Directed Graphical Models and their Application to Adaptive Testing",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:kgyft3b4/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:kgyft3b4/"
        },
        {
          "pid": "bdr:f6ca38j8",
          "object_type": "pdf",
          "abstract": [
            "I developed two public data systems deployed longitudinally in the wild: Drafty and Sketchy. Drafty is a tabular dataset of Computer Science professors that has evolved annually for eight years. During four-minute tasks, Sketchy's users simultaneously contribute sketches and view their peers’ contributions to maintain inspirational stimuli. This dissertation shows that unpaid everyday visitors make higher quality contributions when they have more domain-specific knowledge, especially compared to paid crowdworkers. Drafty automatically extracts user interest from their passive interactions to prompt and elicit accurate contributions. However, more is needed to increase the likelihood that someone contributes. Sketchy shows that providing users autonomy over their interactions to access randomized examples from its evolving dataset motivates higher-quality contributions. Integrating these lessons, Drafty is re-developed to give users similar freedom and functionality. Drafty generates dynamically generated insights (Databaits) from its evolving data. A naturalistic study shows that, unlike users in traditional recommendation systems, Drafty's users prefer randomized insights. Like Sketchy, when Drafty's users interact with these insights from its source data, they make more accurate contributions. What motivates paid and unpaid users to contribute to public data systems? This dissertation develops a theoretical framework that explores the trade-offs people make when selecting different crowd contribution tasks. While money is the strongest motivator for paid crowdworkers, unpaid everyday users are intrinsically motivated by their perception of tasks (i.e., their interest and if their contribution might help others). All users prefer shorter, easier tasks where they contribute their specialized knowledge. Evidence shows that unpaid everyday users make trade-offs, preferring longer tasks that pay less if those tasks are interesting and their contributions can help others. Highly accurate users make these same trade-offs. Together these public data systems demonstrate longitudinal evidence of engaging anonymous users in the wild to view and maintain quality evolving data."
          ],
          "keyword": [
            "human-computer interaction",
            "Crowdsourcing",
            "peer production"
          ],
          "primary_title": "Public Data Systems for Evolving Information",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:f6ca38j8/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:f6ca38j8/"
        },
        {
          "pid": "bdr:k824bkjq",
          "object_type": "pdf",
          "abstract": [
            "De Bruijn graphs are a popular data structure that have a wide variety of uses. In this thesis I will be going over a large number of properties, theorems, and algorithms relating to de Bruijn graphs from articles and papers I have read. I will also be adding some additional knowledge to spaces that I found had room to grow. This information is split up into two chapters “combinatorics” and “computational biology” which are the two most common uses for de Bruijn graphs. For all sections, if you would like to learn more, I highly encourage reading the original paper or article cited in that section. Furthermore this thesis assumes the reader has some background knowledge in de Bruijn graphs and how they work."
          ],
          "keyword": [
            "De Bruijn graphs",
            "combinatorics",
            "Computational Biology"
          ],
          "primary_title": "A Survey of De Bruijn Graph Properties, Theorems, and Algorithms",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:k824bkjq/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:k824bkjq/"
        },
        {
          "pid": "bdr:psxu8nxy",
          "object_type": "pdf",
          "abstract": [
            "The adoption of cloud services is on the rise, with both personal and corporate data being stored on third-party server databases. In order to protect such data in the cloud, (i) databases should be stored in encrypted form, and (ii) queries should be executed without prior decryption. To achieve this objective, practical searchable encryption schemes are being deployed in real-world applications. These schemes are designed to balance security and performance by providing efficient algorithms which, unfortunately, leak some information about the data. We consider range queries on encrypted multidimensional data and explore the feasibility of reconstructing the plaintext data by exploiting the information leakage from such queries. We analyze common types of leakage, including (i) access pattern, i.e., individually encrypted records in query responses, (ii) volume pattern, i.e., encrypted entire query responses, and (iii) search pattern, i.e., identifying unique queries. Our analysis includes developing concrete attacks that reconstruct the plaintext values of the encrypted records by exploiting these common types of leakage and identifying the information-theoretic limitations of an adversary. Furthermore, we develop a framework for searchable encryption schemes that supports multidimensional range queries and present six concrete schemes that offer a variety of trade-offs between security and privacy. We assess both theoretically and experimentally the vulnerability of these schemes to reconstruction attacks that exploit their scheme-specific leakage. By furthering the understanding of the security limitations of encrypted cloud data, our work enables developers to make more informed choices when deploying searchable encryption solutions."
          ],
          "keyword": [
            "Computer security",
            "Cryptography",
            "Computer science"
          ],
          "primary_title": "Exploring Searchable Encryption Leakage from Range Queries",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:psxu8nxy/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:psxu8nxy/"
        },
        {
          "pid": "bdr:enataxuv",
          "object_type": "pdf",
          "abstract": [
            "Combinatorial optimization problems are fundamental in many real-world applications, where the goal is to find the optimal or near-optimal solution to a problem subject to constraints. However, most real-world optimization problems are computationally intractable, especially ones with discrete domains for the objective function. Traditional methods that solve optimization problems typically rely on handcrafted heuristics for computationally-intensive decisions during the solving process and overlook patterns in the input data for problems that are being solved repeatedly. This leads to sub-optimal solutions and longer runtimes. To improve the solving process and the solution optimality, this dissertation introduces machine learning methods within the solving procedure of existing algorithms. The premise is that machine learning methods can offer an efficient and effective way to trim the search space of candidate solutions by exploiting the patterns and structures in the problem instance data, hence reducing the time required to find a (sub)optimal solution. This thesis makes the following contributions toward the advancement of machine learning methods for combinatorial optimization. First, we address the inefficiencies inherent in making local optimal choices within the solver's loop of a combinatorial algorithm. We propose a novel model-based reinforcement learning (RL) methodology, designed to guide the solver towards decisions in local optimizations that maximize future rewards. We showcase our method on the problem of Boolean logic synthesis of hardware chip designs, where the quality of results (QoR) depends on the sequence of optimizations applied. Our work improves on expert-crafted and heuristics-based approaches, enhancing QoRs by an average of 13%. Second, we delve into the difficulties surrounding hyper-parameter tuning within combinatorial solvers. While current practices adopt a few well-performing configuration parameters and use them for new unseen problems, machine learning can help in selecting instance-aware configuration parameters. Rather than training a model to directly predict configuration parameters, we propose a new method based on deep metric learning, which enables the selection of hyperparameters by identifying similarities in problems being repeatedly solved. Empirical results on Mixed Integer Linear Programming (MILP) problems show that our method is capable of predicting a solver's hyperparameters that improve solutions' costs by up to 38%, and is practical to deploy on production environments. Third, we investigate the exploration-exploitation dilemma in existing optimization algorithms. For large problem instances that necessitate prolonged solving times, we present a model capable of predicting a solver's outcome on scalable compute resources. The goal is to let solvers explore more of the solution space, while maintaining the costs of the provisioned compute resources. In scaling electronic design automation (EDA) workloads to cloud environments, our method is able to predict the most promising exploration runs of an optimization algorithm, while reducing the total compute costs by 35%. Finally, we advocate for the development of next-generation combinatorial optimization algorithms that are native to hardware accelerators such as GPUs. Instead of merely parallelizing existing algorithms' implementations, we propose a new approach grounded in contemporary deep learning frameworks that solves the Maximum Satisfiability (MaxSAT) problem through a single differentiable function. We demonstrate that it is feasible to develop GPU-native methods for combinatorial optimization, despite their discrete domains. This could have a significant impact on developing new approaches for tackling large-scale optimizations."
          ],
          "keyword": [
            "Machine Learning",
            "Deep learning (Machine learning)",
            "Combinatorial optimization"
          ],
          "primary_title": "Machine Learning Methods for Combinatorial Optimization",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:enataxuv/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:enataxuv/"
        },
        {
          "pid": "bdr:65jdyake",
          "object_type": "pdf",
          "abstract": [
            "Neural networks continually surprise us. Many of these surprises are cases of impressive generalization. Others, however, are embarrassing failures. For example, large language models can break down word problems into a series of logical steps, and arrive at reasonable answers without being explicitly trained to do so. Yet, at the same time, powerful visual models can be misled simply by adding a sticky note to the object in the image. Both of these examples are emblematic of neural network behavior. This leads to the question: Can we understand the conceptual structure of neural networks--do neural networks learn something approximating the traditional view of AI as a symbolic system or do they learn something radically new? This thesis addresses this question in two parts, testing neural networks across a variety of domains including vision, language, and board games. We use top-down methods, which are characterized by their capacity to pose and test high-level hypotheses about the structure and behavior of neural networks, for instance, falsifying (to a degree) whether a specific network represents a specific concept. In the first part of this thesis, in both visual and board game domains, we show that neural networks learn recognizable concepts that humans consider to be important to the task at hand. Our results highlight that the concepts are to some degree first-class objects in these neural networks. Next, in the second part, we look at when models use such concepts. Our results suggest that while pre-training of neural networks encodes many concepts, the model may not use them. Moreover, these concepts may not be accessible across contexts: In text-to-image models, we find that a number of training-data priors modulate the relations a model can successfully generate. Using top-down methods, we set and test hypotheses that help elucidate when neural networks use concepts in their decision-making processes. This thesis contributes to a field-wide shift in how we think about testing neural networks: We care not only about what answers models give but also why models give those answers."
          ],
          "keyword": [
            "Neural networks (Computer science)",
            "Computer science",
            "Concepts",
            "Interpretability",
            "Model Understanding"
          ],
          "primary_title": "Understanding the Conceptual Structure of Neural Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:65jdyake/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:65jdyake/"
        },
        {
          "pid": "bdr:mvwuprkx",
          "object_type": "pdf",
          "abstract": [
            "Humans are able to solve complex problems by distilling their knowledge of the world into simplified task-relevant representations and creating plans to achieve their goals. In addition, central to effective human-human collaboration is the ability to teach these concise models of the world to situated partners with ease. Motivated by these properties, this thesis develops methods that enable mobile manipulators to learn action and state abstractions for task planning, and to effectively communicate and learn relevant abstractions from humans via Mixed Reality (MR) communication channels. First, we focus on autonomously learning action abstractions. We describe a novel policy class for efficiently learning sustained-contact manipulation skills, and a method for bootstrapping learning of dynamic motor skills with motion planning. Next, we focus on autonomously learning state abstractions. We describe research on learning symbolic representations for navigation to support task planning on a mobile manipulator platform. Lastly, we describe research on learning action and state abstractions from end-users via MR. Our MR system enables humans to easily teach robots how to manipulate objects as well as label scene information to support planning. Collectively, these works lay the groundwork for enabling mobile manipulators to solve tasks in complex environments by learning state and action abstractions from interacting with the world or human teachers."
          ],
          "keyword": [
            "human-robot interaction",
            "Artificial intelligence",
            "Robotics",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Abstraction for Autonomous Human-Robot Interaction",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:mvwuprkx/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:mvwuprkx/"
        },
        {
          "pid": "bdr:v9u7xbyx",
          "object_type": "pdf",
          "abstract": [
            "The current dominant way to use neural networks for language applications is to pretrain a language model (LM): Given a large corpus, randomly mask out some percentage of words, and train a model to predict the masked out words. Empirically, pretraining can yield a single language model capable of performing numerous downstream tasks. To specify which task the model is supposed to perform, a user can now simply describe the desired task as a natural language instruction (a.k.a. a prompt) and ask the model to predict the answer (e.g., “Check if the following document has any grammatical errors.”) However, pretrained language models' ability to follow instructions is both impressive in the best case and comically brittle in the average case. This thesis introduces instruction tuning, a method which dramatically improves instruction following performance and robustness, as well as enabling zero-shot generalization to following instructions of novel tasks. Further, this thesis extensively studies and discusses a main limitation of instruction tuning: it can make a model too robust and insensitive to the semantics of the instruction in unexpected ways. Finally, this thesis furnishes a human study which highlights the closing gap between the instruction following behaviors of language models and that of humans, cautioning future research to not naively compare or interpret language model behaviors relative to assumptions about human behaviors."
          ],
          "keyword": [
            "Large Language Models"
          ],
          "primary_title": "Tuning Language Models to Follow Instructions",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:v9u7xbyx/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:v9u7xbyx/"
        },
        {
          "pid": "bdr:dnrdhmmd",
          "object_type": "pdf",
          "abstract": [
            "Generally intelligent agents must learn and plan in complex environments, with sensors and actuators that support various behaviors and tasks. This complexity hinders decision making, necessitating methods that abstract away such intricacies. Such abstractions must be rich enough to be precise, while simple enough to enable efficient learning and planning. Fortunately, many environments contain inherent structure, like Markovian or factored dynamics, that agents can leverage to help manage this tradeoff. By discovering and exploiting the structure in their environment, agents can automatically build decision-making abstractions that are beneficial for learning and planning. This dissertation showcases a collection of methods for abstracting observations, actions, histories, and world models in order to enhance agents' problem-solving capabilities while striking a useful balance between richness and simplicity."
          ],
          "keyword": [
            "Decision making",
            "Reinforcement learning",
            "Artificial intelligence",
            "Abstraction"
          ],
          "primary_title": "Structured Abstractions for General-Purpose Decision Making",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:dnrdhmmd/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:dnrdhmmd/"
        },
        {
          "pid": "bdr:tff65psv",
          "object_type": "pdf",
          "abstract": [
            "Within the last decade, databases have undergone a major shift in designs largely due to two important hardware trends. While the increases in main-memory capacities made it possible to hold even large systems in the RAM space, the advent of multi-core processors created opportunities for a multitude of parallel query processing techniques. As a result, we have seen the advent of massively parallel, main-memory data management system designs that leverage these new hardware platforms. Currently, originating from physical limitations known as the dark silicon effect, we witness the evolution of modern processors towards heterogeneous designs, where the vendors replace some general purpose cores with high-performance, energy-efficient specialized compute-units. Motivated by these trends, we propose novel query processing techniques that target these modern processing environments for data analytics workloads. We first focus on result-reuse techniques for main-memory DBMSs. While existing reuse approaches require heavy-weight materialization operations, our novel reuse techniques cache internal data structures, in particular hash tables, created by query operators and make them directly reusable for downstream processing with little or no additional overhead. We implement a prototype called HashStash to confirm the feasibility of our approach, and demonstrate significant performance gains for typical analytical workloads. Then, we study novel query processing techniques for main-memory DBMSs operating on top of emerging heterogeneous compute platforms. These platforms host compute units with varying performance, functionality and execution characteristics, thus create new challenges for efficient query processing solutions. To target these systems, we propose SiliconDB, a new query processing approach that uses a fine-grained, adaptive workload execution model. SiliconDB splits queries into small chunks of work units, and uses queuing theory to dynamically assign these work elements to available compute units to maximize overall resource utilization. As the final component of this dissertation, we extend SiliconDB's relational query engine to directly operate on raw input columns without stalling on an initial data loading pipeline. We implement our approach using an FPGA-based heterogeneous multi-core platform and show how we leverage the salient properties of this environment to speed up processing."
          ],
          "keyword": [
            "Databases",
            "Query Processing",
            "Main Memory"
          ],
          "primary_title": "Query Processing for Data Analytics on Modern Multicore Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:tff65psv/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:tff65psv/"
        },
        {
          "pid": "bdr:dbwmmjjd",
          "object_type": "pdf",
          "abstract": [
            "Designing Artificial Intelligence (AI) is still reserved for experts, and the existing design paradigm follows a data-driven approach: domain experts start with a hypothetical model, verify the model on a task-specific dataset to acquire performance metrics, then revise the model based on prior experiences, hoping to improve the model in the next loop. This thesis seeks to build an intelligent agent to substitute domain experts in this design process. I start with formalizing the current design process as a computational model, upon which I further investigate issues to the algorithmic efficiency and system utilization to build an agent that algorithm and system can synergistically work together. Specifically, I propose a new black box solver, Latent Action Monte Carlo Tree Search (LA-MCTS), to address the sample efficiency and build a deep learning framework to expand the design space far beyond the available GPU DRAM. These results collectively provide a partial path toward AI democratization by creating a practical MCTS-based AI agent that efficiently designs complex AI without experts in a reasonable amount of time."
          ],
          "keyword": [
            "Artificial intelligence"
          ],
          "primary_title": "Building an Intelligent Agent to Design Neural Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:dbwmmjjd/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:dbwmmjjd/"
        },
        {
          "pid": "bdr:xextjmwj",
          "object_type": "pdf",
          "abstract": [
            "Financial insecurity characterizes modern life. It reflects ongoing instability in employment and earnings which are notably pronounced in the gig economy. This instability is compounded by the widespread use of automated decision-making tools that directly affect employment and income. Over time, this “precariousness” unfolds as a sequence of events for individuals. Thus, to understand and address it, a shift in perspective from decision-makers to individuals is necessary. This requires that we develop “artificial societies” - computational simulations of an agent-based behavioral model capable of capturing various related phenomena simultaneously, including individual consumption responses to financial shocks, the influence of predictive tools on income, and the long-term behavior of individuals striving to maximize utility. This individual-level perspective is one direction to study precarity and inequity in artificial societies with computational simulations or models designed to replicate and investigate the intricate behavior of complex social systems. However, there is also a societal-level viewpoint wherein neither a singular decision-maker nor defined agent behavior rules exist. Consequently, there is a need for a model that does not attempt to describe underlying systems or capture individual actions. Adopting a system-based approach to studying inequity in feedback loops opens avenues to explore social systems that are otherwise challenging to model directly. This dissertation first introduces the concept of latent financial instability, or precarity, to the artificial intelligence community. It develops agent-based computational models embodying realistic human-like behaviors to explore precarity dynamics, drawing from various strands of inquiry in economics. Additionally, we investigate work schedule instability and the impact of foresight on financial security. Next, we present a model from linear systems theory to quantify feedback in social systems holistically, enabling the examination of long-term policy effects even without individually characterized feedback mechanisms. Our frameworks facilitate the examination of precarity dynamics, the development of mitigation strategies for precarity, policy investigations, and the production and sustainment of long-term equity."
          ],
          "keyword": [
            "Artificial intelligence",
            "Fair Machine Learning",
            "Algorithmic Fairness",
            "Social Computing",
            "Artificial Societies"
          ],
          "primary_title": "Modeling and Simulation of Artificial Societies to Study Precarity and Inequity",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:xextjmwj/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:xextjmwj/"
        },
        {
          "pid": "bdr:gerzguhq",
          "object_type": "pdf",
          "abstract": [
            "Abstract of Advancements in Portfolio Methods for Optimal Multi-Agent Pathfinding, by Eric Ewing, Ph.D., Brown University, October 2024 Multi-Agent Pathfinding (MAPF) is a critical problem in artificial intelligence and robotics, with applications in domains such as warehouse logistics and autonomous vehicle coor- dination. However, solving MAPF optimally is NP-hard under various cost objectives. While many optimal algorithms have been developed, none dominates across all problem instances. This thesis proposes novel techniques to leverage algorithm portfolios for optimal MAPF, advancing understanding of MAPF and the state of the art in optimal MAPF algorithms. This dissertation makes three main contributions to the field. First, we present a unifying framework for conflict-based algorithms in MAPF, formalizing how many algorithms search through conflict-space to find collision-free paths. Second, we present our work on the relationship between betweenness centrality and providing practical insights for the design of MAPF environments and algorithms. We show theoretically and empirically how high betweenness centrality values in MAPF environments lead to more conflicts between agents and difficulties for MAPF algorithms. Finally, we present our novel algorithm Partial-Dependency-based Instance Decomposition (PDID). PDID attempts to decompose MAPF instances into smaller independent sub-instances which can be solved efficiently with existing MAPF algorithms. PDID is able to leverage the strengths of a portfolio of existing MAPF algorithms to outperform any single existing algorithm on certain types of instances."
          ],
          "keyword": [
            "Multi-Agent Pathfinding"
          ],
          "primary_title": "Advancements in Portfolio Methods for Optimal Multi-Agent Pathfinding",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:gerzguhq/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:gerzguhq/"
        },
        {
          "pid": "bdr:c9394zer",
          "object_type": "pdf",
          "abstract": [
            "Strong separation between the OS kernel and user applications has been a standard design approach for several decades. However, the security benefit of this model comes at the cost of additional context switching and data copying. The BPF subsystem addresses this problem by allowing a user to safely specify and delegate computations in the OS kernel. Enabling the kernel to carry out computations on a user’s behalf can significantly reduce the cost of separation, without necessarily hindering the safety of the kernel. BPF is used in a wide variety of different contexts, including software-based networking, performance profiling, scheduling, and application-level hardening. Yet, the versatility of BPF is not without a cost. In this dissertation, we show that (1) BPF can weaken kernel security, and (2) kernel security can be improved by applying software hardening techniques to BPF. We investigated two aspects of the problem. The first aspect is impact of BPF on kernel security against the exploitation of memory-safety vulnerabilities. For this, we present EPF: an exploitation technique using BPF as a vehicle for malicious payloads, circumventing popular ret2usr defenses; we also developed a series of defenses that strengthen the BPF interpreter, preventing EPF-style attacks. The second aspect is the impact of BPF on kernel security against transient execution attacks. For that, we present BeeBox: a sandboxing technique that prevents CPU speculation (in BPF programs) from disclosing sensitive data. Compared to existing mitigations, BeeBox provides more comprehensive protection with significantly lower performance overhead."
          ],
          "keyword": [
            "Computer security",
            "Computer software--Security measures"
          ],
          "primary_title": "Hardening In-kernel Execution Environments against Memory-safety and Transient- execution Vulnerabilities",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:c9394zer/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:c9394zer/"
        },
        {
          "pid": "bdr:tyrqr7f5",
          "object_type": "pdf",
          "abstract": [
            "Machine learning (ML) technologies have experienced unprecedented advancements, catalyzing theoretical breakthroughs and enabling wide-ranging applications across various domains. However, as these systems become more complex and are expected to perform a wider range of tasks, the challenges associated with efficient training, fine-tuning, and deployment become increasingly prominent. One key strategy to address these challenges is parameter efficiency, which involves minimizing the number of parameters that need adjustment during training or inference, thus reducing computational resources while maintaining or improving performance. This dissertation examines various approaches to enhance parameter efficiency in ML models through exploring novel architectures and unconventional computing paradigms. We present three main contributions aimed at enhancing parameter efficiency in ML models while maintaining or improving performance. The first contribution introduces MTLoRA, a fine-tuning framework that leverages low-rank adaptation matrices for parameter-efficient training of MTL models. By effectively disentangling the parameter space for diverse tasks, MTLoRA minimizes the number of parameters that need to be adjusted, training only a small fraction of the model while maintaining high adaptability and performance. Building on the principles of parameter-efficient architectures, the second contribution, CRLoMTL, addresses the challenges of multi-objective optimization in MTL related to sharing parameters between different tasks. CRLoMTL employs parameter-efficient methods to tackle the issue of conflicting gradients across the shared backbone. By utilizing low-rank matrix adaptations to handle gradient conflicts, CRLoMTL integrates a multi-pass training strategy to effectively capture task interactions and systematically minimize inter-task conflicts in the shared parameters, leading to improved convergence and overall model performance. We further extend these ideas by improving the parameter-efficiency of MTL models through an adaptive framework for input-dependent dynamic sparsification in multi-task learning. Our framework employs a lightweight policy network to recognize unnecessary computations based on input complexity, further optimizing model adaptability and efficiency. Moving beyond traditional silicon-based systems, the third contribution explores unconventional platforms to build parameter-efficient systems. Unconventional computing, such as chemical computing, offers unique computational paradigms with potential inherent parameter efficiency through analog encoding, parallelism, and bio-compatibility. We present a molecular computation system that uses acid-base reactions to perform calculations, efficiently encoding information in analog chemical form. By leveraging the natural complementarity of acids and bases, our approach demonstrates the feasibility of constructing neural network classifiers in a chemical medium. Our approach efficiently encodes information in analog chemical form, relying on the natural complementarity of acids and bases, demonstrating the feasibility of constructing neural network classifiers in a chemical medium. The model is distinguished by two key advantages: inherent parallelism and signal encoding efficiency. The model leverages simultaneous chemical reactions to model the parallel processing of the neural network computation. Furthermore, our model achieves parameter efficiency through analog encoding of signals as acid-base concentration levels, providing a more granular and expansive range of values for each computational unit compared to traditional binary encoding. Furthermore, we extend our chemical computation platform to leverage enzymatic reactions to model neural networks and digital gates through pH modulation, showcasing the potential for diversifying the computational strategies for building parameter-efficient architectures."
          ],
          "keyword": [
            "Artificial intelligence",
            "Deep learning (Machine learning)",
            "Efficient Training",
            "Multi-Task Learning",
            "Unconventional Computing",
            "Efficient Machine Learning"
          ],
          "primary_title": "Optimizing Machine Learning Models through Parameter-Efficiency",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:tyrqr7f5/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:tyrqr7f5/"
        },
        {
          "pid": "bdr:htwcqasj",
          "object_type": "pdf",
          "abstract": [
            "Prior work on planning in computing education has primarily focused on intuiting students' intended plans by ex post facto analysis of student programs. In this dissertation, I introduce an explicit planning step in the problem-solving process that students engage with before writing any code, then evaluate this planning process through a sequence of empirical studies. I demonstrate that novice programmers can produce correct, semi-structured plans at a higher level of abstraction than code. These plans can be used to enable useful and scalable feedback. This dissertation walks through the evolution of various designs we've used for the planning process so far, including the kinds of plans produced, feedback given, and measures of perceived student utility for each."
          ],
          "keyword": [
            "Computers--Study and teaching",
            "Computers and education"
          ],
          "primary_title": "Designing and Studying an Explicit Planning Process for Programming Novices",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:htwcqasj/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:htwcqasj/"
        },
        {
          "pid": "bdr:5qk7tf7c",
          "object_type": "pdf",
          "abstract": [
            "Abstract of Practical Privacy via New Systems and Abstractions, by Kinan Dak Albab, Ph.D., Brown University, May 2025. Data privacy has become a focal point of public discourse. In response, data protection and privacy regulations have been enacted throughout the world, including GDPR and CCPA, and companies make various promises to end users in their privacy policies. However, high-profile privacy violations remain commonplace, in part because complying with regulations and policies is challenging for applications and developers. This dissertation demonstrates how we can help application developers achieve privacy compliance by designing new privacy-conscious systems and abstractions. The dissertation discusses two systems. The first, K9db, is a privacy-compliant database that supports GDPR-style subject access requests by construction. The second, Sesame, is a system for end-to-end enforcement of privacy policies in web applications. Sesame provides practical guarantees by combining a new static analysis for data leakage with advances in memory-safe languages, lightweight sandboxing, and engineering practices around code review. This dissertation demonstrates that creating privacy abstractions at the system level simplifies compliance and guarantees privacy requirements by design."
          ],
          "keyword": [
            "Databases",
            "Privacy",
            "GDPR Compliance",
            "Computer Data Systems",
            "Information flow control"
          ],
          "primary_title": "Practical Privacy via New Systems and Abstractions",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:5qk7tf7c/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:5qk7tf7c/"
        },
        {
          "pid": "bdr:ph4ytr5w",
          "object_type": "pdf",
          "abstract": [
            "Processors rely on memory devices to store data that cannot does not fit in on-chip components, such as registers and caches. In doing so, an implicit assumption is made that data that is loaded on chip from memory is consistent with the state that was stored. This primitive is critical for security and privacy; user data and program state alike are stored in memory, so its corruption can lead to dangerous behaviors such as malicious execution or data leakage. Unfortunately, memory devices are subject to a variety of well known attacks that allow for this corruption. Given the potential vulnerability of memory devices, the study secure memory, which describes how processor can reason about the storage of data in vulnerable memory devices, is of particular interest. Doing so explicitly enforces the implicit assumptions by extending the memory controller logic to maintain metadata associated with the data stored in memory. This is a valuable primitive as it allows processors to reason about the privacy and integrity of stored data. With that said, such a protocol comes with several limitations to its practicality: (1) implementing secure memory comes at the cost of runtime performance; (2) security metadata requires a significant space to store; and (3) primitive definitions in secure memory limit its adaptability to emerging memory technologies. As a result, more recent versions of secure processors seldom implement a comprehensive secure memory protocol. In order to make secure memory a more palatable feature for commodity secure hardware, it is necessary to develop an in-depth understanding of the secure memory protocol. This dissertation first contributes a robust study of secure memory entailing a detailed qualitative and quantitative description of its mechanisms, the translation of this theoretical protocol to practice, and the state-of-the-art optimizations to the protocol. Doing so will classify the overheads of each of its components and highlight the impact of the various design decisions that lead to the current secure memory landscape. This dissertation will introduce four novel adaptations of the secure memory protocol: two of which extend its baseline approach and two of which target its adaptability to emerging technologies. It will first introduce Cordelia, a modification to the secure memory protocol that benefits runtime performance. This protocol is robust to the trend of increasingly memory-intensive applications. Furthermore, it proposes the Baobab Merkle Tree to alleviate the spatial overhead pressure imposed by secure memory metadata. Each of these protocols leverage insights concerning runtime behaviors of the application and metadata to optimize each of these ends. Afterwards, this dissertation explores the impact of two emerging memory technologies on the secure memory protocol. It will first propose A Midsummer Night’s Tree, which describes an adaptation of secure memory for non-volatile memories that explores the trade-off space imposed by these devices in-depth. Then it will propose CAPULET and describe how emerging disaggregated memories can be leveraged to improve the existing optimizations to secure memory. From each of these ends, this dissertation will formalize the problems that these emerging technologies impose on the practicality of secure memory."
          ],
          "keyword": [
            "Computer architecture",
            "Computer security",
            "Main Memory"
          ],
          "primary_title": "Towards a Practical Secure Memory for Modern Deployment",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ph4ytr5w/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ph4ytr5w/"
        },
        {
          "pid": "bdr:qekb9qv7",
          "object_type": "pdf",
          "abstract": [
            "In the wake of the Second World War, as they presided over major public expenditure reforms, European and American governments supported the development of rigorous mathematical models of economies to guide economic policy. Over the next two decades, general equilibrium models (or Walrasian economies) emerged as the dominant framework. However, as these models were often analytically intractable, as early as the 1960s, a group of researchers led by Herbert Scarf turned their attention to finding “a general method for the explicit numerical solution of [general equilibrium models].” While some methods had limited success in solving simple models, 50 years later, a general method for computing solutions to more complex models remains elusive. Nevertheless, these models---and their often inaccurate solution methods---continue to be widely used in applications such as resource allocation and public policy analysis, raising concerns about the impact of inaccurate solutions on the public good. This thesis addresses this issue by leveraging tools from computer science and game theory to analyze algorithms for general equilibrium models. The first part of this thesis focuses on variational inequalities (VIs), a mathematical modeling paradigm, and their application to Walrasian economies (i.e., models driven by demand and supply). The second part of this thesis concerns pseudo-games, a multiagent optimization framework, and their application to Arrow-Debreu economies (i.e., Walrasian economies in which demand and supply are generated by consumers and firms). The final part of this thesis concerns Markov pseudo-games, a multiagent \\emph{stochastic} optimization framework, and their application to Radner economies (i.e., a generalization of Walrasian and Arrow-Debreu economies that explicitly model time and uncertainty). While Parts 1 and 2 of this thesis resolve Scarf's challenge by solving general equilibrium models developed during his lifetime, Part 3 merely scratches the surface of a new research direction. Above all, it raises an exciting and contemporary analog to Scarf’s challenge at the intersection of deep learning, reinforcement learning, and mathematical economics---namely, finding a general method for the explicit numerical solution of Radner economies."
          ],
          "keyword": [
            "Artificial intelligence",
            "Deep Learning",
            "Mathematical optimization",
            "Computer Science and Game Theory",
            "Game theory",
            "Equilibrium (Economics)",
            "Variational inequalities (Mathematics)",
            "Computational complexity"
          ],
          "primary_title": "An Algorithmic Theory of General Equilibrium",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:qekb9qv7/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:qekb9qv7/"
        },
        {
          "pid": "bdr:5mc74mtt",
          "object_type": "pdf",
          "abstract": [
            "With rising rates of mental health struggles among undergraduate students, social support---specifically perceived social support---has become vital in maintaining emotional health. Perceived social support has long been directly tied to metrics of mental, emotional, and physical well-being, including reduced rates of depression, anxiety, and loneliness. Similarly, emotional self-disclosure has shown strong correlations with perceived social support. Online platforms have become a primary form of communication among undergraduates, and thus a central tool towards building social support. An understanding of the role of these platforms in encouraging self-disclosure and social support is a crucial step towards supporting student mental health. This dissertation aims to 1) understand the causal factors that contribute to increased self-disclosure and perceived social support in online spaces and 2) investigate the features of private online spaces that facilitate more open self-disclosure. This dissertation includes three studies investigating how users self-disclose in online spaces, as well as how that disclosure relates to perceived social support. An exploratory study on online communication habits of undergraduate students before and during the COVID-19 pandemic finds a strong correlation between comfort with self-disclosure in private messaging and perceived social support. Social support unexpectedly showed a stronger correlation with willingness to self-disclose over private messaging than over phone or video calls. A separate study using the data extraction platform Sochiatrist investigates the accuracy of emotional disclosure over private and public online communication platforms. Compared to previous work, this study shows private online messages to more accurately represent emotional state than posts in public online spaces. Following these two studies, Chirp was developed as a social app centering privacy, perceived control over information spread, and ability to reflect on emotional self-disclosures over time. User feedback of Chirp gives evidence that when we leverage these aspects of private communication, users can form meaningful relationships which can lead to social support, even in semi-public, anonymous spaces with extremely limited communication. Through these studies, this dissertation investigates how attributes resulting from these aspects that define private online communication may lead to more social support and connection-building behavior. Specifically, it focuses on the attributes of reflection on previous conversations, providing users a feeling of control, and user-driven incentive to continue communication. This dissertation shows that integrating these attributes into a public online communication space can lead to connection and support between users."
          ],
          "keyword": [
            "Social networks",
            "Social media",
            "College students--Mental health"
          ],
          "primary_title": "Building Online Social Support Systems Through Investigation of Undergraduate Private Messaging Spaces",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:5mc74mtt/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:5mc74mtt/"
        },
        {
          "pid": "bdr:rjekj9x7",
          "object_type": "pdf",
          "abstract": [
            "The ability to understand 3D shapes is crucial across many application domains, such as robot vision, video games, animation, VR/AR, and 3D product design. In recent years, supervised methods have achieved remarkable results in 3D shape analysis and understanding. However, teaching machines to understand 3D shapes with very limited supervision is still a challenging problem. In this thesis, I present several approaches for analyzing 3D shape structure and kinematics without supervision. I first introduce a method to represent 3D shapes as sequences of modeling operations without supervision, I then propose a method to decompose 3D shapes conditioned on a library of 3D parts without supervision, finally I propose a method to detect kinematic motions of 3D shapes without supervision. In the experiments I demonstrate my proposed methods achieve good effectiveness and outperform some baseline methods."
          ],
          "keyword": [
            "Computer graphics",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Unsupervised 3D Shape Structure and Kinematics Analysis",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:rjekj9x7/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:rjekj9x7/"
        },
        {
          "pid": "bdr:2mtjsw56",
          "object_type": "pdf",
          "abstract": [
            "General-purpose agents must sense the world using high-dimensional sensors such as cameras and affect the world using low-level actions such as motor torques. Such a rich input and output space allows the same agent to solve many tasks, but complicates the efficient search for solutions. To solve any specific problem efficiently, the agent must form abstractions---only retain the information necessary to make good decisions in the current task. Sequential decision-making problems permit two types of abstraction: state and action abstractions; this thesis studies how artificial agents may autonomously acquire both types of abstraction via interaction with an environment. Hierarchical reinforcement learning (HRL) is our starting point for action abstraction because it augments the agent’s action set with temporally extended actions called skills. When a human programmer carefully designs temporally extended skills for an AI agent, it can rapidly solve many challenging tasks. But, how can an agent automatically discover skills that are useful for obtaining mastery over the environment? We present algorithms that discover skills that are temporally composable, meaning that they can be reliably executed one after the other to solve long-horizon problems. Action abstraction, although necessary, is not sufficient for effective decision making; it must be accompanied by state abstractions. We provide algorithms that can learn state abstractions that complement the skills discovered by the agent so that it may create a whole new decision problem that is easier to solve than the original one it was faced with. Solutions in the abstract decision problem correspond to good solutions in the original problem, providing a way for artificial agents to rapidly make high-quality decisions. These algorithms bridge the gap between classical AI techniques that specialize in symbolic search, and deep reinforcement learning techniques that effectively deal with high-dimensional and continuous sensorimotor spaces. Finally, we tackle the problem of exploration in vast environments: when the agent interacts with a large environment, what data should it collect so that it can maximize its learning progress? Here, we present algorithms for discovering those goals that are controllable, reachable, novel, and task-relevant as a way of guiding the curiosity of the agent. Taken together, this thesis presents algorithmic ingredients for building agents that explore with the intention of building plannable models of the world that are abstract in both state and time."
          ],
          "keyword": [
            "Reinforcement learning",
            "Artificial intelligence"
          ],
          "primary_title": "Skill Discovery for Exploration and Planning",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:2mtjsw56/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:2mtjsw56/"
        },
        {
          "pid": "bdr:ecbdcfud",
          "object_type": "pdf",
          "abstract": [
            "Language models (LMs) have accelerated NLP research in the past years, but we are still far from deeply understanding how they work. In this thesis, I will provide evidence that, despite their reputation as black boxes, LMs often use very simple and intuitive algorithms to solve tasks. First, I will show that the same interpretable structure that emerges in static embedding models (that allows for vector arithmetic to solve relations) also appears in the hidden space of LMs. This structure supports interventions and allows us to control the output of LMs across contexts. Then, I will present a few examples of modularity of mechanisms in transformer LMs. At the circuit-level, LMs reuse simple structures in non-trivial ways across tasks. We can even find such structures without running these models, by analyzing their weights alone. In doing so, we uncover the signal that represents communication between components in a model (in this case between attention heads) is implemented through low-rank subspaces of the latent representation space, making it interpretable and simple to intervene on. Thus, at least part of the success of LMs seems to stem from the ability to combine simple components into more expressive ones. I will use these results to argue that we can interpret many of the complex behaviors of LMs as the coordination of simple learned mechanisms, and why we should center this perspective for future work in the field."
          ],
          "keyword": [
            "Natural language processing (Computer science)",
            "Artificial intelligence",
            "Interpretability",
            "Large Language Models",
            "mechanistic interpretability",
            "Natural language processing (Computer science) (NLP (Computer science))"
          ],
          "primary_title": "Simple Mechanisms Underlying Complex Behaviors in Transformer Language Models",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:ecbdcfud/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:ecbdcfud/"
        },
        {
          "pid": "bdr:scvupgrz",
          "object_type": "pdf",
          "abstract": [
            "Standard statistical learning models (such as PAC learning) assume an independent and identically distributed training set, and evaluate the performance of their learned models with respect to the same distribution as the training set. In many data analysis applications, ranging from customers' preferences to weather conditions, the assumption that samples are identically distributed is unrealistic. The underlying distribution gradually changes over time, and solving a learning problem with respect to the current distribution inevitably relies on past data from related but not identical distributions. In this work, we consider a more general setting, where we are given a sequence of independent observations from the last T steps of a drifting distribution, and we want to solve a learning problem with respect to the current distribution at time T. The key challenge is determining how much past data to use for learning, carefully balancing the increased variance from using fewer recent samples against the drift-induced bias from incorporating older, less relevant observations. The main contribution of this thesis is to introduce novel algorithms that adaptively solve the problem of learning with distribution drift based only on sample data and, unlike previous work, do not require prior knowledge about the magnitude of the drift. We demonstrate applications of our techniques and establish lower bounds for learning problems such as binary classification, discrete distribution estimation, and vector quantization in a drift setting."
          ],
          "keyword": [
            "Machine Learning",
            "Mean estimation",
            "Statistical Learning Theory",
            "Nonstationary"
          ],
          "primary_title": "Statistical Learning with Unknown Distribution Drift",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:scvupgrz/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:scvupgrz/"
        },
        {
          "pid": "bdr:a5uzmqzu",
          "object_type": "pdf",
          "abstract": [
            "Despite the widespread use of machine learning in consumer products and high-impact sectors, measuring the performance of these algorithms remains a fuzzy science. This is because it is often unclear how to define good performance rigorously. Years of research have produced a plethora of mathematical interpretations of what it means for an algorithm to “work well,” often in the form of metrics and definitions. This abundance of options creates challenges for practitioners, who are forced to navigate seemingly arbitrary tradeoffs between performance measures. Worse, these decisions can negatively impact individuals affected by the technology, further underscoring the need for more rigorous evaluation frameworks. In this dissertation, I will demonstrate how we can address these limitations in existing model evaluations. I leverage several topics in statistics to (1) reveal shortcomings in current evaluation methodologies, and (2) demonstrate new ways that evaluations can be improved to create better outcomes in machine learning systems, when viewed through a sociotechnical framing."
          ],
          "keyword": [
            "Measurement"
          ],
          "primary_title": "Sociotechnical Approaches for the Evaluation of Machine Learning Systems",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:a5uzmqzu/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:a5uzmqzu/"
        },
        {
          "pid": "bdr:tv6dag4u",
          "object_type": "pdf",
          "abstract": [
            "Zero-shot generalization—the ability of an intelligent system to generalize to new classes, tasks, and environments without human annotations—has become one of the most widely studied research areas in deep learning. This thesis introduces new learning methods and evaluation techniques for zero-shot generalization, capturing the dramatic shift in the field over the years from training models from scratch to adapting off-the-shelf, pretrained foundation models in language and vision. First, I study zero-shot generalization using structured knowledge. I propose to train zero-shot models from scratch using large common sense knowledge graphs. To address the challenges of learning with a large, noisy knowledge graph, I introduce a framework with a new graph convolutional network to model the graph and demonstrate improved performance compared to smaller knowledge graphs. Next, I study zero-shot generalization in pretrained vision-language models (VLMs) through compositionality, a natural framework for understanding generalization. I introduce a new parameter-efficient learning method that decomposes classes into sub-concepts and recomposes them at test time for improved downstream performance. Then, I test whether VLMs can model different types of compositions involving multiple objects and find that they often fail when these compositions require concept binding. Finally, I study zero-shot task adaptation of large language models (LLMs) with synthetic datasets. I create a new model that converts unannotated text from specialized domains into instruction tuning datasets to train LLMs. My work challenges the conventional wisdom of domain adaptation and shows that adapting LLMs with synthetic instruction tuning data is significantly better than self-supervision. Overall, this thesis provides a systematic approach to studying zero-shot generalization and offers a roadmap for building flexible, intelligent systems in the future."
          ],
          "keyword": [
            "Natural language processing (Computer science)",
            "Computer Vision",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Zero-Shot Generalization with Structured Knowledge, Composition, and Synthetic Datasets",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:tv6dag4u/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:tv6dag4u/"
        },
        {
          "pid": "bdr:x6hwffhs",
          "object_type": "pdf",
          "abstract": [
            "Recent advances in digital authoring techniques and tools have made it easier for artists to work fluidly within digital environments. However, the exact relationship between an artist’s creative intent and the digital operations they follow to express it remains nuanced. As more technologies replace tedious steps in the authoring workflow, questions remain on how efficiency can be balanced with artist autonomy and intent. To ensure smooth conveyance of intent across all authoring steps, this thesis proposes a framework for designing creativity support tools (CSTs) that lets users directly author with “concepts”—the core ideas they intend to express—first instead of starting from granular details. This shift in authoring focus is motivated by the idea that both creators and audiences naturally think in concepts. Four end-to-end authoring systems across diverse visual domains including visualization, illustration, space design, and storytelling serve as examples of the framework in practice. The framework was then iteratively refined, distinguishing between high-level and low-level concepts, through both formative and post-hoc user studies. Altogether, these systems demonstrate that concept-authoring can support creative outputs that are well-aligned with author intent across different working styles and visual domains."
          ],
          "keyword": [
            "human-computer interaction",
            "SVG (Document markup language)",
            "creativity support"
          ],
          "primary_title": "Concept-Authoring for Creative Visual Media",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:x6hwffhs/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:x6hwffhs/"
        },
        {
          "pid": "bdr:2rqsyzke",
          "object_type": "pdf",
          "abstract": [
            "Genome organization modulates critical cellular processes, such as gene expression, that drive cell differentiation and proliferation. In contrast, small, aberrant changes in this organization can lead to developmental disorders and cancer. Recent advances in next-generation sequencing have paved the way for both bulk and single-cell resolution genome-wide chromosome conformation capture techniques, such as Hi-C and scHi-C, that produce paired-end reads that coalesce as contact maps capturing the spatial proximity of two regions of DNA in 3D space. Analysis of these contact maps unveils crucial structural components that are known to be intimately tied to gene regulation mechanisms. Unfortunately, it is often impractical to acquire high-quality contact maps because they require billions of reads; thus, most studies produce sparse contact maps. The challenge is further exacerbated in single-cell sequencing, where the reads are further divided across all the cells, making contact maps even sparser for an individual cell. In this thesis, we will discuss a series of computational solutions that propose graph-based representations that combine “easily obtainable” structural and sequential genomics graph-based representations to generate contact maps at three levels of cellular resolution. First, we present GrapHiC, which proposes a graph-generative model that produces high-quality Hi-C contact maps using commonly available ChIP-seq (sequential) and a sparse Hi-C contact map (structural). Second, we present scGrapHiC, which proposes a novel graph deconvolution approach that extracts cell-type specific pseudo-bulk single-cell Hi-C from embryonic stem cell bulk Hi-C using pseudo-bulk scRNA-seq as a guiding signal. Finally, I will explain my plans to develop an extension of scGrapHiC where we relax the constraint on the large number of cells required per pseudo-bulk (~190) as we inch closer to a true single-cell resolution. We believe the work proposed in this thesis would facilitate researchers in elucidating structural mechanisms that mediate cell differentiation and disease progression."
          ],
          "keyword": [
            "Chromatin",
            "Gene expression",
            "epigenetics",
            "Computational biology",
            "Deep learning (Machine learning)",
            "Graph Neural Networks (GNNs)"
          ],
          "primary_title": "Connecting sequential and structural genomics through graph based representations",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:2rqsyzke/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:2rqsyzke/"
        },
        {
          "pid": "bdr:uxx526bt",
          "object_type": "pdf",
          "abstract": [
            "As new cryptographic techniques mature into real-world deployment, they are enabling new forms of analytics and data collection while preserving privacy. This then creates potential to conduct studies that can generate new insights for public policy and build new forms of governmental data collection that carry fewer privacy risks. However, public institutions have diverse sets of resources and usable systems must be designed to accommodate these disparities. This work demonstrates two such examples of systems designed specifically for lower-resourced institutions to participate in large public policy studies and data collection. The first work instantiates a legislative proposal for a decentralized encrypted gun registry that is built using structured encryption, secure multi-party computation, and secret sharing. The second work details a system for supporting analytics over encrypted data while designing for the usability considerations of public institutions. To do so, it combines cryptographic primitives such as structured encryption, somewhat homomorphic encryption, and oblivious pseudorandom functions. Lastly, enabling more government access to data and analysis of said data, even in encrypted forms, must be balanced with mechanisms for transparency. We propose the notion of ephemeral encrypted data structures, which guarantee time-based erasure of data without requiring any explicit deletion actions to be taken or relying on a trusted database operator. Altogether, this proposal contributes to dialogue between cryptography and its role in aiding public policy initiatives while balancing accountability and transparency."
          ],
          "keyword": [
            "Cryptography"
          ],
          "primary_title": "Cryptography in Support of Public Policy Use Cases",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:uxx526bt/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:uxx526bt/"
        },
        {
          "pid": "bdr:fk4sma5d",
          "object_type": "pdf",
          "abstract": [
            "This thesis aims to advance weak supervision by integrating foundation models—particularly large language models (LLMs) and vision-language models (VLMs)—to improve labeling efficiency and enhance model adaptability to downstream tasks. The central goal is to empower expert knowledge expression by reducing reliance on large-scale labeled datasets while enabling more flexible and scalable weak supervision techniques. The contributions of this thesis are organized around three directions. First, it introduces new methods to improve weak supervision, including a probabilistic generative model that allows labeling functions to output subsets of possible class labels, enabling more flexible and expressive expert supervision. Second, it presents techniques for efficiently adapting foundation models to downstream tasks, such as a novel parameter-efficient fine-tuning method for pretrained transformers: compositional soft prompting (CSP), which enhances generalization in vision-language models to unseen attribute-object combinations without task-specific training data. Third, this thesis explores the synergy between weak supervision and foundation models through prompted supervision techniques, including Alfred, a system for training data curation using natural language prompts; Alfred-Apps, a standardized suite for benchmarking prompted weak supervision methods; Agent Alfred, a prototype agentic system designed to further automate and streamline prompted weak supervision workflows; and a structure refining module that improves label aggregation by modeling dependencies among labeling function prompts via embedding similarities. These contributions have been empirically validated across diverse domains, including visual recognition, text classification, and multi-modal tasks, particularly in settings where labeled data is scarce or expensive to obtain. By creating a more expressive and accessible weak supervision framework, this thesis aims to bridge the gap between human expertise and machine learning systems, enabling more efficient knowledge transfer and adaptation in real-world applications."
          ],
          "keyword": [
            "Natural language processing (Computer science)",
            "Computer Vision",
            "Deep learning (Machine learning)",
            "Weak Supervision"
          ],
          "primary_title": "Enhancing Weak Supervision with Foundation Models: Empowering Expert Knowledge Expression",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:fk4sma5d/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:fk4sma5d/"
        },
        {
          "pid": "bdr:kzmx65ct",
          "object_type": "pdf",
          "abstract": [
            "In this thesis, we explore the use of human mental causal models in guiding planning under uncertainty in autonomous agents for tasks modeled as Partially Observable Markov Decision Processes (POMDPs). The ability to build causal models is a cornerstone of human cognition. It enables humans to plan under uncertainty and seamlessly generalize to unfamiliar domains. Machine learning algorithms on the other hand, often rely on sophisticated pattern recognition or extensive interactions with the environment to make predictions. We propose to leverage both by combining mental causal models from humans with different planning architectures to improve a robot’s decision making under uncertainty. We hypothesize that human mental maps of objects in the form of causal graphs, if integrated with planning frameworks in embodied agents operating under uncertainty, will improve planning outcomes in object-related tasks. To that end, we present (1) A method to embed crowd sourced causal models of objects into the belief state of POMDPs as imperfect priors to inform two different downstream tasks — object assembly and troubleshooting (2) Our investigation on transferring crowd sourced causal models to generate causal graphs for new objects using language models, and (3) A framework to integrate expert human-generated causal graphs with a Large Language model based planner for improved decision-making under partial observability in object assembly and troubleshooting. We show that the reward obtained when embedding crowdsourced causal models in the belief state of POMDPs becomes 2X the baseline reward obtained by solving the POMDPs without any priors for assembly and 1.4X the baseline reward for troubleshooting. We also show that the average reward across all objects and tasks obtained by using LLM powered planners go up when augmenting them with an expert human causal model. (from 8.9% to 76.2% for GPT-4o, from 75% to 81.7% for o3-mini high and from 64.9% to 74.2% for o4-mini high). We demonstrate that integrating causal models from humans into the planning paradigm of a robot improves its decision making under uncertainty. In the future, we hope to explore how our approaches extend to other domains — particularly in tasks within dynamic environments. We could obtain and integrate causal models efficiently because the number of parts and their functions in the objects are fixed. However, in other planning domains such as planning trajectories in a dynamic environment, a causal model might be harder to define or obtain."
          ],
          "keyword": [
            "Robotics",
            "human causal model",
            "planning under uncertainty"
          ],
          "primary_title": "Exploration of the use of human-generated causal models to guide planning under uncertainty in robots",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:kzmx65ct/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:kzmx65ct/"
        },
        {
          "pid": "bdr:29tux9ab",
          "object_type": "pdf",
          "abstract": [
            "Modern programming languages share a core set of linguistic concepts, including mutable variables, mutable data structures, and their interactions with scope and higher-order functions. Despite their ubiquity, students often struggle with these topics. How can we identify and effectively correct misconceptions about these cross-language concepts? This dissertation presents systems designed to identify and correct such misconceptions in a multilingual setting, along with a formal classification of misconceptions distilled from studies with these systems. At the core of this work are: (1) the idea that formally defining misconceptions allows for more effective identification and correction, and (2) a self-guided tutoring system that diagnoses and addresses misconceptions. Grounded in established educational strategies, this tutor has been tested in multiple settings. My data show that (a) the misconceptions addressed are widespread and (b) the tutor improves student understanding on some topics and does not hinder learning on others. Additionally, I introduce a program-tracing tool that explains programming language behavior with the rigor of formal semantics while addressing usability concerns. This tool supports the tutoring system in correcting misconceptions and appears to be valuable in its right."
          ],
          "keyword": [
            "Programming languages (Electronic computers)--Semantics"
          ],
          "primary_title": "Identifying and Correcting Programming Language Behavior Misconceptions",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:29tux9ab/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:29tux9ab/"
        },
        {
          "pid": "bdr:dv2eyjtg",
          "object_type": "pdf",
          "abstract": [
            "Shape analysis and generation methods are critical to many visual computing applications. Stakeholders often want to populate physical and artificial spaces with high-quality, structured assets that support interaction and manipulation. Different shape representations support these desiderata to varying degrees. Programmatic representations (e.g. procedural models) are a popular choice with many benefits, but also come with inherent limitations: they are expensive to author, have limited output variety, and typically require a thoughtfully designed domain-specific language (DSL). This dissertation explores a suite of neurosymbolic systems that combine learning with programmatic representations to aid in shape analysis and generation. When datasets of procedural assets are available, we can train generative models that synthesize novel shapes by learning to write programs. When we lack a dataset of procedural assets, we can train networks to search for programs that explain visual inputs with a bootstrapped, self-supervised learning paradigm. We show performance can be improved by reframing this program synthesis task as a program editing task, and also that this paradigm can be extended to infer stochastic programs capable of capturing a distribution of visual inputs. Finally, we investigate ways to discover better DSLs with little or no expert intervention. We propose two bottom-up library learning works that augment a starting DSL with automatically proposed functions that improve a data-driven compression objective, starting from shape datasets of either imperative programs or unstructured primitives. We also explore an alternative top-down framing, where we task a Large Language Model with authoring a library of shape abstraction functions from two forms of user design intent: text descriptions of functions to include in the library and a seed set of exemplar shapes. Together, these works demonstrate that the limitations of the procedural representation can be successfully mitigated through the application of hybrid neurosymbolic methods that learn to synthesize, infer, and abstract visual programs."
          ],
          "keyword": [
            "Machine Learning",
            "Artificial intelligence",
            "Computer graphics"
          ],
          "primary_title": "Neurosymbolic Methods for Shape Analysis and Generation",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:dv2eyjtg/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:dv2eyjtg/"
        },
        {
          "pid": "bdr:q6h7mdxs",
          "object_type": "pdf",
          "abstract": [
            "Object search is a central problem for human-robot interaction, as finding, localizing, and then grasping an object is a first step for almost anything a person would want the robot to do in the physical world. Additionally, natural language and gesture are the two most popular communication modalities due to the familiarity and comfort they afford the majority of human users. Human-guided object search is a difficult problem as the robot must identify objects based on the natural language and gesture specifications, which might lack information and be ambiguous. Furthermore, object detection accuracy can suffer from sensor noise and the robot's partial observation of the environment. This dissertation integrates language-conditioned visual models with a model-based decision-theoretic framework to enable effective robotic object search with complex natural language and gesture specifications. I will first present our research on affordance-based object retrieval which learns to encode language and visual inputs into a joint embedding space. Next, I will discuss our work on incorporating the language-conditioned visual detector's uncertainty into the planner's observation model for improved state estimation and object search. Lastly, I will describe our project on utilizing pointing gesture information in robot object search."
          ],
          "keyword": [
            "human-robot interaction",
            "Robotics",
            "Object Search"
          ],
          "primary_title": "Human-guided Robot Object Search",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:q6h7mdxs/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:q6h7mdxs/"
        },
        {
          "pid": "bdr:zas4hvxp",
          "object_type": "pdf",
          "abstract": [
            "Excess greenhouse gasses are one of the largest contributors to climate change, a crisis which has had a devastating effect on human beings, biodiversity, and the Earth more broadly. The amount of carbon dioxide specifically, which has been emitted in incredibly large quantities since the Industrial Revolution, has served as a catalyst for the Earth’s warming; yet, emissions are not decreasing quickly enough to prevent temperatures from crossing irreversible thresholds of damage. A potential solution for this problem is Carbon Capture and Storage (CCS); carbon capture is a method for removing or reducing the amount of greenhouse gasses in the atmosphere via some chemical process. One such process is through the use of crystal materials called Metal Organic Frameworks (MOFs). MOFs have a cage-like structure which can capture carbon by trapping carbon dioxide molecules while letting other atmospheric gasses simply pass through the MOF. The challenge is to find the MOF(s) which are best able to capture carbon, whether that be by having the largest capacity for the amount of CO2 molecules it can hold or being particularly selective in trapping CO2 compared to other atmospheric molecules. This, then, leads to the application of machine learning, particularly methods which easily depict chemical structures such as Graph Neural Networks (GNNs), to find the most optimal MOF for capturing carbon. This project uses GNNs to analyze the ability of MOFs to capture carbon holistically, i.e. in which MOFs are able to perform carbon capture such that various metrics of CCS are optimized instead of just one dimension (the literature norm). This project goes on to describe future steps and implications of this work."
          ],
          "keyword": [
            "Climate change mitigation",
            "Deep learning (Machine learning)"
          ],
          "primary_title": "Analyzing Carbon Capture Abilities of MOFs Using Graph Neural Networks",
          "uri": "https://repository.library.brown.edu/studio/item/bdr:zas4hvxp/",
          "json_uri": "https://repository.library.brown.edu/api/items/bdr:zas4hvxp/"
        }
      ]
    }
  }